- 메모리 계층 (상-하층 순)
    - 레지스터
    - 캐시
    - 메모리
    - 하드디스크

- 외부 단편화와 내부 단편화란?
    - 외부 단편화 : 작업보다 많은 공간이 있더라도 실제로 그 작업을 받아들일 수 없는 경우 (메모리 배치에 따라 발생하는 문제)
    - 내부 단편화 : 작업에 필요한 공간보다 많은 공간을 할당받음으로써 발생하는 내부의 사용 불가능한 공간

- 힙 vs 스택
    - 메모리
        - 프로그램을 실행시키면 운영체제는 실행한 프로그램을 위해서 메모리 공간을 할당해줌. 이 때 할당되는 메모리 공간이 크게 코드, 데이터, 힙, 스택 영역으로 나누어짐.
          할당되는 장소는 메인 메모리(RAM)에 할당되며, 용도는 프로그램 실행 시 필요한 공간을 지정하기 위해서 할당된다.

        - 코드 영역
            - 실행할 프로그램의 코드가 저장되는 영역
            - 텍스트 영역이라고 부르기도 한다.
            - CPU 는 코드 영역에서 저장된 명령을 하나씩 가져가서 처리하게 된다.
            - 프로그램이 시작하고, 종료될 떄까지 메모리에 계속 남아있는다.

        - 데이터 영역
            - 전역 변수와 정적(static) 변수가 저장되는 영역
            - 프로그램의 시작과 함께 할당되며 프로그램이 종료되면 소멸
            - main 이 호출되기 전에 할당됨. 그래서 프로그램이 종료될 떄까지 메모리 상에 존재
            - 상수도 저장
            - 프로그램이 종료될 때까지 지워지지 않을 데이터 저장
            - 프로그램이 실행 중일 때는 메모리를 해제할 수 없다. 메모리가 비효율적으로 사용될 수 있으므로 사용 자제

        - 힙 영역
            - 사용자의 동적 할당 (프로그래머가 직접 공간을 할당, 해제)
            - 런타임에 크기가 결정됨
            - 스택보다 큰 메모리를 할당받기 위해 사용
            - 스택보다 느림
            - 용량은 크지만 메모리 단편화의 위험성을 갖고 있다. (메모리 단편화 - 메모리를 100% 활용하지 못하는 경우)
              제대로 반환하지 않으면 메모리 누수가 일어나서 메모리 손해를 볼 수 있다. (메모리 누수 - 프로그램이 필요하지 않은 메모리를 계속 점유하고 있는 현상)
              주로 동적할당 시 발생. 스택은 스택 오버플로우가 발생하면 바로 에러를 뱉어버리기 때문에 쉽게 알 수 있다.(용량도 적다.)
              하지만 동적할당이 이루어지는 힙 영역은 문제 없이 잘 동작할 수도 있기 때문에 이런 누수 현상이 발생하는 것. 힙 영역은 용량이 크기 때문에
              누수 현상이 발생해도 알아차리기 어렵다.
              누수 현상이 발생하면 프로그램이 불안정해지고 컴퓨터도 느려진다. 요즘 컴퓨터는 램 용량이 부족할 시 하드디스크를 램처럼 끌어와 쓴다.
              당연히 하드는 램보다 느리므로 컴퓨터가 느려지는 것.

        - 스택 영역
            - 지역변수, 매개 변수 (프로그램이 자동으로 사용하는 임시 메모리 영역)
            - 컴파일 타임에 크기가 결정됨
            - 잠깐 사용하고 삭제하는 데이터 저장
            - 접근이 매우 빠르고, 변수를 명시적으로 할당 해제할 필요 없다. 컴파일러에 의해 메모리 해제 할당
              메모리에 낭비되는 공간 없이 사용 가능. 스택의 의미대로 그냥 차례대로 쌓기 때문 (연속적 메모리 영역) 하지만 용량이 매우 적다.
            - 함수의 호출하는 위치도 저장
            - 함수가 실행 중일때만 접근 가능한 공간이므로 중간에 접근하려고 해도 불가능한 경우가 많다.
            - 함수 호출 시 생성되는 지역 변수와 매개 변수가 저장되는 영역이고, 함수 호출이 완료되면 사라진다.
              이때 스택 영역에 저장되는 함수의 호출 정보를 스택 프레임(stack frame) 이라고 한다.

            - 스택 프레임   https://www.jiwon.me/explain-heap-and-stack/    https://all-young.tistory.com/17
                - 스택 영역에 함수를 구분하기 위해 생성되는 공간
                - 컴퓨터가 프로그램을 실행할 때, 코드영역에서 코드를 읽으며 필요한 데이터를 데이터,힙,스택영역에서 레지스터로 가져온다.
                  cpu 는 레지스터 값을 가지고 연산을 하고, 결과물을 다시 저장영역에 돌려준다.
                - 데이터를 가져올 때 필요한 것이 스택이다. 데이터영역은 전역변수나 상수만 저장되기 때문에 한번 읽으면 끝이다.
                  나머지 변수는 코드를 실행할 때마다 다시 읽어야 한다. 왜냐하면 변수이기 때문에 이전에 읽어들인 값이 지금 값과 같지 않을 수 있기 때문이다.
                  힙 영역에 접근할때도 스택이 필요하다. 데이터가 힙에 저장 되어 있더라도, 그 데이터에 접근하기 위한 변수(포인터)는 스택에 저장되기 때문이다.
                - 스택은 스택 자료구조 형태로 저장되고 최상단에 있는 스택프레임이 현재 실행하고 있는 프로그램에 필요한 변수가 저장되어 있다.
                  즉 컴퓨터는 현재 연산을 수행할 때, 가장 위에 있는 스택프레임만 보면 되는 것이다.
                - 그렇다면 왜 스택구조(후입선출)로 쌓는 것일까?
                  프로그램은 순차적으로 실행하기 때문에 가장 최근에 할 일을 가장 위쪽에 위치하게 하는 것이다.

    - 오버 플로우
        - 한정된 메모리 공간이 부족하여 메모리 안에 있는 데이터가 넘쳐 흐르는 현상
        - 힙은 메모리 낮은 주소부터 할당되고, 스택은 메모리 높은 주소부터 할당되기 때문에 각 영역이 상대 공간을 침범하는 일이 발생할 수 있다.
        - 힙 오버 플로우
            - 힙이 스택을 침범하는 경우
        - 스택 오버 플로우
            - 스택이 힙을 침범하는 경우


- 사용자 수준 스레드 vs 커널 수준 스레드 차이는?
  #사용자 수준 스레드
  장점 : context switching이 없어서 커널 스레드보다 오버헤드가 적음 (스레드 전환 시 커널 스케줄러 호출할 필요가 없기 때문)
  단점 : 프로세스 내의 한 스레드가 커널로 진입하는 순간, 나머지 스레드들도 전부 정지됨 (커널이 스레드의 존재를 알지 못하기 때문에)

  #커널 수준 스레드
  장점 : 사용자 수준 스레드보다 효율적임. 커널 스레드를 쓰면 멀티프로세서를 활용할 수 있기 때문이다. 사용자 스레드는 CPU가 아무리 많아도 커널 모드의 스케줄이 되지 않으므로, 각 CPU에 효율적으로 스레드 배당할 수가 없음
  단점 : context switching이 발생함. 이 과정에서 프로세서 모드가 사용자 모드와 커널 모드 사이를 움직이기 때문에 많이 돌아다닐 수록 성능이 떨어지게 된다.


- Race Condition이란?
    - 두 개 이상의 프로세스가 공통 자원을 병행적으로 읽거나 쓸 때, 공용 데이터에 대한 접근이 순서에 따라 실행 결과가 달라지는 상황
    - ace Condition이 발생하게 되면, 모든 프로세스에 원하는 결과가 발생하는 것을 보장할 수 없음. 따라서 이러한 상황은 피해야 하며 상호배제나 임계구역으로 해결이 가능하다.


- 프로세스의 상태도 저장된다고 했는데 프로세스의 상태는 어떤것들이 있나요?
    - 실행 : CPU를 점유하고 있는 상태이며 실행되고 있는 상태입니다.
    - 준비 : CPU만 할당 받는다면 즉시 수행가능합니다.
    - 봉쇄 : CPU할당 받아도 실행못하는 상태입니다. 예를 들어 입출력 작업은 CPU가 할 수 있는일이 없기 때문에 다른 장치로부터 신호가 와야합니다.


- 스레드는 왜 독립적인 스택 메모리를 가지나요?
    - 스택영역의 정의부터 정확하게 알아야합니다. 스택영역은 스레드가 프로세스 내에서 함수호출시 전달되는 인자, 함수의 리턴주소등을 저장하기 위한 메모리 영역입니다.
      스레드가 하나의 프로세스 내에서 독립적인 기능을 하기 위해서는 독립적인 함수를 호출해야함을 의미하며 이는 독립적인 스택 메모리를 가져야만 가능합니다.


- 멀티 프로세스와 멀티 스레드를 비교해주세요.
    - 문맥교환시 멀티스테드는 스택영역의 내용만 변경되면 되지만 멀티 프로세스의 경우 힙,데이터,코드 영역의 정보까지 바뀌므로 문맥교환의 비용이 멀티프로세스가 높습니다.
    - 멀티 프로세스는 많은 CPU시간과 메모리공간을 차지하며 하나의 프로세스가 죽더라도 다른 프로세스에 영향을 주지 않습니다.
    - 멀티 스레드는 적은 메모리 공간을 차지하고 문맥교환 비용이 적으며 자원을 공유하기 때문에 동기화 문제가 발생할 수 있고 하나의 스레드 장애가 여러 스레드의 장애로 이어질 위험이 있습니다.


- 멀티 프로세스들이 실행되는 과정을 간략하게 표현해주세요.


- 프로세스간 통신은 어떻게 하나요?
    - 스레드는 서로의 자원을 공유하고 있기 때문에 스레드간 통신은 별도의 자원을 사용하지 않아도 됩니다.
      하지만 프로세스의 경우 데이터를 공유하고 있지 않기 때문에 공유메모리를 사용하는 방법과 메세지전달 방법으로 2가지 방법이 있습니다.
    - 공유 메모리 : 프로세스들이 주소 공간 일부를 공유하는 것을 의미합니다. 커널이 메모리에 별도의 shared memory를 할당합니다.
      프로세스들은 shared memory의 정보를 이용해 서로 데이터를 전달 및 공유 합니다.
    - 메세지 전달 : 시스템 콜을 이용하여 구현합니다. 시스템 콜이란 커널영역에서 사용하는 명령어들이며 커널을 통해 메세지를 보내고 받습니다.
      즉, 프로세스들 사이에 커널이 데이터를 중재하는 것입니다. 예를 들어, pipe , socket , message queue 가 있습니다.
    - 2가지 방식의 장단점 역시 존재합니다. 공유메모리를 사용할 경우 간단하지만 메모리를 공유하기 때문에 자원 동기화의 문제가 있을 것이며 메세지를 전달한다면 동기화 문제는 없지만 속도가 느립니다.


- 멀티 프로세스 , 스레드 환경에서 동기화 문제는 어떻게 해결하나요?
    - 해결하는 방법은 뮤텍스 또는 세마포어를 사용합니다.
    - 뮤텍스 : 1개의 스레드만 공유자원에 접근가능합니다.
    - 세마포어 : S개의 스레드만이 공유자원에 접근가능합니다. 스레드가 접근할 때 마다 S의 개수를 감소시키며 S가 0이라면 자원에 접근 할 수 없습니다.

    - critical section : 공유할 자원들 즉, 코드 영역에서 여러 스레드들이 접근 할 수 있는 부분을 통제하기 위해서 둘 이상의 프로세스 또는
      스레드가 접근하는 것을 막는 코드 부분을 임계영역 (critical section)이라고 합니다. 또한 임계영역내에서 코드들은 원자적으로 실행되어야 합니다.
    - entry section : 진입 허가 요청이 완료되어야 임계영역으로 갈수 잇습니다.
    - exit section : 임계영역 실행후 끝나는 부분
    - entry section -> critical secition -> exit section 으로 이동합니다.


- 운영체제는 프로세스를 메모리로 올릴 때 메모리 관리를 어떻게 하나요?
    - 프로세스를 통째로 연속된 메모리에 올려놓는 일은 거의 없습니다. 이때까지 편의를 위해 그림으로 그렇게 표현했지만 프로세스들을 무작정 물리 메모리에 순서대로 올리면 많은 문제들이 발생합니다.
    - 효율적으로 프로세스들에게 메모리를 할당해주기 위한 기법이 생기기 시작했습니다. 바로 페이징과 세그멘테이션입니다.


- 페이징(Paging)은 무엇인가요?
    - 물리 메모리를 동일한 크기로 조각조각 냅니다. 이때 하나의 조각을 페이지라고 합니다. 메모리에 올릴 프로세스도 페이지의 크기만큼 조각내서
      물리 메모리에 저장하는 방식입니다. 그렇게 되면 물리 메모리에 연속되서 저장될 필요없이 페이지 단위로 분산되어 저장 가능합니다.
      프로세스 입장에서는 자신의 페이지들이 어디있는지 알아야하는데 이를 위해 페이지 테이블이라는 것이 존재합니다.
    - 프로세스당 하나의 페이지 테이블을 가집니다. 하나의 프로세스 내에서 페이지단위로 물리적메모리에 저장되기 때문에 주소를 바인딩하기위해 별도의 테이블이 필요합니다.
      물리메모리 주소를 프레임번호라고도 합니다.


- 페이징 기법에서 문제점이 있나요?
    - 메모리 내부 단편화 문제가 발생합니다. 프로세스를 페이지 단위로 쪼개다보면 페이지 안에 안쓰는 공간이 남습니다.
      예를 들어 100 크기의 프로세스를 30크기의 페이지로 쪼개면 4개의 페이지로 나오고 마지막 페이지는 20이 남습니다.
      이러한 페이지를 물리메모리에 할당하게 되면 20이라는 공간은 아무것도 안하고 남아있는 공간이 됩니다.
    - 외부 단편화 문제는 발생하지 않습니다. 외부단편화는 남아있는 공간은 작은데 그 공간보다 큰 프로세스를 집어넣으려고 할때 발생하는데
      프로세스를 페이지 단위로 나누기 때문에 페이지 단위로 무조건 할당됨을 보장하므로 메모리를 할당하지 못해서 나오는 외부단편화 문제는 발생하지 않습니다.


- 세그먼테이션(Segmentation)이 무엇인가요?
    - 페이지는 프로세스를 크기단위로 똑같이 나뉘었다면 세그먼테이션은 의미단위로 메모리를 나누는 것을 의미합니다.
      프로세스를 코드,데이터,힙,스택으로 기능 단위로 정의한다면 4가지로 조각내어 메모리에 저장합니다.
      세그먼테이션은 페이징과 마찬가지로 물리적 주소로 변환하기 위한 세그먼테이션테이블을 가지고 있습니다.
    - 만약 논리적 주소가 2 / 200 이라면 어떨까요? 2는 세그먼트 번호를 의미하고 200은 떨어진 거리를 의미합니다.
      따라서 세그먼트2번의 주소가 3500 부터 시작하고 3500에서 200떨어진 거리인 3700이 2/200 의 논리적주소가 변경된 물리적 주소입니다.
    - 이처럼 세그먼트는 모든 크기가 다르기 때문에 항상 시작점과 limit를 같이 표기해야합니다.


- 세그먼테이션의 단점이 있나요?
    - 세그먼테이션은 메모리의 내부단편화 문제가 없습니다. 왜냐하면 필요한 만큼 메모리를 정확하게 할당 받기 때문입니다.
    - 외부 단편화문제가 존재합니다. 세그먼트가 너무 커서 특정 공간에 할당 못할 수도 있습니다. 세그먼트를 쪼개면 가능하겠지만 이는 불가능합니다.


- 가상메모리가 무엇인가요?
    - 운영체제는 메모리를 더욱더 효율적으로 사용하기 위해서 도입한 개념입니다.
    - 정의 : 프로세스 전체가 메모리에 올라오지 않아도 실행이 가능하도록 하는 것입니다.

    - 이때까지 메모리에 프로세스 전체가 올라가는 그림이 그려졌습니다. 하지만 가상메모리는 프로세스 전체가 메모리로 올라오지 않습니다.
      어떻게 메모리에 전체 프로세스를 올리지 않더라도 프로세스를 실행할 수 있는 것일까요?
    - 운영체제는 가상메모리 기법을 통해서 프로그램의 논리적 주소 영역에서 필요한 부분만 물리 메모리에 적재하고 직접적으로 필요하지 않은 부분은 디스크(SWAP)에 저장하는 것을 말합니다.

    - 페이징 기법을 쓰는 운영체제에서 가상메모리를 사용한다고 가정해봅시다. 당장 사용될 주소 공간을 page단위로 메모리에 적재하는 방법을 요구페이징이라고 합니다.
      요구 페이징에서는 특정 페이지에 대해 cpu요청이 들어온 후에 해당 페이지를 메모리에 적재합니다. 당장 실행에 필요한 페이지만을 메모리에 적재하기 때문에
      메모리 사용량이 감소하고 프로세스 전체를 메모리에 적재하는 입출력 오버헤드도 감소합니다. 요구페이징 기법에서는 유효/무효 비트를 두어서 각 페이지가 메모리에 존재하는지 표시하게 됩니다.


- 요구페이징에서 CPU가 요청한 메모리에 없고 디스크에 있다면 어떻게 되나요?
    1) CPU가 페이지 참조
    2) 페이지 테이블을 보니 페이지 상태가 "무효" 상태
    3) MMU에서 페이지 폴트 트랩을 발생
    4) 디스크에서 메모리로 페이지를 가져온 후 "유효" 상태로 변경

    이와 같이 요청한 페이지가 없는 경우를 페이지 폴트(Page fault)라고 합니다.

- 페이지 폴트 발생 후 메모리로 페이지를 가져오려는 데 메모리도 부족한 경우라면 어떤 페이지를 교체하나요?
    - 이를 위한 알고리즘이 존재합니다. 바로 페이지교체 알고리즘입니다.
    - LRU : 가장 오랫동안 사용하지 않은 페이지를 메모리에서 디스크로 방출합니다.
    - LFU : 참조 횟수가 가장 적었던 페이지를 메모리에서 디스크로 방출합니다.


- 동기, 비동기, Blocking, Non-Blocking
    - 동기
        - I/O가 진행되는 동안 다음 명령을 수행하지 않고 기다린다.
          I/O 상태의 프로세스는 blocked state로 전환된다.
          I/O가 완료되면 인터럽트를 통해 완료를 알린다. 이후 CPU의 제어권이 기존 프로그램에게 넘어간다.
          blocked state의 프로세스는 wait 상태로 돌아간다.

        - 여러 프로세스가 동시에 I/O 요청을 할 경우 각 요청을 큐에 넣어 순서대로 처리한다.

    - 비동기
        - CPU의 제어권을 입출력 연산을 호출한 프로그램에게 곧바로 다시 부여
        - CPU는 I/O 결과와 상관 없이 처리 가능한 작업부터 처리한다.
        - I/O 연산이 완료되면 인터럽트를 통해 알린다.

    - Blocking I/O
        - 직접 제어할 수 없는 대상의 작업(I/O)이 완료될 때까지 기다린다.
            - I/O가 완료되어야 제어권이 프로세스로 넘어간다.
        - 동기와 마찬가지로 자원이 낭비된다.

    - Non-Blocking I/O
        - 외부에 I/O 작업을 하도록 요청한 후, 즉시 다음 작업을 처리
            - 시스템 자원을 더 효율적으로 사용할 수 있게된다.
        - I/O 작업이 완료된 이후에 처리해야 하는 후속 작업이 있다면, I/O 작업이 완료될 때까지 기다려야 한다.
          따라서 이 후속 작업이 프로세스를 멈추지 않도록 만들기 위해 I/O 작업이 완료된 이후 후속 작업이 이어서 진행할 수 있도록 별도의 약속(Polling, Callback function 등)을 한다.

    - 동기/비동기는 인터럽트 발생으로 인한 제어권 반한 시점에 중점을 두고 Blocking/Non-Blocking은 제어권 자체에 중점을 둔다는 점에서 차이가 있다.

- 프로세스와 스레드는 각각 무엇이고 어떤 차이점이 있을까요?
    프로세스는 실행 중인 하나의 애플리케이션입니다.
    운영체제로부터 실행에 필요한 메모리를 할당받아 애플리케이션의 코드를 실행합니다.
    필요한 메모리 영역은 프로그램의 코드를 저장하는 Text 영역, 전역 정적 변수들을 저장하는 Data, 지역 변수들을 저장할 Stack, 동적 메모리 할당을 받을 Heap 영역입니다.

    운영체제에서 각 프로그램들은 동시에 실행할 수 있도록 멀티 프로세스로 동작합니다.
    이는 CPU 자원을 잘게 쪼갠 시간 내에 점유한다는 뜻입니다.
    하나의 프로세스가 CPU 자원을 점유하려는 순간 프로세스의 상태를 불러와야 합니다.
    이 순간에 프로세스 상태를 교체하고 새로운 CPU 레지스터 변수들을 불러오는 작업을 컨텍스트 스위칭(Context Switching)이라고 합니다.
    점유에 벗어나는 순간에 프로세스의 상태를 저장해야 합니다.

    프로세스마다 각자 고유 상태를 갖고 있습니다. 이를 PCB(Process Control Block)이라고 합니다.
    현재 프로세스 상태가 실행 중인지, 대기 중인지를 알려주는 Process State,
    고유 프로세스의 식별값을 나타내는 Process ID,
    다음 실행할 프로그램 코드 위치를 저장하는 Program Counter,
    메모리 정보를 관리할 페이지 테이블과 세그먼트 테이블, CPU의 스케쥴링 큐 포인터와 우선순위 정보, I/O 정보들 등등으로 이루어져 있습니다.

    반면, 스레드는 하나의 프로세스 안에서 실행되는 여러 코드 흐름을 뜻합니다.
    프로세스가 갖고 있는 Text 영역, Data 영역, Heap 영역을 공유하지만, 스레드 내부에서 Stack을 별도로 할당받습니다. 어떤 프로그램이든 하나의 주요 흐름을 실행하는 Main 스레드는 가지고 있습니다.
    Data 영역과 Heap 영역을 공유하기에 다른 스레드와 데이터를 통신할 수 있지만, 어느 스레드가 먼저 실행할 지 모르기에 동기화와 교착 상태에 각별한 주의가 필요합니다.


- 프로그램 코드 실행을 하드웨어단에서 어떻게 하나요?
    프로그램을 실행할 때, 적당한 메모리 위치에 프로그램이 쓸 영역을 올립니다.
    프로그램이 사용할 영역은 방금 말씀드렸다시피 코드를 저장하는 영역, 전역 정적 변수들을 저장하는 영역, 지역 변수들을 저장하는 영역, 동적메모리 할당을 받을 영역으로 나누어져있습니다.

    그 다음 코드를 해석하고 실행하는 장소는 CPU 입니다.
    CPU 에서는 Program Counter 라는 레지스터 변수로 다음 실행할 위치의 코드를 저장하는데요.
    PC 위치에 있는 코드를 메모리로부터 읽어오고 명령어를 해석하여 적절한 행동을 실행합니다.
    다시 PC에 다음 코드의 위치를 저장하고 프로그램이 종료할 때 까지 반복합니다.


- 프로그램을 메모리에 올릴 때, 적당한 메모리 공간의 위치를 어떻게 효율적으로 정할 수 있을까요?
    메모리에 필요한 공간을 할당할 때, 메모리의 빈 공간을 탐색해서 할당합니다.
    모든 프로그램마다 할당받을 메모리의 크기도 모두 다르기에, 무턱대고 할당하고 해제하기를 반복한다면
    메모리의 빈 공간을 탐색하는데 시간이 많이 들 뿐 더러, 불필요하게 빈 메모리가 많이 생길 것 입니다.

    효율적인 메모리 관리 기법으로 할당할 메모리 주소의 탐색 시간을 줄이고 빈 메모리 공간을 줄입니다.
    이를 관리하는 하나의 하드웨어 단위가 있습니다. 바로 MMU (Memory Management Unit) 입니다.

    최초의 메모리 할당 방법은 연속적으로 할당하는 방법입니다. MMU에서 Offset 레지스터로 CPU가 읽은 가상 주소로부터 물리 주소에 매핑을 시켜주는 역할을 합니다. 현재 프로그램 영역을 벗어나지 않도록 Limit 레지스터로 주소 접근을 제한하기도 합니다. 만약에 제한을 넘어갔을 때 인터럽트로 시스템에게 예외를 처리하도록 부탁합니다. 연속적 할당 방법의 장점은 구현이 간단합니다. 그러나 초반에 말했던 단점들은 고스란히 남아있습니다.
    할당과 해제의 과정을 거치면서 메모리 중간에 빈 공간들이 생겨나는데요.

    빈 공간들을 모조리 합치면 새로 들어올 프로그램의 메모리를 옮길 수 있지만,
    연속적으로 할당한다는 특징으로 할당이 불가능합니다.
    총 여유 메모리로 충분히 할당하고도 남지만, 실제로 할당할 수 없는 경우를 외부 단편화(External Fragmentation) 라고 합니다.
    메모리 공간의 빈 공간들을 모두 없애 앞쪽으로 땡기는 Compact 기법이 있습니다. 하나 하나 메모리 영역을 복사하여 빈 공간이 없도록 반복하게 되는데, I/O 문제가 발생할 수 밖에 없습니다.

    이를 해결할 방법은 메모리 영역을 쪼개는 방식입니다. 이를 페이징(Paging) 방식이라고 합니다.
    할당 받은 프로그램의 물리 메모리를 특정 크기의 프레임으로 쪼개 순서에 상관없이 저장합니다.
    그렇다면, 각 프로세스는 어떻게 순서대로 프로그램을 실행할 수 있을까요?
    프로세스 별로 페이지 테이블을 가지고 있습니다. 논리적인 페이징 테이블로 해당하는 실제 메모리 주소에 있는 프레임을 매핑하여 위치를 찾아냅니다.

    페이징 방식의 장점으로 프로그램 메모리를 잘개 쪼개 효율적으로 메모리에 저장하여 외부 단편화를 해소한다는 점이고요. 페이징 방식의 단점으로는 특정 크기의 프레임으로 메모리 영역을 분리하는데요.
    만약에 메모리 크기가 딱 맞지 않다면은 여전히 빈 공간이 생깁니다.
    이를 내부 단편화(Internal Fragmentation)라고 합니다.
    프레임 크기를 줄일수록 페이지 테이블이 커지고 그에 따른 오버헤드는 훨씬 빈번히 발생할 것 입니다.

    물리 메모리에 접근할 때 늘 페이지 테이블에 접근해서 매핑된 물리 메모리에, 총 두 번 접근합니다.
    이를 개선하기 위해 연관 메모리인 TLB(Translation Look-aside Buffer, 변환 색인 버퍼)를 사용합니다.
    페이지 테이블을 대상으로 일종의 캐시 역할을 해줍니다.

    더 나아가 메모리 크기가 커지면서 페이지 테이블을 여러 단계로 계층화 시킨다든지, 해쉬 테이블을 이용한다든지, 하나의 페이지 테이블로 통합시킨다든지 하는 방법들이 나왔습니다.
    자세히는 모르지만 현대에 와서는 멀티 코어 시대인만큼 병렬화를 적극 이용하지 않았을까 싶습니다.


- 프로그램 코드를 읽어올 때 CPU에서 어떻게 효과적으로 읽을 수 있는지 생각해보셨나요?
    CPU와 메모리의 연산 속도는 다르기에 CPU 에서도 효과적으로 메모리로부터 데이터를 읽는 방법이 필요합니다. CPU 내부에 여러 캐시 메모리를 장착하고 있어 최대한 CPU 사이클보다 느린 메모리로부터 데이터나 코드를 불러오는 것을 지양하면서 캐시 메모리를 적극 활용해 속도를 개선합니다.

    코드를 읽을 때, 메모리에서 다음에 실행할 코드 뭉치들을 미리 불러와 캐시 메모리에 저장합니다.
    캐시 메모리에 저장한 명렁어들을 소진될 때 까지 순차적으로 실행해 속도를 개선합니다.


- Context Switching이 일어날 때 운영체제 내에서 어떻게 동작하나요?
    현재 프로그램 코드에서 다른 프로그램 코드 영역으로 옮길 때, 현재 프로세스의 레지스터 정보와 PCB를 저장합니다. 운영체제 내에서 레지스터 정보와 PCB를 저장하는 테이블에 등록합니다.
    그리고 다른 프로세스의 정보를 탐색해 불러옵니다. 다시 원래 프로세스로 돌아올 때 저장했던 테이블에서 탐색해 불러옵니다.


- 스레드 동기화는 어떻게 이루어지나요?
    스레드 간에 객체를 공유하여 수정할 시, 다른 스레드에도 그 정보가 반영되지 않아 예상치 못한 오류가 일어날 수 있습니다.
    멀티 스레드 환경에서 단 하나의 스레드만 실행할 수 있는 코드 영역을 임계 영역이라고 합니다.
    자바에서는 이를 위해 동기화(synchronized) 메소드와 동기화 블록을 제공합니다.
    스레드 내부의 동기화 메소드 또는 블록에 들어가면 즉시 객체에 잠금을 걸어 다른 스레드가 해당 임계 영역 코드를 실행하지 못하도록 합니다.


- 스레드 개수가 많아지면 더 효율적일까요?
    과도한 스레드 개수로 애플리케이션의 성능이 저하될 수 있습니다.
    스레드 개수가 증가할수록, 스레드 생성과 스케줄링으로 CPU 점유율이 올라가고 메모리 사용량이 늘어납니다.

    스레드의 폭증을 막으려면 스레드풀(ThreadPool)을 사용해야 합니다.
    스레드를 제한된 개수만큼 정해 놓고 작업 큐에 들어오는 작업들을 하나씩 스레드가 맡아 처리합니다.
    작업 처리가 끝난 스레드는 다시 작업 큐에서 새로운 작업을 가져와 처리합니다.

    자바에서는 java.util.concurrent 패키지에서 ExecutorService 인터페이스와 Executors 클래스를 제공합니다.
    Executors 클래스의 정적 메소드로 다양한 ExecutorService 구현 객체인 스레드풀을 만들 수 있습니다.

    스레드풀의 종류는 세 가지가 있습니다.
    newCachedThreadPool()은 동적 크기로 처리할 작업이 있을 때 새 스레드를 생성합니다. 60초 동안 추가된 스레드가 아무 작업을 하지 않으면 추가된 스레드를 종료하고 풀에서 제거합니다.
    newFixedThreadPool()은 최대 스레드 개수를 설정하며, 유후 스레드가 있더라도 스레드 개수가 줄지 않습니다.
    newSingleThreadPool()은 하나의 스레드를 생성하며, 작업이 차례대로 실행되며 스레드 안전하다.


- 콘보이 현상(convoy effect)이란 무엇이고, 콘보이 현상이 발생될 수 있는 CPU 스케줄러 알고리즘은 무엇인지 설명해주세요.
    - 콘보이 현상이란 작업 시간이 긴 프로세스가 먼저 큐에 도착해서 다른 프로세스의 실행 시간이 전부 늦춰져 효율성을 떨어뜨리는 현상을 말합니다.
    - FCFS(First-Come First Served) 스케줄링은 비선점형으로, 순차적으로 먼저 큐에 들어온 작업부터 실행하므로 콘보이 현상이 발생할 수 있습니다.


- 선점형 스케줄링과 비선점형 스케줄링의 차이를 설명해주세요.
    - 선점형은 하나의 프로세스가 다른 프로세스 대신에 CPU를 차지할 수 있음을 말하고,
    - 비선점형은 하나의 프로세스가 끝나지 않으면 다른 프로세스는 CPU를 사용할 수 없음을 말합니다.


- Critical Section(임계영역)에 대해 설명해주세요.
    - 임계 영역이란 프로세스간에 공유자원을 접근하는데 있어 문제가 발생하지 않도록 한번에 하나의 프로세스만 이용하게끔 보장해줘야 하는 영역을 말합니다.
    - 임계 영역 문제를 해결하기 위해서는 아래의 3가지 조건을 충족해야 합니다.
        - 상호 배제(Mutual exclution) - 하나의 프로세스가 임계 영역에 들어가 있다면 다른 프로세스는 들어갈 수 없어야 한다.
        - 진행(Progress) - 임계 영역에 들어간 프로세스가 없는 상태에서 들어가려 하는 프로세스가 여러 개라면 어느 것이 들어갈지 결정 해주어야 한다.
        - 한정 대기(Bounded waiting) - 다른 프로세스의 기아를 방지하기 위해, 한 번 임계 구역에 들어간 프로세스는 다음 번 임계 영역에 들어갈 때 제한을 두어야 한다.


- 뮤텍스(Mutex)와 세마포어(Semaphore)의 차이에 대해 설명해주세요.
    - 뮤텍스는 Lock을 사용해 하나의 프로세스나 쓰레드를 단독으로 실행하게 합니다.
      반면에 세마포어는 공유자원에 세마포어 변수만큼의 프로세스(또는 쓰레드)가 접근할 수 있습니다.
    - 현재 수행중인 프로세스가 아닌 다른 프로세스가 세마포어를 해제할 수 있습니다.
      하지만 뮤텍스는 락(lock)을 획득한 프로세스가 반드시 그 락을 해제해야 합니다.


- 페이지 교체 알고리즘에 대해 설명해주세요.
    - 페이징 기법으로 메모리를 관리하는 운영체제에서 필요한 페이지가 주기억장치에 적재되지 않았을 시(페이징 부재시) 어떤 페이지 프레임을 선택해 교체할 것인지 결정하는 방법을 페이지 교체 알고리즘이라고 합니다.

    - FIFO(first in first out)
        가장 간단한 알고리즘으로, 메모리에 올라온 지 가장 오래된 페이지를 교체합니다. 간단하고, 초기화 코드에 대해 적절한 방법이며, 페이지가 올라온 순서를 큐에 저장합니다.
    - 최적(Optimal) 페이지 교체
        앞으로 가장 오랫동안 사용되지 않을 페이지를 교체하는 알고리즘입니다. 최적 페이지 교체는 선행 조건이 있는데, 프로세스가 앞으로 사용할 페이지를 미리 알아야 한다는 것입니다. 이 조건은 실제 활용에선 알 방법이 없기 때문에 최적 알고리즘은 구현이 불가능한 알고리즘입니다. 때문에 연구를 목적으로 주로 사용됩니다.
    - LRU(least-recently-used)
        가장 오래 사용되지 않은 페이지를 교체하는 알고리즘입니다. OPT 알고리즘의 방식과 비슷한 효과를 낼 수 있는 방법이며, OPT 알고리즘보다 페이지 교체 횟수가 높지만 FIFO 알고리즘 보다 효율적입니다.
    - LFU(least-frequently-used)
        참조 횟수가 가장 작은 페이지를 교체하는 알고리즘입니다. 만약 대상인 페이지가 여러 개 일 경우, LRU 알고리즘을 따라 가장 오래 사용되지 않은 페이지로 교체합니다.
    - MFU(most-frequently-used)
        LFU 알고리즘과 반대로, 참조 횟수가 가장 많은 페이지를 교체하는 알고리즘입니다.
    - LFU와 MFU는 실제 사용에 잘 쓰이지 않는다.
        구현에 상당한 비용이 들고,
        최적 페이지 교체 정책을 (LRU 만큼) 제대로 유사하게 구현해내지 못하기 때문이다.


- 멀티 쓰레드 프로그래밍을 개발할 때 주의해야 할 점
    - 공유 데이터를 사용하는 코드 영역인 임계구역에서 서로 다른 쓰레드가 간섭하지 않도록 쓰레드를 동기화 시켜 신뢰성 있는 데이터와 로직이
      산출(?)될 수 있게끔 코드를 작성해야합니다. 락을 거는 행위는 성능에 영향을 미치고 데드락을 유발할 수도 있으니 조심해야 합니다.

- 쓰레드를 구현하기 위한 인터페이스, 클래스
    Runnable 인터페이스를 사용하여 람다 혹은 내부클래스로 run() 메서드 구현
    새로운 클래스를 정의하고 Thread 클래스를 상속받은 후 run() 메서드 구현

    다른 클래스를 상속받지 않아도 될 때 => Thread 클래스 상속 후 구현
    Thread 클래스에 존재하는 다른 메소드들도 사용하고 싶을 때 => Thread 클래스 상속 후 구현
    그외는 Runnable 구현


- 스레드는 왜 써야하는 것일까요?
    https://cbw1030.tistory.com/282,  https://www.crocus.co.kr/1510,  https://itchipmunk.tistory.com/437,  http://blog.skby.net/%EC%93%B0%EB%A0%88%EB%93%9C-thread/
    https://hyunie-y.tistory.com/31
    - 프로세스
        - 프로세스란
            - 프로세스는 실행중인 프로그램(어플리케이션)이다. 즉, 메모리에 올라간 상태이다.
              실행파일을 클릭했을 때, 메모리(RAM)할당이 이루어지고, 이 메모리 공간으로 코드가 올라간다. 이 순간부터 이 프로그램은 '프로세스'라 불리게 된다.
            - 필요한 메모리 영역은 프로그램의 코드를 저장하는 Text 영역, 전역 정적 변수들을 저장하는 Data, 지역 변수들을 저장할 Stack, 동적 메모리 할당을 받을 Heap 영역입니다.
        - 프로세스의 스케줄링
            - CPU 는 하나인데, 동시에 여러 프로세스가 실행되어야한다. CPU 는 여러개의 프로세스를 번갈아가면서 실행하는데 매우 고속이기 때문에
              우리 눈에는 동시에 실행되는 것처럼 보인다. 이러한 멀티프로세스 운영체제에서 프로세스의 CPU 할당 순서 및 방법을 결정짓는 것을 '스케줄링'이라 한다.
        - 프로세스의 상태변화
            - 프로세스는 Ready, Running, Blocked 상태를 지닌다. Running 상태인 프로세스는 더 우선순위가 높은 프로세스가 실행될 경우
              Ready 상태가 되고, 우선순위가 높은 프로세스가 실행된다. Blocked 상태에 있는 프로세스는 스케줄러의 관심 밖에 있어서
              스케줄러에 의해 선택이 되지 않는 프로세스이다.
        - 컨텍스트 스위칭(Context Switching)
            - CPU 내에 존재하는 레지스터들은 현재 실행중인 프로세스 관련 데이터들로 채워진다.
              실행중인 프로세스가 변경이 되면, CPU 내 레지스터들의 값이 변경되어야 하는데, 변경되기 전에 이전 프로세스가 지니고 있던
              데이터들을 어딘가에 저장해 주어어야 한다.( 이어서 실행하기 위해 ). 그리고 새로 실행되는 프로세스가 아니라면 이전에 실행될 때
              레지스터들이 지니고 있던 데이터들을 불러와서 이어서 실행해야 한다. 이 과정이 컨텍스트 스위칭이다.
              실행되는 프로세스의 변경 과정에서 발생하는 컨텍스트 스위칭은 시스템에 많은 부담을 준다.
              레지스터 개수가 많을수록, 프로세스별로 관리되어야할 데이터 종류가 많을수록 더 부담이 된다.
              컨텍스트 스위칭에 소요되는 시간을 줄이려면 저장하고 복원하는 컨텍스트 정보의 개수를 줄여주면 된다.
            - 프로세스 상태를 저장하고 새로운 CPU 레지스터 변수들을 불러오는 작업을 컨텍스트 스위칭. 즉, CPU 에서 실행할 프로세스를 교체하는 기술.
              점유에서 벗어나는 순간 프로세스의 상태를 저장해야 하는데 PCB 라는 별도의 메모리 공간에 프로세스 상태값들을 저장.
              PCB 는 프로세스가 실행중인 상태를 스냅샷 찍어 저장하는 공간.
            - 컨텍스트 스위칭 작동 순서
                1. 스케줄러가 A 프로세스의 실행을 중단하고 B 프로세스를 실행할 것을 요청함.
                2. A 프로세스 정보들을 PCB 에 저장 (운영체제에서 관리)
                3. B 프로세스 정보들을 PCB 에서 불러와 덮어씌움.

        - 메모리 구조 관점에서 본 프로세스와 쓰레드
            - 자식 프로세스가 생성되고 난 다음에는 부모프로세스가 가진 핸들테이블은 상속되지만 모든것(Code/data/heap/stack 영역) 이
              독립적으로 만들어진다. 이러한 메모리 구조를 지녔기에, 프로세스간에 데이터를 주고받기 위해서는
              IPC(Inter Process Communication)가 필요하다. 하지만 쓰레드를 생성하는 경우 메모리 구조는 다르다.
              프로세스가 쓰레드 A와 B를 생성한 경우 쓰레드를 생성할 때마다, 해당 쓰레드만을 위한
              ThreadStack 영역(실행 흐름의 추가를 위한 최소조건)만이 생성될 뿐, 나머지 영역(Code, Data, Heap)은
              부모 프로세스 영역을 공유하고 있다. 쓰레드마다 스택을 독립적으로 할당해준다.

            - Data 영역과 Heap 영역을 공유하기에 다른 스레드와 데이터를 통신할 수 있지만, 어느 스레드가 먼저 실행할 지 모르기에 동기화와 교착 상태에 각별한 주의가 필요합니다.
    - 스레드
        - 쓰레드는 실행 흐름(절차)을 갖는 줄이다. 즉, 프로세스 내에서 실행 흐름을 의미한다. 프로세스 내의 명령어 블록으로 시작점과 종료점을 가짐.
        - 자바에서 스레드를 사용하는 이유
            - 동시에 두 가지 이상의 활동을 하기 위함이다.
            - 그리고 프로세스끼리는 정보를 주고 받을 수는 없지만, 다중 스레드 작업 시에 각 스레드끼리는 정보를 주고받을 수 있는 장점이 있다.
        - 스레드 사용 이유
            - 운영체제는 시스템 작업을 효율적으로 관리하기 위해 스레드를 이용한다.
            - 즉, 멀티 프로세스로 실행되는 작업을 멀티 스레드로 실행하게 되면 프로세스를 생성하여 자원을 할당하는 과정도 줄어들 뿐더러
              프로세스를 컨텍스트 스위칭(Context Switching)하는 것 보다 오버헤드를 더 줄일 수 있게 된다.
            - 뿐만 아니라 프로세스간의 통신 비용보다 하나의 프로세스 내에서 여러 스레드간의 통신 비용이 훨씬 적으므로 작업들 간의 통신 부담을 줄일 수 있게 된다.
            - 멀티스레드를 사용하는 큰 이유 중의 하나가 바로 다중 CPU 하드웨어를 충분히 활용하고자 하는 것이다.
            - 동시에 두 가지 이상의 활동을 하기 위함이다.
              프로세스끼리는 정보를 주고 받을 수 없지만, 다중 쓰레드 작업 시에 각 쓰레드끼리는 정보를 주고받을 수 있는 장점이 있다.
        - 스레드가 프로세스보다 안좋을 때
            - 멀티 프로세스 구조에서 여러 개의 자식 프로세스 중 하나에 문제가 발생하면 자식 프로세스 하나만 죽는다 해서 다른 곳에 영향을 끼치지는 않는다.
              하지만 멀티 스레드 구조에서 자식 스레드중 하나에 문제가 생긴 경우에는 전체 프로세스가 영향을 받게 된다.(ex : thread I/O)
              그리고 멀티 스레딩을 너무 자주 사용하게 된다면 컨텍스트 스위칭의 비용이 상당히 높기 때문에 오히려 시스템 성능 저하를 초래 할 수도 있고
              메모리가 공유되기 때문에 안정성 및 보안을 좀 더 추구하는 경우에는 멀티 스레드보단 멀티 프로세싱이 더 좋다.
            - 메모리 구분이 필요할 경우에는 멀티프로세스가 유리하고, 컨텍스트 스위칭이 자주 일어나고 데이터 공유가 빈번하면 멀티 스레드를 사용하는것이 유리합니다.
        - 특징  https://jungwoong.tistory.com/45, https://olivejua-develop.tistory.com/68
            - 메모리 영역을 공유를 하기 때문에 메모리 공간과 시스템 자원 소모가 줄어들게 되며,
              멀티 프로세스와 다르게 캐시메모리를 초기화할 필요가 없기 때문에 컨텍스트 스위칭 연산이 빠릅니다.
            - 스레드간의 자원을 공유하기 때문에 동기화 문제가 발생할 수 있기 때문에 주의깊은 설계가 필요합니다.
            - 하나의 스레드가 문제가 생기면 프로세스내의 다른 스레드에도 문제가 생길 수 있습니다.
            - 쓰레드가 프로세스보다 컨텍스트 스위칭이 빠른 이유는 메모리 영역을 공유하기 때문이다. 실제로 공유되는 데이터가 있고 아닌 데이터가 있다.
            - 프로세스에서 스레드가 생성되면 운영체제는 프로세스의 주소 공간에 스레드 스택으로 사용할 영역을 예약하고
              영역의 일부만 물리적 저장소에 커밋합니다. 일반적으로 스레드를 생성하면 1MB 주소 공간으로 설정됩니다.
              스택의 메모리 크기는 지정할 수 있습니다.
            - 어떤 스레드는 기본적으로 프로그램이 시작할 때 실행됩니다. 이 스레드가 바로 ’메인 스레드’입니다. 메인 스레드는 작업을
              처리하기 위해 새로운 스레드를 생성합니다. 이러한 새 스레드는 다른 스레드와 병렬로 실행되며,
              대개 실행이 완료되면 메인 스레드와 결과를 동기화합니다.
            - 멀티스레딩은 여러 코어에서 한 번에 여러 개의 스레드를 처리하는 CPU 성능을 활용하는 프로그래밍의 한 유형입니다.
            - 이러한 멀티스레딩 방식은 여러 개의 작업이 오랫동안 실행되는 경우에 적합합니다. 하지만 일반적으로 게임 개발 코드에는
              한 번에 실행해야 할 작은 명령이 많이 들어 있습니다. 각 명령에 대해 스레드를 만들면 그 수가 너무 많아지고 각각의 수명도 짧아집니다.
              따라서 CPU 및 운영체제의 프로세싱 능력을 초과할 수 있습니다.
            - 스레드 풀을 사용하면 스레드 수명 주기 문제를 완화할 수 있습니다. 하지만 스레드 풀을 사용해도 동시에 활성화된 스레드 수가
              너무 많을 수 있습니다. CPU 코어보다 스레드 수가 더 많으면 CPU 리소스를 놓고 스레드 간에 경쟁이 벌어지고,
              이로 인해 컨텍스트 스위칭이 빈번하게 발생합니다. 컨텍스트 스위칭은 실행 도중에 스레드 상태를 저장하고 다른 스레드에 대한
              작업을 진행한 후 첫 번째 스레드를 재구성하여 나중에 계속 처리하는 프로세스입니다. 컨텍스트 스위칭은 리소스를
              매우 많이 소모하므로 가급적 사용하지 않는 것이 좋습니다.
        - 컨텍스트 스위칭이 빨라진 쓰레드와 캐쉬 적중  https://agh2o.tistory.com/12
            - 쓰레드는 공유하는 영역이 많기 때문에 컨텍스트 스위칭이 빠르다.
              캐쉬는 CPU 와 메인메모리 사이에 위치하며 CPU 에서 한번 이상 읽어들인 메모리의 데이터를 저장하고 있다가,
              CPU 가 다시 그 메모리에 저장된 데이터를 요구할 때, 메인메모리를 통하지 않고 데이터를 전달해 주는 용도이다.
              프로세스 컨텍스트 스위칭이 일어났을 경우, 공유하는 데이터가 없으므로 캐쉬가 지금껏 쌓아놓은 데이터들이 무너지고
              새로 캐쉬정보를 쌓아야 한다. 이것이 프로세스 컨텍스트 스위칭에 부담이 되는 요소이다.
              반면, 쓰레드라면 저장된 캐쉬 데이터는 쓰레드가 바뀌어도 공유하는 데이터가 있으므로 의미있다. 그러므로 컨텍스트 스위칭이 빠른 것이다.

        - 운영체제나 JVM 이 CPU 를 많이 사용하면 할수록 실제 프로그램 스레드가 사용할 수 있는 CPU 양은 줄어든다.
          컨텍스트가 변경되면서 다른 스레드를 실행하려면 해당 스레드가 사용하던 데이터가 프로세서의 캐시 메모리에 들어 있지 않을 확률도 높다.
          그러면 캐시에서 찾지 못한 내용을 다른 저장소에서 찾아와야 하기 때문에 원래 예정된 것보다 느리게 실행되는 것이다.
          대기 중인 스레드가 밀려 있다고 해도, 현재 실행 중인 스레드에게 최소한의 실행 시간을 보장해주는 정책을 취하고 있다.
          대기 상태에 들어가는 연산을 많이 사용하는 프로그램(블로킹 I/O를 사용하거나, 락 대기 시간이 길거나, 상태 변수의 값을 기다리는 등)은
          CPU 를 주로 활용하는 프로그램보다 컨텍스트 스위칭 횟수가 훨씬 많아지고, 따라서 스케줄링 부하가 늘어나면서 전체적인 처리량이 줄어든다.
          (넌블록킹 알고리즘을 사용하면 컨텍스트 스위칭에 소모되는 부하를 줄일 수 있다.)
          컨텍스트 스위칭에 필요한 비용은 프로세서상에서 5,000 ~ 10,000 클럭 사이클 또는 수 마이크로초 동안 시간을 소모한다고 알려져 있다.

        - 메모리 동기화
          메모리 배리어는 캐시를 플러시하거나 무효화하고, 하드웨어와 관련된 쓰기 버퍼를 플러시하고, 실행 파이프라인을 늦출 수도 있다.
          멀티스레드를 사용하는 큰 이유 중의 하나가 바로 다중 CPU 하드웨어를 충분히 활용하고자 하는 것이다.
          암달의 법칙에 따르면 애플리케이션 확장성은 반드시 순차적으로 실행되야만 하는 코드가 전체에서 얼마만큼의 비율이 차지하냐에 달렸다고 한다.


- 프로그램 코드 실행을 하드웨어단에서 어떻게 하나요?
    - 프로그램을 실핼할 때, 적당한 메모리 위치에 프로그램이 쓸 영역을 올린다. 프로그램이 사용할 영역은 방금 말씀드렸다시피 코드를 저장하는 영역,
      전역 정적 변수들을 저장하는 영역, 지역 변수들을 저장하는 영역, 동적메모리 할당을 받을 영역으로 나누어져있습니다. 그 다음 코드를 해석하고 실행하는 CPU 이다.
      CPU 에서는 Program Counter 라는 레지스터 변수로 다음 실행할 위치의 코드를 저장한다. PC 위치에 있는 코드를 메모리부터 읽어오고
      명령어를 해석하여 적절한 행동을 실행한다. 다시 PC 에 다음 코드의 위치를 저장하고 프로그램이 종료될 때까지 반복한다.

    - 현재 프로그램 코드에서 다른 프로그램 코드 영역으로 옮길 때, 현재 프로세스의 레지스터 정보와 PCB 를 저장합니다.
      운영체제 내에서 레지스터 정보와 PCB 를 저장하는 테이블에 등록합니다. 그리고 다른 프로세스의 정보를 탐색해 불러옵니다. 다시 원래 프로세스로 돌아올 때 저장했던 테이블에서 탐색해 불러옵니다.


- 프로그램을 메모리에 올릴 때, 적당한 메모리 공간의 위치를 어떻게 효율적으로 정할 수 있을까요?
    - 메모리에 필요한 공간을 할당할 때, 메모리의 빈 공간을 탐색해서 할당합니다.
      모든 프로그램마다 할당받을 메모리의 크기도 모두 다르기에, 무턱대고 할당하고 해제하기를 반복한다면
      메모리의 빈 공간을 탐색하는데 시간이 많이 들 뿐 더러, 불필요하게 빈 메모리가 많이 생길 것 입니다.

      효율적인 메모리 관리 기법으로 할당할 메모리 주소의 탐색 시간을 줄이고 빈 메모리 공간을 줄입니다.
      이를 관리하는 하나의 하드웨어 단위가 있습니다. 바로 MMU (Memory Management Unit) 입니다.

      최초의 메모리 할당 방법은 연속적으로 할당하는 방법입니다. MMU에서 Offset 레지스터로 CPU가 읽은 가상 주소로부터 물리 주소에 매핑을 시켜주는 역할을 합니다.
      현재 프로그램 영역을 벗어나지 않도록 Limit 레지스터로 주소 접근을 제한하기도 합니다. 만약에 제한을 넘어갔을 때 인터럽트로 시스템에게 예외를 처리하도록 부탁합니다.
      연속적 할당 방법의 장점은 구현이 간단합니다. 그러나 초반에 말했던 단점들은 고스란히 남아있습니다.
      할당과 해제의 과정을 거치면서 메모리 중간에 빈 공간들이 생겨나는데요.

      빈 공간들을 모조리 합치면 새로 들어올 프로그램의 메모리를 옮길 수 있지만,
      연속적으로 할당한다는 특징으로 할당이 불가능합니다.
      총 여유 메모리로 충분히 할당하고도 남지만, 실제로 할당할 수 없는 경우를 외부 단편화(External Fragmentation) 라고 합니다.
      메모리 공간의 빈 공간들을 모두 없애 앞쪽으로 땡기는 Compact 기법이 있습니다. 하나 하나 메모리 영역을 복사하여 빈 공간이 없도록 반복하게 되는데, I/O 문제가 발생할 수 밖에 없습니다.

      이를 해결할 방법은 메모리 영역을 쪼개는 방식입니다. 이를 페이징(Paging) 방식이라고 합니다.
      할당 받은 프로그램의 물리 메모리를 특정 크기의 프레임으로 쪼개 순서에 상관없이 저장합니다.
      그렇다면, 각 프로세스는 어떻게 순서대로 프로그램을 실행할 수 있을까요?
      프로세스 별로 페이지 테이블을 가지고 있습니다. 논리적인 페이징 테이블로 해당하는 실제 메모리 주소에 있는 프레임을 매핑하여 위치를 찾아냅니다.

      페이징 방식의 장점으로 프로그램 메모리를 잘개 쪼개 효율적으로 메모리에 저장하여 외부 단편화를 해소한다는 점이고요. 페이징 방식의 단점으로는 특정 크기의 프레임으로 메모리 영역을 분리하는데요.
      만약에 메모리 크기가 딱 맞지 않다면은 여전히 빈 공간이 생깁니다.
      이를 내부 단편화(Internal Fragmentation)라고 합니다.
      프레임 크기를 줄일수록 페이지 테이블이 커지고 그에 따른 오버헤드는 훨씬 빈번히 발생할 것 입니다.

      물리 메모리에 접근할 때 늘 페이지 테이블에 접근해서 매핑된 물리 메모리에, 총 두 번 접근합니다.
      이를 개선하기 위해 연관 메모리인 TLB(Translation Look-aside Buffer, 변환 색인 버퍼)를 사용합니다.
      페이지 테이블을 대상으로 일종의 캐시 역할을 해줍니다.

      더 나아가 메모리 크기가 커지면서 페이지 테이블을 여러 단계로 계층화 시킨다든지, 해쉬 테이블을 이용한다든지, 하나의 페이지 테이블로 통합시킨다든지 하는 방법들이 나왔습니다.
      자세히는 모르지만 현대에 와서는 멀티 코어 시대인만큼 병렬화를 적극 이용하지 않았을까 싶습니다.


- 프로그램 코드를 읽어올 때 CPU 에서 어떻게 효과적으로 읽을 수 있는지 생각해보셨나요?
    - CPU 와 메모리의 연산 속도는 다르기에 CPU 에서도 효과적으로 메모리로부터 데이터를 읽는 방법이 필요합니다. CPU 내부에 여러 캐시 메모리를 장착하고 있어
      최대한 CPU 사이클보다 느린 메모리로부터 데이터나 코드를 불러오는 것을 지양하면서 캐시 메모리를 적극 활용해 속도를 개선합니다.
      코드를 읽을 때, 메모리에서 다음에 실행할 코드 뭉치들을 미리 불러와 캐시 메모리에 저장합니다.
      캐시 메모리에 저장한 명렁어들을 소진될 때 까지 순차적으로 실행해 속도를 개선합니다.


- 스레드 동기화는 어떻게 이루어지나요?
    - 스레드 간에 객체를 공유하여 수정할 시, 다른 스레드에도 그 정보가 반영되지 않아 예상치 못한 오류가 일어날 수 있습니다.
      멀티 스레드 환경에서 단 하나의 스레드만 실행할 수 있는 코드 영역을 임계 영역이라고 합니다.
      자바에서는 이를 위해 동기화(synchronized) 메소드와 동기화 블록을 제공합니다.
      스레드 내부의 동기화 메소드 또는 블록에 들어가면 즉시 객체에 잠금을 걸어 다른 스레드가 해당 임계 영역 코드를 실행하지 못하도록 합니다.


- 자바에서 스레드의 상태를 제어하는 방법을 알려주세요.
      스레드는 다음과 같은 상태를 가집니다, NEW, RUNNABLE, TIMED_WAITING, BLOCKED, TERMINATED.
      스레드 객체를 생성하고 start() 메소드가 호출되지 않은 상태일 때 NEW 상태입니다.
      RUNNABLE 은 실행 상태로 언제든지 갈 수 있는 상태입니다.
      WAITING 은 다른 스레드가 notify 해줄 때 까지 기다리는 상태입니다.
      TIMED_WAITING 은 주어진 시간 동안 기다리는 상태입니다.
      BLOCKED 는 사용하고자 하는 객체의 락이 풀릴 때 까지 기다리는 상태입니다.
      TERMINATED 는 실행을 마친 상태입니다.

      1. 쓰레드를 생성하고 start()를 호출하면 바로 실행되는 것이 아니라 실행대기열에 저장되어 자신의 차례가 될 때까지 기다려야 합니다. (실행 대기열은 큐와 같은 구조로 먼저 실행대기열에 들어온 쓰레드가 먼저 실행됩니다.)
      2. 자기 차례가 되면 실행상태가 됩니다.
      3. 할당된 실행시간이 다되거나 yield()메소드를 만나면 다시 실행 대기상태가 되고 다음 쓰레드가 실행됩니다.
      4. 실행 중에 suspend(), sleep(), wait(), join(), I/O block에 의해 일시정지상태가 될 수 있습니다.
          ※ I/O Block은 입출력 작업에서 발생하는 지연상태를 말합니다. 사용자의 입력을 받는 경우를 예로 들 수 있습니다.
      5. 지정된 일시정지시간이 다되거나, notify(), resume(), interrupt()가 호출되면 일시정지상태를 벗어나
         다시 실행 대기열에 저장되어 자신의 차례를 기다리게 됩니다.
      6. 실행을 모두 마치거나 stop()이 호출되면 쓰레드를 소멸됩니다.

      스레드의 wait() 메소드는 동기화 블록 내에서 스레드를 WAITING 상태로 만듭니다. 매개값으로 주어진 시간이 지나면 자동적으로 RUNNABLE 상태가 된다.
      주어지지 않으면, notify(), notifyAll() 메소드로 RUNNABLE 상태로 이동한다.
      notify(), notifyAll() 메소드는 동기화 블록 내에서 wait() 메소드에 의해 WAITING 상태에 있는 스레드들을 RUNNABLE 상태로 만든다.
      join() 메소드를 호출한 스레드는 WAITING 상태로 만든다. RUNNABLE 상태로 가기 위해서 join() 메소드를 호출 받은 스레드가 종료되거나, 매개값으로 주어진 시간이 지나야 한다.


- Blocking I/O, Non-Blocking I/O(NIO)와 대용량 트레픽
    https://junghyungil.tistory.com/131
    https://m.blog.naver.com/PostView.nhn?blogId=joebak&logNo=220063974083&proxyReferer=https:%2F%2Fwww.google.com%2F
    https://helloinyong.tistory.com/293
    - Thread 수는 접속자 수가 많아질 수록 Thread 수도 많아지게 된다. Thread 가 많으면 CPU 의 Context Switching 및 interrupt 횟수와 오버헤드 증가하게 된다.
      이러한 것들이 발생할 때 cpu 는 일하지 못한다. 때문에, 실제 작업하는 양에 비하여 훨씬 비효율적으로 동작하게 될 것이다. 즉, 성능에 악영향을 줄 수 있다.
      그래서 NIO 는 Non-Blocking 방식으로 이 문제를 해결하였다

    - Non-Blocking I/O은 I/O 작업을 진행하는 동안 쓰레드의 작업을 중단시키지 않는다. 쓰레드가 커널에게 I/O를 요청하는 함수를 호출하면,
      함수는 I/O를 요청한 다음 진행상황과 상관없이 바로 결과를 반환한다.
      I/O는 스트림으로 단 반향으로만 가능하지만, NIO 는 Channels 과 Buffers 를 이용해 양방향으로 가능하다. 또 Selectors 가 있다.

    - Channels
        - 채널은 항상 데이터를 버퍼로 읽고 버퍼는 데이터를 채널에 쓴다.
            채널을 읽고 쓸 수 있다. 스트림은 일반적으로 단방향 (읽기 또는 쓰기)이다.
            채널은 비동기 적으로 읽고 쓸 수 있다. ( ServerSocketChannel 이나 SocketChannel 의 경우는 Selector 를 활용해 Non-Blocking 프로그래밍이 가능하다.)
            채널은 항상 버퍼에서 읽거나 버퍼에서 쓴다.
            채널에서 버퍼로 데이터를 읽고 버퍼에서 채널로 데이터를 쓴다.

    - Buffers
        - Java NIO 버퍼는 NIO 채널과 상호 작용할 때 사용된다. 버퍼는 기본적으로 데이터를 쓸 수있는 메모리 블록이며 나중에 다시 읽을 수 있다.
          이 메모리 블록은 NIO Buffer 객체로 래핑되어 메모리 블록으로 작업하기 쉽게하는 일련의 메서드를 제공한다.

    - Channels 과 Buffers 를 이용한 Non-Blocking I/O - NIO
        - 자바 NIO 에서는 non-blocking IO를 사용할 수 있다.
        - 하나의 스레드는 버퍼에 데이터를 읽도록 채널에 요청할 수 있다.
          채널이 버퍼로 데이터를 읽는 동안 스레드는 다른 작업을 수행할 수 있다.
          데이터가 채널에서 버퍼로 읽어지면, 스레드는 해당 버퍼를 이용한 processing(처리)를 계속 할 수 있다.
          데이터를 채널에 쓰는 경우도 non-blocking 이 가능하다.

    - Selectors
        - 셀렉터를 사용하면 하나의 스레드가 여러 채널을 처리(handle)할 수 있다.
        - 셀렉터는 사용을 위해 하나 이상의 채널을 셀렉터에 등록하고 select() 메서드를 호출해 등록 된 채널 중 이벤트 준비가 완료된 하나 이상의 채널이 생길 때까지 봉쇄(block)된다.
        - 메서드가 반환(return)되면 스레드는 채널에 준비 완료된 이벤트를 처리할 수 있다.
          즉, 하나의 스레드에서 여러 채널을 관리할 수 있으므로 여러 네트워크 연결을 관리할 수 있다. (SocketChannel, ServerSocketChannel)

    - 커널 수준의 쓰레드 vs  사용자 수준의 쓰레드
        - 커널 수준의 쓰레드
            - 커널 수준 스레드는 커널 레벨에서 생성되는 스레드이다. 운영체제 시스템 내에서 생성되어 동작하는 스레드로, 커널이 직접 관리한다.
            - 그런데 커널 수준에서는 프로세스가 주기억 장치에 여러 개가 적재되어 CPU 할당을 기다리며 동작한다.
            - CPU 에서 인터럽트 발생으로 현재 작업 중인 프로세스가 Block 되고 다른 프로세스로 변경할 때,
              CPU 내 재배치 레지스터에 다음에 실행할 프로세스 정보들로 교체를 하고 캐시를 비운다. 이것을 컨텍스트 스위칭이라고 한다.
            - 이 컨텍스트 스위칭이 일어날 때는 CPU 가 일을 못한다. 그래서 이게 자주 일어나면 성능에 영향이 발생하게 되는 단점이 있다.
            - 하지만 커널이 직접 관리하므로 특정 스레드가 Block 이 되어도 다른 스레드들은 독립적으로 일을 할 수 있다.

        - 사용자 수준의 쓰레드
            - 스레드를 관리하는 라이브러리로 인해 사용자 단에서 생성 및 관리되는 스레드
            - 커널이 따로 관리하지 않고, 커널이 이 스레드에 대해서 알지도 못한다. (한 마디로 커널 레벨 밖에 있는 스레드)
            - 여기의 스레드는 운영체제 단의 기능을 하는 것이 아니라, 개발자가 기능 구현할 때 현재 기능 내에서 일 처리를 하는 스레드를 만들 듯이,
              프로세스 내 커널과 관련 없는 기능들만 수행하는 스레드이다. 그래서 커널이랑 관련도 없고 커널은 이런 스레드들이 있는지도 모른다.
            - 그렇기에 사용자 수준 스레드는 컨텍스트 스위칭이 없는 것이고, 스레드 교체 등으로 인한 오버헤드 발생이 없는 것이다.
            - 쓰레드 패키지를 사용자 영역에 두고 운영체제 커널은 단일 프로세스만을 관리한다.
            - 쓰레드 패키지를 런타임 시스템에서 사용한다.
            - 운영체제를 사용하는 입장에서는 런타임 시스템도 하나의 프로세스로 인식한다.
            - 쓰레드를 운영하지 않는 운영체제제에서 실행할 수 있으므로 이식성이 뛰어나다.
            - 즉, 입출력 인터럽트가 발생하면 커널은 '사용자 모드'가 되어서 사용자 수준 스레드의 응답을 기다린다. 사용자 수준 스레드의 응답이 오면 다시 '커널 모드'로 변환되어 이어서 커널 스레드가 일 처리를 하게 되는 것이다.
            - 컨텍스트 스위칭이 발생하지 않는다.

            - 프로세스 내에서 스레드들끼리는 자원을 공유하여 일 처리를 하기 때문에(스레드 통신 기법), 커널이 관리하는 스레드는 스레드 동기화 기법으로 자원 관리를 할 수 있으므로,
              하나의 스레드가 Bloc 이 되어도 다른 스레드들은 커널 관리로 인해 계속해서 동기화가 되어 일 처리가 가능하다.
              하지만 사용자 수준 스레드는 커널의 관리를 받지 않음으로, Block 으로 인해 공유 자원의 무결성에 대한 문제가 발생할 수 있으므로
              하나의 스레드가 인터럽트 당하면 모든 스레드가 멈추도록 하는 것 아닐까 추측한다.

        -> Blocking I/O가 커널 수준의 쓰레드, Non-Blocking NIO 가 사용자 수준의 쓰레드

    -> 정리
        - Blocking I/O는 하나의 호출마다 Thread 를 생성한다. 그에 따른 컨텍스트 스위칭이 발생하기 때문에 성능상 단점이 있다.
        - Non-Blocking IO는 요청을 받는 Thread 는 오직 하나다. Thread 내부에서 채널과 버퍼를 이용하여 Non-Blocking 방식으로 진행한다. 그래서 컨텍스트 스위칭이 발생하지 않는다.
          Context-switching 은 OS 단에서 처리하는데 이것을 사용자(개발자)가 직접 처리한다는 개념에서 최적화 시킬 수 있다는 장점이 있다.
          직접 처리한다는 것은 단일 쓰레드의 내부로 NIO 의 채널과 버퍼를 이용해서
        - 하지만 요청이 적다면, Blocking I/O가 더 좋다. 호출마다 thread 를 생성하니 요청이 적은 서비스에는 최적의 성능을 낼 수 있다.
          cpu 코어 갯수많큼 쓰레드를 생성하는게 최적이다. (병렬 작업의 장점)
        - I/O의 요청이 많아질 때 생기는 성능상의 이유로, 많은 요청을 해결하기 위해 자바 1.4에서 NIO 가 나왔고, NIO 는 대용량 트레픽 처리를 위해 꼭 알아야 하는 개념


- Blocking IO와 Non-Blocking IO 의 차이를 말씀해주세요.
    https://junghyungil.tistory.com/131
    https://velog.io/@octo__/BlockingNon-Blocking-IO-IO-%EC%9D%B4%EB%B2%A4%ED%8A%B8-%ED%86%B5%EC%A7%80-%EB%AA%A8%EB%8D%B8
    https://luv-n-interest.tistory.com/1121

    - 동기
        - I/O 작업은 user space 에서 직접 수행할 수 없기 때문에 user process 가 kernel 에 I/O 작업을 요청하고 응답을 받는 구조.
          응답을 어떤 순서로 받는지 (synchronous, asynchronous)
          어떤 타이밍에 받는지 (blocking, non-blocking)
          에 따라 여러 모델로 분류되는 것이다.

        - 동기 작업이란 한 번에 하나씩 수행되는 것을 의미한다. 즉, 해당 작업이 끝나기 전까지는 현재 진행중인 작업 외의 다른 작업을 수행하지 못함.
          HTTP 요청은 요청을 하면 무조건 응답을 받는다. 이것이 동기적이라고 말할 수 있다.

        - 모든 I/O 요청-응답 작업이 일련의 순서를 따르는 것이다. 즉, 작업의 순서가 보장된다.
          작업 완료를 user space 에서 판단하고 다음 작업을 언제 요청할지 결정한다. 일련의 Pipeline 을 준수하는 구조에서 효율적이다.
          작업의 순서를 보장한다는 말은 '현재 작업의 응답'을 받는 시점과 '다음 작업을 요청'하는 시점을 맞추는 일이다.
          다음 작업이 있다는 것 자체가 순서가 있다는 것을 의미하며 결국 이전 작업이 완료되기 전까지 다음 작업이 수행되지 않는 것이다.

    - 비동기
        - 비동기 작업이란 한 번에 하나 이상이 수행될 수 있음을 의미. 즉, 형재 작업을 진행중이더라도 다른 작업을 수행할 수 있다.
          또한 작업에 대한 결과를 바로 원하지 않는다.
          ex) 내가 이메일을 보내는 작업은 상대방이 바로 답장하기를 원해서 보내는 작업은 아니다.

        - kernel 에 I/O 작업을 요청해두고 다른 작업 처리가 가능하나, 작업의 순서는 보장되지 않는다.
          작업 완료를 kernel space 에서 통보해준다.
          각 작업들이 독립적이거나, 작업 별 지연이 큰 경우 효율적이다.

        - 만약 다중 요청에 대한 처리와 동시성 처리를 잘할 수 있다면 비동기 작업의 속도가 더 빠를 것이다.
          하지만 서로 의존성이 있는 곳에서 작업을 한다면 비동기는 오히려 오류를 불러올 가능성이 높다.
          때문에 비동기 프로그래밍을 할 때는 서로 독릭접인 일에 대해서 수행하는 것이 좋다.

    - 블로킹
        - Thread 가 Blocking 이 된다는 것은 CPU 가 점유되어 실행되지 못함을 의미.
          요청한 작업이 모두 완료될 때까지 기다렸다가 완료될 때 응답과 결과를 반환 (대기 O)
          CPU 를 점유한다고 사용한다는 것은 아니다.

    - 논블로킹
        - System call 을 받았을 때 제어권을 바로 자신을 호출한 쪽으로 넘기며 자신을 호출한 쪽에서 다른 작업을 할 수 있도록 하는 것을 의미.
          자신은 작업을 그대로 이어 나간다.
          Thread 가 Waiting 하지 않으므로 CPU 제어는 그대로다.

        - 작업 요청 이후 결과는 나중에 필요할 때 전달받는다. (대기 X)
          요청한 작업 결과를 기다리지 않는다. (CPU 점유 X)
          중간중간 필요하면 상태 확인은 해볼 수 있다. (polling)

    * 동기 쓰레드는 작업을 끝내야 다른 작업을 진행할 수 있다. (블로킹과 비슷하지만 Waiting Queue 에 들어가지 않아도 된다.)
      블로킹 쓰레드는 Waiting Queue 에 들어가게 된다.
      논블로킹 쓰레드는 작업이 끝나든 말든 다른 작업을 이어서 한다. (Waiting Queue 에 들어가지 않는다.)
      비동기 쓰레드가 작업 중이면 다른 쓰레드에서 작업을 하는 것이다.

      쓰레드가 블로킹으로 작업을 수행한다면 해당 쓰레드는 Waiting Queue 로 들어가고 Waiting Time 동안 해당 쓰레드를 비동기적으로 수행시킬 수 있다.
      쓰레드가 논블로킹으로 작업을 수행한다면 쓰레드가 CPU 를 계속 점유한 채로 수행된다.
      동기/비동기는 위에서 해당 쓰레드가 하는 일에 대해서 Sequential 하게 수행할 것인가 아니면 Simultaneous 하게 할 것인가

    * 왜 블로킹을 하고 안하고 정해야할까?
        - 블로킹을 하는 이유는 해당 작업이 끝나지 않은 채로 다른 작업을 진행한다고 하자.
          뒤따라온 작업이 앞선 작업의 결과에 의존적인 작업이라면 예상치 못한 결과가 나올 수 있다.


    - Blocking IO 가 일어나면 스레드에는 무슨 일이 생길까요?
        - I/O 작업이 blocking 방식으로 구현되면 하나의 클라이언트가 I/O 작업을 진행하면 해당 쓰레드가 진행하는 작업을 중지하게 된다.
          영향을 미치지 않게 하기 위해서 클라이언트 별로 Thread 를 만들어 연결시켜주어야 한다. Thread 가 많아지면 시간 할당량은 작아진다.
          시간할당량이 작으면 동시에 수행되는 느낌을 가질 수 있다.
        - Thread 수는 접속자 수가 많아질 수록 Thread 수도 많아지게 된다. Thread 가 많으면 CPU 의 Context Switching 및
          interrupt 횟수와 오버헤드 증가하게 된다. 이러한 것들이 발생할 때 cpu 는 일하지 못한다. 때문에, 실제 작업하는 양에 비하여 훨씬
          비효율적으로 동작하게 될 것이다. 즉, 성능에 악영향을 줄 수 있다.
        - Non-Blocking 방식으로 이 문제를 해결
        - Non-Blocking I/O은 I/O 작업을 진행하는 동안 쓰레드의 작업을 중단시키지 않는다. 쓰레드가 커널에게 I/O를 요청하는 함수를 호출하면,
          함수는 I/O를 요청한 다음 진행상황과 상관없이 바로 결과를 반환한다.
        - I/O는 스트림으로 단 반향으로만 가능하지만, NIO 는 Channels 과 Buffers 를 이용해 양방향으로 가능하다. 또 Selectors 가 있다.
            - Channels
                - 일반적인 NIO 의 I/O는 채널에서 시작된다. Java NIO 채널은 몇 가지 차이점을 제외하고 스트림과 유사하다.
                - 채널은 항상 데이터를 버퍼로 읽고 버퍼는 데이터를 채널에 쓴다.
                - 채널을 읽고 쓸 수 있다. 스트림은 일반적으로 단방향 (읽기 또는 쓰기)이다.
                - 채널은 비동기 적으로 읽고 쓸 수 있다. ( ServerSocketChannel 이나 SocketChannel 의 경우는 Selector 를 활용해 Non-Blocking 프로그래밍이 가능하다.)
            - Buffers
                - Java NIO 버퍼는 NIO 채널과 상호 작용할 때 사용된다. 버퍼는 기본적으로 데이터를 쓸 수있는 메모리 블록이며
                  나중에 다시 읽을 수 있다. 이 메모리 블록은 NIO Buffer 객체로 래핑되어 메모리 블록으로 작업하기 쉽게하는 일련의 메서드를 제공한다.
            - Channels 과 Buffers 를 이용한 Non-Blocking I/O - NIO
                - 자바 NIO 에서는 non-blocking IO를 사용할 수 있다. 예를 들면,
                  하나의 스레드는 버퍼에 데이터를 읽도록 채널에 요청할 수 있다.
                  채널이 버퍼로 데이터를 읽는 동안 스레드는 다른 작업을 수행할 수 있다.
                  데이터가 채널에서 버퍼로 읽어지면, 스레드는 해당 버퍼를 이용한 processing(처리)를 계속 할 수 있다.
                  데이터를 채널에 쓰는 경우도 non-blocking 이 가능하다.
            - Selectors
                - 셀렉터를 사용하면 하나의 스레드가 여러 채널을 처리(handle)할 수 있다.
                - 셀렉터는 사용을 위해 하나 이상의 채널을 셀렉터에 등록하고 select() 메서드를 호출해 등록 된 채널 중 이벤트 준비가 완료된 하나 이상의 채널이 생길 때까지 봉쇄(block)된다.
                - 메서드가 반환(return)되면 스레드는 채널에 준비 완료된 이벤트를 처리할 수 있다.
                  즉, 하나의 스레드에서 여러 채널을 관리할 수 있으므로 여러 네트워크 연결을 관리할 수 있다. (SocketChannel, ServerSocketChannel)
        - 커널 수준의 쓰레드 vs 사용자 수준의 쓰레드
            - 커널 수준의 쓰레드
                - 커널 수준 스레드는 커널 레벨에서 생성되는 스레드이다.
                - 운영체제 시스템 내에서 생성되어 동작하는 스레드로, 커널이 직접 관리한다.
                - 그런데 커널 수준에서는 프로세스가 주기억 장치에 여러 개가 적재되어 CPU 할당을 기다리며 동작한다.
                - CPU 에서 인터럽트 발생으로 현재 작업 중인 프로세스가 Block 되고 다른 프로세스로 변경할 때,
                  CPU 내 재배치 레지스터에 다음에 실행할 프로세스 정보들로 교체를 하고 캐시를 비운다. 이것을 컨텍스트 스위칭이라고 한다.
                - 이 컨텍스트 스위칭이 일어날 때는 CPU 가 일을 못한다. 그래서 이게 자주 일어나면 성능에 영향이 발생하게 되는 단점이 있다.
                - 하지만 커널이 직접 관리하므로 특정 스레드가 Block 이 되어도 다른 스레드들은 독립적으로 일을 할 수 있다.
            - 사용자 수준의 쓰레드
                - 쓰레드 패키지를 사용자 영역에 두고 운영체제 커널은 단일 프로세스만을 관리한다.
                - 쓰레드 패키지를 런타임 시스템에서 사용한다.
                - 운영체제를 사용하는 입장에서는 런타임 시스템도 하나의 프로세스로 인식한다.
                - 쓰레드를 운영하지 않는 운영체제제에서 실행할 수 있으므로 이식성이 뛰어나다.
                - 즉, 입출력 인터럽트가 발생하면 커널은 '사용자 모드'가 되어서 사용자 수준 스레드의 응답을 기다린다.
                  사용자 수준 스레드의 응답이 오면 다시 '커널 모드'로 변환되어 이어서 커널 스레드가 일 처리를 하게 되는 것이다.
                - 컨텍스트 스위칭이 발생하지 않는다.
            -> Blocking I/O가 커널 수준의 쓰레드, Non-Blocking NIO 가 사용자 수준의 쓰레드라고 보면 될 것 같다.
        - 정리
            - Blocking I/O는 하나의 호출마다 Thread 를 생성한다. 그에 따른 컨텍스트 스위칭이 발생하기 때문에 성능상 단점이 있다.
            - Non-Blocking IO는 요청을 받는 Thread 는 오직 하나다. Thread 내부에서 채널과 버퍼를 이용하여 Non-Blocking 방식으로 진행한다.
              그래서 컨텍스트 스위칭이 발생하지 않는다.
            - 위에서 말한 사용자 수준 스레드 관점에서 봤을 때, Context-switching 은 OS 단에서 처리하는데 이것을 사용자(개발자)가
              직접 처리한다는 개념에서 최적화 시킬 수 있다는 장점이 있다. 직접 처리한다는 것은 단일 쓰레드의 내부로 NIO 의 채널과 버퍼를 이용해서
            - 하지만 요청이 적다면, Blocking I/O가 더 좋다. 호출마다 thread 를 생성하니 요청이 적은 서비스에는 최적의 성능을 낼 수 있다.
              cpu 코어 갯수많큼 쓰레드를 생성하는게 최적이다. (병렬 작업의 장점)
            - I/O의 요청이 많아질 때 생기는 성능상의 이유로, 많은 요청을 해결하기 위해 자바 1.4에서 NIO 가 나왔고,
              NIO 는 대용량 트레픽 처리를 위해 꼭 알아야 하는 개념

        - NIO  https://deftkang.tistory.com/24
            일반적으로 네트워크의 속도는 컴퓨터의 CPU, 메모리, 심지어 디스크의 속도와 비교해도 매우 느리다. 이러한 상황에서 나오는 현상중
            상대적으로 느린 네트워크를 엄청나게 빠른 CPU 가 기다리는 것이다. CPU 가 느린 네트워크를 기다리지 않고 네트워크보다 앞서
            달리게 하기 위한 전통적인 자바의 해결 방안은 버퍼링과 멀티스레드를 결합하는 것이다. 다수의 스레드가 동시에 다수의
            서로 다른 연결을 통해 보낼 데이터를 생성한다. 그리고 네트워크가 데이터를 보낼 준비가 될 때까지 해당 데이터들을 버퍼에 저장해 둔다.
            그러나 멀티 스레드를 생성할 때 드는 오버헤드와 스레드 전환 시 발생하는 오버헤드를 무시할 수 없다. 그리고 각각의 스레드는
            약 1메가 바이트의 메모리리를 여분으로 필요로 한다. 초당 수천 개의 요청을 처리하는 대규모 서버 환경에서는 스레드가 사용하는
            여분의 메모리와 다양한 오버헤드로 인해 연결마다 스레드를 할당하는 것이 쉽지 않다. 그래서 하나의 스레드가 다수의 연결을 담당하고,
            데이터를 수신할 준비가 된 연결을 골라내서 처리하고, 그리고 다시 준비된 다음 연결을 골라내는 방법을 반복한다면 훨씬 더 빠를 것이다.
            이러한 기능을 java.nio 패키지 에서 제공한다.
        - IO와 NIO 의 차이점
            - IO는 입력 스트림과 출력 스트림이 구분 되어 입/출력 별도의 생성이 필요하다.  또 동기(synchronous) 방식이기 때문에
              입력과 출력이 다 될때까지 스레드는 멈춰 있어야 한다. 이것을 블로킹이라고 하는데 interrupt 로 블로킹(Blocking)을
              빠져 나올 수 없다. 블로킹을 빠져 나오는 유일한 방법은 스트림을 닫는 것이다. 반면 NIO 는 양방향 입출력이 가능하므로
              하나만 생성하면 된다. 또 NIO 는 블로킹(Blocking)과 넌블로킹(Non-Blocking)을 지원하는데 넌블로킹 방식으로 입출력 작업시
              스레드가 블로킹 되지 않는다. 또 NIO 블로킹은 스레드 interrupt 로 인해 빠져 나오는 것이 가능하다.
            - IO
                - 스트림 방식
                - 넌버퍼
                - 동기 방식
                - 블로킹 방식
            - NIO
                - 채널 방식
                - 버퍼
                - 동기/비동기 방식 모두 지원
                - 블로킹/넌블로킹 방식 모두 지원
        - NIO 패키지 핵심개념
            1. 채널(Channel)
                - 채널은 파일, 소켓, 데이터그램 등과 같은 다양한 I/O 소스로부터 데이터 블록을 버퍼로 쓰거나 읽어 온다.
                - 비동기 읽기/쓰기를 지원하는 스트림과 같은 기술이다.
            2. 버퍼(Buffer)
                - nio(new input/output) 에서는 모든 I/O가 버퍼링된다. 게다가 버퍼는 새로운 I/O API의 기초를 이루고 있다.
                - 데이터를 입출력 스트림으로 쓰거나 읽지 않고 대신 버퍼에 일시적으로 저장하여 쓰고 읽는다.
                - 채널과 함께 동작한다.
            3. 셀렉터(Selector)
                - 싱글 스레드에서 다중채널을 처리 하기 위한 기술이다. 즉 준비된 채널을 선택함으로써 읽고 쓸 때 블록하지 않아도 된다.
                - 애플리케이션에서 싱글 스레드로 low-traffic 연결을 처리하는 경우 유용하다.

    - 스레드가 멈춰있는 동안 CPU 는 어떻게 될까요?
    - CPU 가 쉬는 것을 막으려면 어떻게 해야할까요?
    - 스레드를 늘리면 단점이 무엇일까요?
        - 이 것이 Tomcat 이 스레드를 수 백개씩 띄우는 이유입니다.
          Tomcat 은 일반적으로 Blocking 방식이기 때문에 스레드 1개당 요청 1개를 처리할 수 밖에 없습니다.
    - Non-Blocking IO는 CPU 활용률이 어떨까요?
        - Spring WebFlux 와 Netty
            - Spring WebFlux  https://alwayspr.tistory.com/44
                - I/O
                    - 사용자가 I/O 요청을 할 때 CPU 가 I/O Controller 에 요청을 하고 I/O Controller 가 파일을 다 가져오면 그것을
                      Memory 에 적재시키고 CPU 에게 완료되었다고 알려준다. 즉 큰 그림은 CPU -> I/O Controller -> CPU 의 형태이다.
                      핵심은 CPU 가 I/O를 직접 가져오는 것이 아니라, 작은 CPU 라고 불리는 I/O Controller 가 수행한다는 이야기이다.
                      좀 더 나아가면 작업을 단순히 위임시키고 작업이 완료되는 동안에는 다른 일을 수행할 수 있다는 말이다.
                      이러한 예처럼 I/O를 처리하는데 몇 가지 방법이 있다.
                - Blocking I/O
                    - Application 에서 I/O 요청을 한 후 완료되기 전까지는 Application 이 Block 이 되어 다른 작업을 수행할 수 없다.
                      이는 해당 자원이 효율적으로 사용되지 못하고 있음을 의미한다.
                    - 그러나 생각을 해보면 여러분들의 Application 들은 Blocking 방식임에도 불구하고 마치 Block 이 안된듯이
                      동작하는 것처럼 보인다. 이것은 여러분들이 Single Thread 를 기반으로 하는 것이 아닌 Multi Thread 를 기반으로
                      동작하기 때문이다. Block 되는 순간 다른 Thread 가 동작함으로써 Block 의 문제를 해소하였다.
                      그러나 Thread 간의 전환(Context Switching)에 드는 비용이 존재하므로 여러 개의 I/O를 처리하기 위해
                      여러 개의 Thread 를 사용하는 것은 비효율적으로 보인다.
                - Synchronous Non-Blocking I/O
                    - Application 에서 I/O를 요청 후 바로 return 되어 다른 작업을 수행하다가 특정 시간에 데이터가 준비가 다되었는지
                      상태를 확인한다. 데이터의 준비가 끝날 때까지 틈틈이 확인을 하다가 완료가 되었으면 종료된다.
                      여기서 주기적으로 체크하는 방식을 폴링(Polling) 이라고 한다. 그러나 이러한 방식은 작업이 완료되기 전까지
                      주기적으로 호출하기 때문에 불필요하게 자원을 사용하게 된다.
                - Asynchronous Non-blocking I/O
                    - I/O 요청을 한 후 Non-Blocking I/O와 마찬가지고 즉시 리턴된다. 허나, 데이터 준비가 완료되면 이벤트가 발생하여
                      알려주거나, 미리 등록해놓은 callback 을 통해서 이후 작업이 진행된다. 이전 두 I/O의 문제였던
                      Blocking 이나 Polling 이 없기 때문에 자원을 보다 더 효율적으로 사용할 수 있다.
                - Event-Driven
                    - Event-Driven 을 토대로 많은 프레임워크와 라이브러리가 발전하고 있다. 예) Spring WebFlux, Node.js, Vert.x 등
                    - Event-Driven Programming 은 프로그램 실행 흐름이 이벤트
                      (ex : 마우스 클릭, 키 누르기 또는 다른 프로그램의 메시지와 같은 사용자 작업)에 의해 결정되는 프로그래밍 패러다임이다.
                      Event 가 발생할 때 이를 감지하고 적합한 이벤트 핸들러를 사용하여 이벤트를 처리하도록 설계됐다. 순차적으로 진행되는
                      과거의 프로그래밍 방식과는 달리 유저에 의해 종잡을 수 없이 진행되는 GUI(Graphical User Interface)가
                      발전됨에 따라 Event-Driven 방식은 더욱더 많이 쓰이게 되었다.
                - Spring Framework
                    - Spring 은 Reactive Stack 과 Servlet Stack 두 가지 형태를 제공한다. 또한 Reactive Stack 은
                      non-blocking I/O를 이용해서 많은 양의 동시성 연결을 다룰 수 있다고 한다. 과거로 돌아가서 Servlet Stack 의
                      문제점을 파악하고 이를 어떻게 Reactive Stack 으로 해결했는지 알아보자.
                - Spring MVC
                    - 위 그림처럼 유저들로부터 HTTP 요청이 들어올 때 요청들은 Queue 를 통하게 된다. Thread pool 이
                      수용할 수 있는 수(thread pool size)의 요청까지만 동시적으로 작업이 처리되고 만약 넘게 된다면 큐에서 대기하게 된다.
                      즉 하나의 요청은 하나의 Thread 를 요구한다. (one request per thread model)
                    - Thread pool 은 다음과 같다. Thread 를 생성하는 비용이 크기 때문에 미리 생성하여 재사용함으로써 효율적으로 사용한다.
                      그렇다고 과도하게 많은 Thread 를 생성하는 것이 아니라 서버 성능에 맞게 Thread 의 최대 수치를 제한시킨다.
                      참고로 tomcat default thread size 는 200이다.
                    - 그런데 만약 대량의 트래픽이 들어와 thread pool size 를 지속적으로 초과하게 된다면 어떻게 될까?
                    - 설정해놓은 thread pool size 를 넘게 되면 위 그림처럼 작업이 처리될 때까지 Queue 에서 계속해서 기다려야 한다.
                      그래서 전체의 대기시간이 늘어난다. 이런 현상을 Thread pool hell 이라고 한다.
                    - Thread pool 이 감당할 수 있을 때까진 빠른 처리속도를 보이지만, 넘는 순간부터는 지연시간이 급격하게 늘어난다.
                    - 특수한 경우를 제외하면 DB, Network 등의 I/O가 일어나는 부분에서 아마 시간을 많이 소비했을 것이다.
                    - 설명했듯이 I/O 작업은 CPU 가 관여하지 않는다. I/O Controller 가 데이터를 읽어오고 이를 전달받을 뿐이다.
                      위에서 I/O를 처리하는 3가지 방식을 소개했는데 가장 효율이 좋은 방법은 마지막에 설명한
                      Asynchronous Non-blocking I/O 이라고 하였다. Blocking 방식은 I/O Controller 가 데이터를 읽는 동안 CPU 가
                      아무 일도 할 수가 없고, Non-Blocking 방식은 polling 때문에 불필요하게 CPU 를 소비한다고 했다.
                      Spring 에서도 Non-blocking I/O를 이용해서 효율적으로 작업을 처리할 수 있는 방법을 제공한다. 그 수단이 WebFlux 이다.
                - Spring WebFlux
                    - 사용자들에 의해 요청이 들어오면 Event Loop 를 통해서 작업이 처리가 된다. one request per thread model 과의
                      차이점은 다수의 요청을 적은 Thread 로도 커버할 수 있다. worker thread default size 는 서버의 core 개수로
                      설정이 되어있다. 즉 서버의 core 가 4개라면 worker thread 는 4개라는 말이며 이 적은 Thread 를 통해서도
                      traffic 을 감당할 수 있다. 위에서 하나의 Thread 로 3초가 걸리는 API 1000개를 호출했음에도 4초밖에 안 걸렸다는 걸
                      상기시키면 이해에 도움이 될 것이다. 또한 비슷한 Architecture 를 가진 Node.js가 이미 증명을 하고 있다.
                    - 이렇듯 Non Blocking 방식을 활용하면 좀 더 효율적으로 I/O를 제어할 수 있고 성능에도 좋은 영향을 미친다.
                      특히나 유행하는 MSA 에서는 수많은 Microservice 가 거미줄처럼 서로를 네트워크를 통해서 호출하고 있다.
                      즉 많은 수의 Network I/O가 발생할 텐데 이를 Non Blocking I/O를 통해 좀 더 성능을 끌어올릴 수 있다.
                    - 그러나 물론 제한된 점이 있다. WebFlux 로 성능을 최대치로 끌어올리려면 모든 I/O 작업이 Non Blocking 기반으로
                      동작해야 된다. Blocking 이 되는 곳이 있다면 안 하느니만 못한 상황이 되어버린다.
                      예를 들어 멀티코어로 가정을 해보자. 그럼 처리할 수 있는 Thread 는 2개인데 Blocking 이 걸리는 API 를 열명 이서
                      동시에 호출한다면 결국엔 Spring MVC 처럼 8명이 I/O 작업이 끝날 때까지 기다려야 하는 구조가 되어버리기 때문이다.

                    - Java 진영에는 아쉽게도 DB connection 을 non-blocking 으로 지원하는 라이브러리가 널리 보급되어 잘 사용되지는 않고 있다.
                      다만 R2DBC 처럼 개발이 진행 중인 라이브러리, 최근에 release 된 jasync sql 등이 있으며, MongoDB, Redis 등의 NoSQL 은 지원중이다.
                    - 또한 소수의 Thread 에 의해서 수많은 요청을 처리하고, 순서대로 작업이 처리되는 것이 아니라 Event 에 기반하여
                      실타래가 엉킨 것처럼 작업이 처리되기 때문에 트래킹 하기에 힘이 들다는 문제가 있다.

                    - 그렇다면 성능이 좋으니 무조건 WebFlux 를 사용해야 할까?
                        - 위 그림은 Spring MVC 나 Spring WebFlux 둘 다 성능이 동일한 구간이 있다. 서버의 성능이 좋으면 좋아질수록
                          해당 구간은 더 늘어날 것이다. 그렇기에 만약 여러분의 환경이 해당 구간이라면 굳이 사용할 필요가 없다.
                          또한 Spring Document 에서는 동기 방식이 코드 작성, 이해, 디버깅하기 더 쉽다고 한다. 이 말은 즉 높은 생산성을
                          가진다는 말과 같은 것으로 보인다. 그렇기에 이해타산을 잘 따져서 선택해야 할 필요가 있다.
                        - 그리고 우리는 이제 왜 성능이 동일한 구간이 생기는 지를 알 수 있다. 저 구간은 바로 Thread Pool 이
                          감당할 수 있을 정도의 요청이었기에 비동기적으로 잘 수행하다가 이후에는 Queue 에 쌓여 점점 성능이 느려졌던 것이다.
                        -> 'Spring WebFlux 는 어떻게 적은 리소스로 많은 트래픽을 감당할까?'란 궁금증을 시작으로 여기까지 왔다.
                           이에 대한 답은 I/O를 Non Blocking 을 이용하여 잘 사용하는 것과 Request 를 Event-Driven 을 통해서
                           효율적으로 처리하기 때문에 가능하다.

            - Spring MVC vs WebFlux   https://devmoony.tistory.com/174   https://woowabros.github.io/experience/2020/02/19/introduce-shop-display.html
                - WebFlux
                    - Spring WebFlux 는 Spring 5에서 새롭게 추가된 모듈
                    - 장점 : 고성능, spring 과 완벽한 통합, netty 지원, 비동기 non-blocking 메세지 처리
                      단점 : 오류처리가 다소 복잡하다.
                    - Spring WebFlux 는 아래와 같은 용도로 사용하기를 추천
                        - 비동기, 논블로킹 reactive 개발에 사용하는 경우
                        - 효율적으로 동작하는 고성능 웹어플리케이션 개발에 사용
                        - 서비스간 호출이 많은 마이크로서비스 아키텍처에 적합
                    - 논블로킹과 블로킹 코드를 같이 사용하게 되면 비동기 코드가 무의미해지고 성능적인 이점도 볼 수 없기 때문에 고려해야 할 부분이 많다.\

                - Spring MVC
                    - 1 request : 1 thread
                    - sync + blocking
                - WebFlux
                    - many request : 1 thread
                    - async + non-blocking

                -> Spring MVC 는 1:1로 요청을 처리하기 때문에 트래픽이 몰리면 많은 쓰레드가 생겨난다. 쓰레드가 전활될 때 컨텍스트 스위칭 비용이
                   발생하게 되는데 쓰레드가 많아질수록 비용이 커지기 때문에 적절한 쓰레드 수를 유지해야 하는 문제가 있다.
                   이에 반해 WebFlux 는 Event-Driven 과 Asynchronous Non-blocking I/O 를 통해 리소스를 효율적으로 사용할 수 있도록 만들어 준다.


- Blocking vs Non-Blocking, Sync vs Async
    - Blocking vs Non-Blocking
        - Blocking: 자신의 작업을 진행하다가 다른 주체의 작업이 시작되면 다른 작업이 끝날 때까지 기다렸다가 자신의 작업을 시작하는 것
        - Non-Blocking: 다른 주체의 작업에 관련없이 자신의 작업을 하는 것
        -> 다른 주체가 작업을 할 때 자신의 제어권이 있는지 없는지로 볼 수 있다.

    - Synchronous vs Asynchronous
        - Synchronous: 작업을 동시에 수행하거나, 동시에 끝나거나, 끝나는 동시에 시작함을 의미
        - Asynchronous: 시작, 종료가 일치하지 않으며, 끝나는 동시에 시작을 하지 않음을 의미
        -> 결과를 돌려주었을 때, 순서와 결과에 관심이 있는지 아닌지로 판단할 수 있다.

    - Blocking/Sync
        ex) 자바에서 입력요청을 할 때

    - Non-Blocking/Sync
        - Blocking/Sync 와 큰 차이가 없다.

    - Blocking/Async
        - 보통 Non-Blocking/Async 로 하려다가 개발자의 실수로 혹은 기타 이유로 이와 같이 동작

    - Non-Blocking/Async
        ex) 자바스크립트에서 API 요청을 하고 다른 작업을 하다가 콜백을 통해서 추가적인 작업을 처리할 때

- 멀티스레드와 동기화
    - 공유자원과 임계영역
        - 공유자원: 여러 스레드가 동시에 접근할 수 있는 자원
        - 임계영역: 공유자원들 중 여러 스레드가 동시에 접근했을 때 문제가 생길 수 있는 부분

    - 경쟁상태
        - 둘 이상의 스레드가 공유자원을 병행적으로 읽거나 쓰는 동작을 할 때 타이밍이나 접근 순서에 따라 실행 결과가 달라지는 상황
        - Read - Modify - Write, Check - then - act 라는 두 가지 패턴 존재
            - Read - Modify - Write
              경쟁 상태가 발생하는 연산의 패턴 중 가장 유명한 패턴
              메모리에 값을 읽어오고 수정하고 덮어쓰는 세 가지 연산으로 분리가 되는 것
            - Check - then - act
              if 분기문을 통과하기 전에는 조건에 부합했지만, if 분기문을 통과한 후에는 조건에 부합x

    - 원자성과 가시성
        - 원자성
            - 공유 자원에 대한 작업의 단위가 더이상 쪼갤 수 없는 하나의 연산인 것처럼 동작하는 것
            - Read - Modify - Write
                1. 메모리에서 값을 읽어옴 (read)
                2. 읽어온 값을 수정 (modify)
                3. 수정한 값을 다시 메모리에 덮어씀 (write)
                -> 1 2 번 사이에 시간 텀이 생기게 되므로 한 쓰레드가 연산을 할 때 다른 쓰레드의 연산이 개입할 수 있다.
                   이런 경쟁 상태를 발생시키지 않기 위해서는 분리된 명령어들을 하나로 모아주는 과정이 필요한데 이걸 원자성이라 한다.
            - Check - then - act
                1. 분기문 비교 (read)
                2. 로직 (act)
        - 가시성
            - 쓰레드를 실행하는 건 CPU 가 실행을 하는데 쓰레드를 CPU 가 실행할 때 메인 메모리에서 변수 값을 읽어와서 쓰레드를 실행.
              그런데 메인 메모리와 CPU 간의 거리가 멀어서 CPU 캐시 사용. CPU 는 이 쓰레드를 실행할 때 필요한 값을 메인 메모리에서 읽어와서
              CPU 캐시에 담아두고 CPU 캐시의 모든 연산을 반영한 다음 그 값을 메인 메모리에 덮어쓰는 방식으로 동작
            - 메인 메모리에 있는 진짜 값을 보지 못해서 가시성 이라고 함
            - volatile 변수를 선언하면 이 변수는 메인 메모리에서만 값을 읽고 쓰고 CPU 캐시를 사용하지 않는 변수가 된다.

    - 동기화
        - 블로킹
            - 특정 스레드가 작업을 수행하는 동안 다른 작업은 진행하지 않고 대기하는 방식
              ex) Monitor, Synchronized 키워드
            - 모니터
                - 자바에서 동기화를 하기 위한 도구
                - 배타동기큐는 synchronized, 조건동기큐는 wait(), notify(), notifyAll()
                - 임계영역에는 한번에 한 스레드만 락을 가지고 들어가도록 설계되어 있음.
                  만약에 임계영역에 접근하는 여러 스레드가 있다면 그 중에 한 스레드가 먼저 임계영역에 들어갈 수 있고 작업을 수행하다가
                  wait() 라는 연산을 만나면 이 스레드는 슬립 상태가 되면서 조건동기큐로 들어가게 된다. 배타동기큐에 있던 다른 스레드들이
                  임계영역이 비어있기 때문에 임계영역에 락을 가지고 들어올 수 있다. 다른 스레드가 임계영역에서 작업을 수행하다가
                  notify()나 notifyAll()이라는 메서드를 호출하게 되면 조건동기큐에서 자고 있던 스레드가 깨어나면서 임계영역이 비어 있을 때
                  다시 임계영역으로 돌아와서 작업을 수행하도록 하는 걸 모니터 메커니즘이라고 함.
            - Synchronized
                - 배타동기를 선언하는 키워드
                - 연산결과가 메모리에 써질때까지 다른 스레드는 임계영역에 들어오지 못하고 대기
                - 동작 방식
                    임계영역에 한 개의 스레드만 들어올 수 있기 때문에 이 스레드가 들어오고 나서 자기가 연산할 것을 다 연산하고 메인 메모리에
                    반영시킨 이후에 임계영역에서 나가게 된다. 그리고 나서 다른 스레드가 임계영역에 들어오게 되고 들어올 때는 메인 메모리에서
                    이미 동기화된 값을 읽어오기 때문에 문제가 발생하지 않음. 이렇게 순차적으로 접근하는 방식 때문에 원자성과 가시성을
                    모두 만족
                - 단점
                    - 하나의 쓰레드만 임계영역에서 작업을 수행할 수 있기 때문에 나머지 대기하는 스레드들이 발생하고 성능저하로 이어질 수 있음
                    - 임계영역에 들어갈 때 락을 획득하고 들어가기 때문에 데드락이 발생할 수 있다.
                        - 계속 자기가 잡고 있는 자원을 놓지 않고 상대방이 가지고 있는 자원을 놓기를 계속 기다리고 있는 상태가 데드락 상태

        - 논블로킹
            - 다른 스레드의 작업여부와 상관없이 자신의 작업을 수행하는 방식
              ex) Atomic 타입
            - CAS 알고리즘 (Compare and Set)
                - 연산을 할 때 자원 값을 가져오는 데 가져올 때 자원 값이랑 똑같은 값에 기대 값이라는 걸 만든다.
                  이 기대값을 기반으로 연산을 진행해서 새로운 값을 도출. 이 새로운 값을 자원 값에다 덮어쓰기 하기 직전에 내가 이전에
                  만들었던 기대값과 현재의 자원값이 같은지를 확인하는 로직이 있다. 같으면 기존 자원값을 새로운 값으로 수정하고 true, 다르면
                  수정하지 않고 false. 이렇게 자원값과 기대값을 비교하는 과정에서 CAS 를 통해서 원자성을 보장.
                - false 를 반환했을 때 이후에 동작은 어떻게 할 것인지는 개발자의 요구사항에 따라 달라짐.
                  while 문을 계속 돌면서 이 조건이 true 가 나올 때까지 계속 재시도 하는 방법이 있고 아니면 몇 번 시도하다가 exception 을
                  터뜨리고 끝내는 방법도 있다.
            - Atomic 타입
                - 동시성을 보장하기 위해서 자바에서 제공하는 래퍼 클래스
                - CAS 와 Volatile 을 활용해서 원자성과 가시성을 보장
                - AtomicReference
                    - 보통 일반적인 스레드에서는 값을 연산하고자 할 때 연산하는 값을 끌어올 때는 JVM 과 CPU 사이에 있는 캐시값에서 변수를
                      끌어옴. 근데 아토믹 레퍼런스를 설정하게 되면 내부에 volatile 이 박혀있어서 JVM 메모리에서 바로 스레드로 값을
                      당겨올 수 있다. 바로 값을 당겨와서 연산을 진행하게 되는데 연산을 할 때는 compareAndSet() 이라는 메소드가 호출된다.
                      CAS 알고리즘을 녹인 메소드인데 이걸 실행하면 현재 메모리에 저장된 값과 스레드 내부에 이미 만들어 놨던 기대값을
                      비교해서 일치하면 true, 아니면 false 를 반환하는 식으로 동작. 그래서 아토믹 레퍼런스가 volatile 을 통해서 가시성을
                      CAS 를 통해서 원자성을 보장.

    - 스레드 안전한 객체 설계 방법
        - 여러 스레드가 동시에 클래스를 사용하려 하는 상황에서 클래스 내부의 값을 안정적인 상태로 유지할 수 있는 것을 스레드 세이프.
        - 여러 방법들이 있는데 전략에 따라서 선택을 달리 해야하고 구현에 따라서 장단점도 다르다.
        - 가장 확실하고 안전하고 간단한 방법 -> 공유변수 최소화 + 캡슐화 + 문서화
          공유변수 최소화가 가장 스레드 안전한 객체를 설계하는 방법이고 불가피하게 공유 변수를 써야 한다면
          내가 관리해야 될 포인트를 한 곳에 모아서 한 객체에서 캡슐화를 해서 그 객체만 관리할 수 있게끔 하는게 차선.
          공유 변수를 사용하게 되면 동기화 정책을 많이 적용하게 되는데 그런 동기화 정책이 코드에 들어가면 코드 파악이 어려워지므로 문서화 필요


- 페이징과 세그멘테이션
    https://steady-coding.tistory.com/524


- connection timeout/read timeout/socket timeout


- JDBC Timout
    - Timeout 계층 구조
        - timeout은 계층적 구조를 갖고 있다. 상위timeout은 하위timeout에 의존적
        - 즉, 하위 timeout이 제대로 동작해야지 상위 timeout 또한 제대로 동작한다.

        - Transaction Timeout
            - 애플리케이션의 한 트랙잭션에서 수행하는 모든 Statememt가 수행될 때 유효한 timeout
            - spring의 @Transactional(timeout = )으로 직접 설정이 가능하며 default로 시스템 timeout을 이용한다.
            - Thread Local에 Connection 정보를 저장한 뒤 경과 시간을 체크하여 예외를 발생시킨다.

        - Statement Timeout
            - 하나의 쿼리가 수행될 때 유효한 timeout 이다.
            - 쿼리생성부터 요청까지의 시간이므로 네트워크 장애와는 무관하다.

            - Connection.createStatement()를 이용해 Statement생성
            - executeQuery() 호출
            - DBMS로 쿼리 전송
            - 새로운 timeout처리 스레드를 생성하고 등록, 동일한 커넥션 정보로 커넥션 획득
            - timeout이 발생하면 timeout처리 스레드가 DBMS로 직전 쿼리 취소 쿼리 호출

        - JDBC Driver Socket Timeout
            - JDBC Driver가 DBMS와 통신하는 타입은 4가지가 있다. 그 중 소켓을 이용한 타입이 있고 소켓은 네트워크 통신을 담당
            - 적절히 socket timeout을 설정하여 네트워크 장애 또한 상대 서버 장애로 인한 무응답 상태에 대처할 수 있다.

        - OS Socket Timeout
            - JDBC도 결국 OS의 소켓을 이용하기 때문에 OS소켓의 timeout을 설정하면 애플리케이션에도 영향을 끼친다.
            - 위에서 언급한 timeout은 애플리케이션에서 설정한 논리적인 timeout이고 OS Socket Timeout은 시스템 전체에 미치는 timeout이다.


- LRU알고리즘이란 Least-Recently-Used
    - 페이지 교체 알고리즘 중에 하나
    - 메모리에 빈 프레임이 없을 때 적재될 페이지를 위해 적재된 페이지 중 누군가는 자신이 차지한 프레임을 비워주어야 하는 교체 대상이 되어야 한다.
    - 어떤 페이지를 선택하는 것이 최적의 기법이 인지 판단하는 다양한 기법들이 있고 페이지 교체 알고리즘이라고 한다.
    - LRU알고리즘은 최근에 사용하지 않는 페이지를 가장 먼저 내려 보내는 알고리즘이다.


- 프로세스와 스레드의 차이(Process vs Thread)
    - 프로세스
        - 정의
            - 운영체제로부터 시스템 자원을 할당 받는 작업의 단위 , 실행 중인 program

        - 구성
            - 프로세스 : 스레드 + 메모리공간(stack, code , data , heap)
            - [cpu 를!] OS의 스케줄러애 의해 time slice만큼 실행
            - [memory 를!] OS로 할당받은 메모리 공간 사용 code , data , heap , stack

        - 특징
            - cpu는 가상메모리를 통해 Ram주소를 들어간다.
            - 가상메모리는 일종의 프로세스를 가리키는 페이징 테이블이라고 한다. 프로세스간 메모리 공간격리
            - 가상메모리는 프로세스간 주소공간을 논리적으로 차단한다.
                - 프로세스는 자신만의 가상 주소 공간을 가지고 있다.
                - 모든 프로세스들은 자신만의 주소 공간을 가지기 때문에, 특정 프로세스 내에서 쓰레드가 수행 될 때
                - 해당 쓰레드는 프로세스가 소유하고 있는 메모리에 대해서만 접근이 가능하다.

            - task_struct는 커널의 중요한 객체이며 실행중인 프로그램인 프로세스는 리눅스에서는 테스크(task)와 동일한 의미로 사용된다.
              프로세스는 파일 시스템에서 메모리로 로드한 코드, 데이터(프로그램)와 이를 커널에서 관리하기 위한 task_struct 구조체이다.
            - 정리하자면 프로세스는 task_struct구조체를 통해 스레드라는 자원을 관리한다.

    - 스레드
        - 프로세스 내에서 실행되는 흐름의 단위
        - cpu 이용의 기본 단위
        - 프로세스를 만드는 것보다 비용이 저렴
        - 스레드들간 HEAP 메모리 공간 공유
        - 하나의 프로세스에 하나의 스레드가 있는 상황에서 신규 스레드가 추가된다면 신규 스레드가 사용할 stack 메모리 공간만 생성된다.


- 멀티 프로세스 대신 멀티 스레드를 사용하는 이유
    [리눅스기준]
        - 스레드의 생성의 비용이 더 저렴하다.
        - context switch 비용
            - cpu와 LAM 영역 사이에 메모리 인풋 아웃 IO작업이 일어나는데 LAM 영역에 접근하는데 많이 시간이 걸린다.
            - 스레드의 경우 HEAP 메모리 영역을 공유하므로 한 프로세스안에 다른 스레드들이 context switch이 일어나도 캐시에 같이 데이터가 있을 수 있기 때문에
            - 캐싱 측면에서 스레드 모델 성능이 좋다.
            - 멀티 프로세스 모델은 서로간 메모리를 공유 할 수 없기 때문에 context switch시 캐시 레이어를 리셋해버린다. 그렇기 때문에 속도적으로 이슈가 생긴다.


- Thread-safe
    - 멀티 스레드 프로그래밍에서 함수, 변수, 객체 등이 여러 스레드로부터 동시에 접근이 이루어져도 프로그램의 실행에 문제가 없다는 것을 뜻한다.
    - 하나의 함수가 한 Thread로부터 호출되어 실행 중일 때 다른 Thread가 그 함수를 호출하여 동시에 함께 실행되더라도 각 스레드에서의 함수의 수행 결과가 의도한대로 나와야합니다.

    - Thread-safe를 지키기 위한 방법
        - Re-entrancy : 어떤 함수가 한 스레드에 의해 호출되어 실행 중일 때, 다른 스레드가 그 함수를 호출하더라도 그 결과가 각각에게 올바로 주어져야 한다.
        - Thread-local storage : 공유 자원의 사용을 최대한 줄여 각각의 스레드에서만 접근 가능한 저장소들을 사용함으로써 동시 접근을 막는다.
                                 이 방식은 동기화 방법과 관련되어 있고, 또한 공유상태를 피할 수 없을 때 사용하는 방식이다.
        - Mutual exclusion : 공유 자원을 꼭 사용해야 할 경우 해당 자원의 접근을 세마포어 등의 락으로 통제한다.
        - Atomic operations : 공유 자원에 접근할 때 원자 연산을 이용하거나 '원자적'으로 정의된 접근 방법을 사용함으로써 상호 배제를 구현할 수 있다

    - Thread Safe한 코드 작성하기
        - Synchronized 함수 사용
            - 클래스로 만들어진 하나의 인스턴스를 기준으로 동기화가 이루어진다.
        - Synchronized 블럭 사용
            - 전달받은 객체를 기준으로 동기화가 이루어진다.
        - ReentrantLock 사용
            - 시작점과 끝점을 명백히 명시할 수 있다. Synchronized는 암묵적이고 ReentrantLock는 명시적이라는 차이가 있다.
        - 세마포어 사용
            - 하나의 스레드만 임계구역에 들어가면 성능 이슈가 발생하는데, 세마포어는 임계구역에 여러 스레드가 들어갈 수 있는 장점이 있다.
              공유자원이 2개 이상일 때 잘못 사용하면 서로 자원을 점유하기 위해서 대기상태에 빠지므로 DeadLock이 발생할 수 있다.
        - 모니터 사용
            - 모니터는 2개의 Queue가 존재한다.
            - 하나의 Queue는 하나의 Thread만 공유자원에 접근할 수 있게하는 역할을 한다.(상호배타)
            - 다른 Queue는 임계구역에 진입한 wait()을 통해 Thread가 블락되면 새로운 Thread가 진입할 수 있도록 알려주는 역할을 한다.(조건동기)
            - 그리고 새로 진입한 Thread가 notify()를 통해 블락된 Thread를 재진입할 수 있도록 하는 역할도 한다.(조건동기)

        * 임계 구역 : 파일, 입출력, 공유 데이터 등 원자적으로 실행할 필요가 있는 명령문 또는 코드의 일부 영역


- 동기화 객체의 종류
    - Critical section 이란 ?
        - 동기화 방법 중 유일하게 커널 객체를사용하지 않으며 그 내부 구조가 단순하기 때문에 동기화 처리를 하는 데 있어서 속도가 빠르다는 장점이 있으며 동일한 프로세스내에서만 사용할 수 있다는 제약이 있다.
        - 크리티컬 섹션은 커널 객체를 사용하지 않기 때문에 핸들을 사용하지 않고 대신 CRITICAL_SECTION라는 타입을 정의하여 사용하게 되며, 아래 4가지 Win32 API를 사용하여 동기화를 수행하게 된다.
        - 공유 데이타를 여러 프로세스가 동시에 액세스하면 시간적인 차이 때문에 잘못된 결과를 만들어 낼 수 있기 때문에 한 프로세스가 위험 부분을 수행하고 있을 때,
          즉 공유 데이타를 액세스하고 있을 때는 다른 프로세스들은 절대로 그 데이타를 액세스하지 못하도록 하여야 한다.
        * 커널 객체를 사용하지 않는 동기화 객체는 크리티컬 섹션뿐이며 뮤택스를 포함한 동기화 객체들, 스레드(Thread), 파일(File)들까지도 동기화를 위한 커널 객체를 포함하고 있다.

    - 이벤트란(Event)?
        - 이벤트는 어떠한 사건에 대하여 알리기 위한 용도로 사용되는 동기화 객체이다.
        - 이벤트 객체의 경우 우리가 Windows에서 메시지를 교환할 때 사용하는 개념과 같은 개념이다.
        - 즉, Windows의 메시지 교환 방식은 이벤트에 기반한 방식이라고 이야기한다. 키보드가 눌려질 경우 WM_KEYDOWN, 마우스 왼쪽 버튼이 눌러질 경우
          WM_LBUTTONDOWN과 같은 메시지가 날라오며, 이러한 신호는 이벤트를 통하여 통보하게 되는 것이다.
        - 이벤트의 경우 SetEvent() 또는 ResetEvent()를 사용하여 동기화 객체들의 상태를 마음대로 바꿀 수 있게 된다.

    - 뮤텍스란(Mutex)?
        - 공유된 자원의 데이터를 여러 쓰레드가 접근하는 것을 막는 것
        - Mutual Exclusion 으로 상호배제라고도 한다. Critical Section을 가진 쓰레드들의 Runnig Time이 서로 겹치지 않게 각각 단독으로 실행되게 하는 기술입니다.
          다중 프로세스들의 공유 리소스에 대한 접근을 조율하기 위해 locking과 unlocking을 사용한다. 즉, 쉽게 말하면 뮤텍스 객체를 두 쓰레드가 동시에 사용할 수 없다는 의미입니다.
        - 뮤텍스는 스레드가 여러 개 있더라도 자신이 소유한 스레드가 누구인지를 기억하고 있으며, Windows 운영체제는 뮤텍스가 반환되지 않은 상태에서 스레드가 종료될 경우
          그 뮤텍스를 강제적으로 Signaled 해줌으로써 이를 대기하고 있던 스레드가 무한정 기다리는 일이 없도록 해준다.
        - 뮤텍스는 뮤텍스를 소유한 스레드를 기억하고 있음으로써 같은 스레드가 같은 뮤텍스를 중복 호출하더라도 데드락 현상이 발생하기 않게 하고있다.
          즉, 내부적으로 같은 스레드가 같은 뮤텍스를 소유하려 할 경우 뮤텍스의 내부적인 카운트만을 증가시켜 주고 이 스레드에 대한 진입은 허용하여 주게 된다.
          그리고 이 내부적인 카운트가 0으로 내려갔을 경우에 Signaled 상태로 돌려줌으로써 다른 스레드에 대해서는 중복 호출한 스레드가 뮤텍스에 대한
          사용을 모두 종료한 후에 진입하도록 하고 있으며, 이러한 기능은 같은 스레드의 재귀 호출에 의한 데드락 현상을 방지하여 주게 된다.

    - 세마포어란?(Semaphore)
        - 공유된 자원의 데이터를 여러 프로세스가 접근하는 것을 막는 것
        - 세마포어는 사용자가 지정한 개수만큼 이 동기화 객체로 보호하는 자원에 대하여 접근할 수 있도록 하고 있다.
          즉, 세마포어에서는 사용 가능한 자원의 개수를 세팅할 수 있도록 하고 있으며, 그 값은 세마포어 초기화 시에 세팅하게 된다.
        - 세마포어는 리소스의 상태를 나타내는 간단한 카운터로 생각할 수 있다.
          일반적으로 비교적 긴 시간을 확보하는 리소스에 대해 이용하게 되며, 유닉스 시스템의 프로그래밍에서 세마포어는 운영체제의 리소스를 경쟁적으로
          사용하는 다중 프로세스에서 행동을 조정하거나 또는 동기화 시키는 기술이다.
          세마포어는 운영체제 또는 커널의 한 지정된 저장장치 내 값으로서, 각 프로세스는 이를 확인하고 변경할 수 있다. 확인되는 세마포어의 값에 따라,
          그 프로세스가 즉시 자원을 사용할 수 있거나, 또는 이미 다른 프로세스에 의해 사용 중이라는 사실을 알게 되면 재시도하기 전에 일정 시간을 기다려야만 한다.
          세마포어는 이진수 (0 또는 1)를 사용하거나, 또는 추가적인 값을 가질 수도 있다.
          세마포어를 사용하는 프로세스는 그 값을 확인하고, 자원을 사용하는 동안에는 그 값을 변경함으로써 다른 세마포어 사용자들이 기다리도록 해야한다.


- 뮤텍스와 세마포어의 차이
    - Semaphore는 Mutex가 될 수 있지만 Mutex는 Semaphore가 될 수 없습니다.(Mutex 는 상태가 0, 1 두 개 뿐인 binary Semaphore)
    - Semaphore는 소유할 수 없는 반면, Mutex는 소유가 가능하며 소유주가 이에 대한 책임을 집니다. (Mutex 의 경우 상태가 두개 뿐인 lock 이므로 lock 을 ‘가질’ 수 있습니다.)
    - Mutex의 경우 Mutex를 소유하고 있는 쓰레드가 이 Mutex를 해제할 수 있습니다. 하지만 Semaphore의 경우 이러한 Semaphore를 소유하지 않는 쓰레드가 Semaphore를 해제할 수 있습니다.
    - Semaphore는 시스템 범위에 걸쳐있고 파일시스템상의 파일 형태로 존재합니다. 반면 Mutex는 프로세스 범위를 가지며 프로세스가 종료될 때 자동으로 Clean up된다.

    -> 가장 큰 차이점은 관리하는 동기화 대상이 갯수다. Mutex는 동기화 대상이 오직 하나뿐일 때, Semaphore는 동기화 대상이 하나 이상일 때 사용한다.


- 스케줄러(Scheduler)란?
    - 스케줄러
        - 프로세를 스케줄링하기 위한 Queue에는 세 가지 종류가 존재한다.
            - Job Queue : 현재 시스템 내에 있는 모든 프로세스의 집합
            - Ready Queue : 현재 메모리 내에 있으면서 CPU를 잡아 실행되기를 기다리는 프로세스의 집합
            - Device Queue : Device I/O 작업을 대기하고 있는 프로세스의 집합

    - 장기 스케줄러(Long-term scheduler or job scheduler)
        - 메모리는 한정되어 있는데 여러 프로세스들이 한 번에 메모리에 올라올 경우에 대용량 메모리(일반적으로 디스크)에 임심로 저장된다.
        - 이 Pool에 저장되어 있는 프로세스 중 어떤 프로세스에 메모리를 할당하여 Ready Queue 로 보낼지 경정하는 역할을 한다.
            - 메모리와 디스크 사이의 스케줄링을 담당
            - 프로세스에 메모리(및 각종 리소스)를 할당(admit)
            - degree of multiprogramming 제어
              (메모리에 여러 프로그램이 올라가는 것) 몇 개의 프로그램이 올라갈 것인지를 제어
            - 프로세스의 상태
              new => ready(in memory)

    - 중기 스케줄러(Medium-term scheduler or Swapper)
        - 여유 공간 마련을 위해 아직 실행하지 않아도 되는 프로세스를 통째로 메모리에서 제거하고 디스크로 쫓아냄 (Swapping)
        - Performance (속도)는 느려지지만 여러 프로세스르 실행시키기 위해서는 반드시 필요한 작업

    - 단기 스케줄러(short-term scheduler or CPU scheduler)
        - CPU 와 메모리사이의 스케줄링을 담당
        - 굉장히 자주 돌며 Ready Queue 에 존재하는 프로세스 중 어떤 프로세스를 running 시킬 것인지 결정
        - 시간을 엄청 많이 나누어 그 순간마다 여러 프로세스를 실행시키는 즉, 멀티태스킹을 하게 해주는 스케줄러


- 동기와 비동기
    - Sync와 Async 를 구분하는 기준은 작업순서이다.
    - 동기식 모델은 모든 작업들이 일련의 순서를 따르며 그 순서에 맞게 동작한다.
    - 즉, A,B,C 순서대로 작업이 시작되었다면 A,B,C 순서로 작업이 끝나야 한다. 설령 여러 작업이 동시에 처리되고 있다고 해도,
      작업이 처리되는 모델의 순서가 보장된다면 이는 동기식 처리 모델이라고 할 수 있다.

    - 많은 자료들이 동기식 처리 모델을 설명할 때, 작업이 실행되는 동안 다음 처리를 기다리는 것이 (Wait) Sync 모델이라고도 하지만, 이는 잘 알려진 오해이다.
      이 Wait Process 때문에 Blocking 과 개념의 혼동이 생기는 경우가 흔하다.
      동기 처리 모델에서 알아두어야할 점은 작업의 순서가 보장된다는 점 뿐이다.

    - 반면 비동기식 모델은 작업의 순서가 보장되지 않는다. 말 그대로 비동기(Asynchronous) 처리 모델로 A, B, C 순서로 작업이 시작되어도 A, B, C 순서로 작업이 끝난다고 보장할 수 없다.
      비동기식 처리 모델이 이득을 보는 경우는 각 작업이 분리될 수 있으며, Latency 가 큰 경우이다.
      예를 들면 각 클라이언트 또는 작업 별로 Latency 가 발생하는 네트워크 처리나 File I/O 등이 훌륭한 적용 예시인 것


- 프로세스 동기화
    - 병행 프로세스(Concurrent Process)의 문제
        - 병행 프로세스(Concurrent Process)는 두 개 이상의 프로세스들이 동시에 존재하여 실행 상태에 있는 것을 의미
        - 컴퓨터 자원에는 어느 한 시점에 하나의 프로세스가 할당되어 수행되는데, 동시에 두 개 이상의 프로세스를 병행 처리하면 여러 문제점(race condition 과 같은)이 발생될 수 있다.
          이러한 문제를 해결하기 위해 임계 영역, 상호 배제 기법, 동기화 기법 등을 사용하는 것

    - 임계 구역(Critical Section)
        - 임계 구역(Critical Section)은 여러 개의 프로세스가 공유하는 데이터 및 자원에 대하여 어느 한 시점에서는 하나의 프로세스만 사용할 수 있도록 지정(공유 자원의 독점을 보장)하는 것을 의미
            - 임계 구역에는 하나의 프로세스만 접근가능하며, 반납 후에만 다른 프로세스가 사용 가능
            - 임계 구역은 특정 프로세스가 독점할 수 없으며, 임계 영역에서 수행 중인 프로세스는 인터럽트가 불가능
            - 임계 구역의 자원, 데이터는 여러 프로세스가 사용해야 하므로 임계 구역 내에서의 작업은 신속하게 이루어져야 함

    - 상호 배제 기법(Mutual Exclusion)
        - 상호 배제(Mutual Exclusion)는 특정 프로세스가 공유 자원을 사용하고 있을 경우 다른 프로세스가 해당 공유 자원을 사용하지 못하게 제어하는 기법을 의미
            - 공유 자원을 사용할 때 여러 프로세스가 번갈아가며 공유 자원을 사용하도록 하여 임계 구역을 유지하는 기법
            - 상호 배제 기법을 구현하기 위한 방법은 소프트웨어적 구현과 하드웨어적 구현이 있다.
                - 소프트웨어적 구현 방법 :
                    데커(Dekker) 알고리즘, 피터슨(Perterson) 알고리즘 => 2개의 프로세스 기준
                    Lamport의 빵집 알고리즘 => 여러 개의 프로세스 기준
                - 하드웨어적 구현 방법 :
                    Test & Set 기법과 Swap 명령어 기법이 있다.

    - 동기화 기법(Synchronization)
        - 동기화 기법(Synchronization)은 두 개 이상의 프로세스를 한 시점에서는 동시에 처리할 수 없으므로 각 프로세스에 대한 처리 순서를 결정하는 것으로, 상호 배제의 한 형태
            - 동기화를 구현할 수 있는 방법으로는 세마포어(Semaphore)와 모니터(Monitor)가 있다.

    - 세마포어(Semaphore)
        - '신호기', '깃발'을 뜻하며, 각 프로세스에 제어 신호를 전달하여 순서대로 작업을 수행하도록 하는 기법
        - 다익스트라가 제안했으며, P와 V라는 두 개의 연산에 의해 동기화를 유지시키고 상호 배제의 원리를 보장한다.
        - 정리하자면 세마포어(S)는 프로세스가 임계 구역에 들어가려할 때(wait) 값이 감소하고, 임계 구역의 작업을 끝내고 반납할 때(signal) 값이 증가
          만약 세마포어가 0이 된다면 모든 자원들이 프로세스들에 의해 사용중이라는 것을 나타내며, 자원을 사용하기 위해서는 세마포어가 0보다 커지기를 기다려야 함

    - 모니터(Monitor)
        모니터(Monitor)는 동기화를 구현하기 위한 특수 프로그램 기법으로 특정 공유 자원을 프로세스에게 할당하는데 필요한 데이터와 이 데이터를 처리하는 프로시저로
        구성 사실 세마포어(Semaphore)는 오래된 동기화 도구라고 할 수 있다. 그래서 현재 사용되는 도구 중 하나가 모니터이며 특히 자바 프로그램에서는 모니터에 대한 활용이 높다.

        모니터의 경우 두 개의 Queue 가 있는데 각각 배타동기와 조건동기의 역할을 한다.
        배타동기의 Queue 는 하나의 스레드(thread)만 공유자원에 접근할 수 있게 하는 공간이다. 특정 스레드(thread)가 공유 자원을 사용하는
        함수를 사용하고 있으면 다른 스레드(thread)는 대기해야 한다.
        조건동기의 Queue 는 진입 스레드(thread)가 블록되면서 새 스레드(thread)가 진입가능하게 하는 공간이다. 새 스레드(thread)는
        조건동기로 블록된 스레드(thread)를 깨울 수 있다. 깨워진 스레드(thread)는 현재 스레드(thread)가 나가면 재진입할 수 있다.

        자바의 모든 객체는 모니터가 될 수 있다. 배타 동기는 synchronized 키워드를 통해 지정할 수 있다.
        조건 동기는 wait(), notify(), notifyAll() 함수를 사용한다. 배타 동기를 선언해주는 Synchronized 는 적어주는 것만으로도 상호 배제 원리를 만족시키는 함수로 만들어준다.
        조건 동기의 경우 wait() 함수를 실행하면 진입 쓰레드를 조건 동기 Queue 에 블록 시키며, notify() 함수는 그렇게 블록된 함수를
        새로운 스레드(thread)가 실행하는 방식으로 깨우게 되고, notifyAll()은 모든 스레드(thread)를 깨우는 것으로 사용할 수 있다.


- 메모리 관리 전략
    - 제한된 물리 메모리의 효율적인 사용과 메모리 참조 방식을 제공하기 위해 필요

    - 연속 메모리 할당
        - 프로세스를 메모리에 연속적으로 할당하는 기법
        - 할당과 제거를 반복하다보면 Scattered Holes가 생겨나고 이로 인한 외부 단편화가 발생

        - Scattered Holes
            - 연속 메모리 할당에서 외부 단편화를 줄이기 위한 방식 3가지
                최초 적합(First-fit)
                    메모리를 순차적으로 탐색하여 제일 먼저 발견한 적절하게 들어갈 수 있는 곳을 찾아 프로세스를 적재하는 방법이다.

                최적 적합(Best-fit)
                    메모리를 탐색하여 메모리 공간 중에서 제일 적절하게 들어갈 수 있는 곳을 찾아 프로세스를 적재하는 방법이다.
                    메모리 공간의 크기와 프로세스의 크기 차이가 제일 적은 경우

                최악 적합(Worst-fit)
                    메모리에 넣는데 크기와 제일 안 맞는 공간(프로세스보다 큰 메모리 공간 중에서)에 프로세스를 넣는 방식이다.

    - 페이징
        - 패이징이란 논리주소의 고정된 페이지라고 불리는 블록들로 분할 관리하는 기법
        - 각각의 페이지는 물리 메모리의 프레임과 맵핑
        - 페이지를 가리키는 논리주소에서 프레임을 가리키는 물리주소로 변환
        - 결론은 외부 단편화를 보완하기 위해 사용

        1.logical address를 동일한 크기로 자름(고정 분할)
        2.physical address도 이것과 동일한 크기로 자름
        3.A,B프로세스의 고정 분할된 일부의 page는 물리메모리의 어디에 배치되고 어디에 배치되고.... 계속 하게 되면 외부 단편화가 발생하지 않음

    - 세그멘테이션
        - 페이징기법과 반대로 논리 메모리와 물리 메모리를 같은 크기가 아닌 다른 크기의 논리적 단위인 세크멘트로 분할
        - 세그먼트의 크기는 일반적으로 같지 않다.
        - 메모리를 자르는 방법을 빼고 메모리에 힐당하는 방법은 페이징 기법과 방 같다.
        - 세그멘테이션 페이징 혼용 기법
            - 페이징과 세그멘테이션도 각각 내부 단편화와 외부 단편화가 발생
            - 페이징과 세그멘테이션을 혼용해 이러한 단편화를 최대한 줄이는 전략
            - 프로세스를 세그먼트(논리적 기능 단위)로 나눈 다음 세그먼트를 다시 페이지 단위로 나누어 관리
            - 매핑 테이블을 두 번 거쳐야하므로 속도가 느려짐


- 가상 메모리
    - 가상 메모리 기법은 프로그램 전체가 아닌 필요한 일부부만 실제 메모리에 올리는 기업입니다.
    - 실제 사용하는 메모리는 작다는 점에서 고안된 기술
    - 즉, 가상메모리는 프로세스의 물리 메모리와 논리 메모리를 분리하기 위해 생겨난 것입니다.

    - 가상 메모리를 사용하는 이유
        - 각 프로세스마다 충분한 메모리를 할당하여 사용하기에는 메모리 크기에 한계가 있다.
        - 프로세스간 메모리 영역간 침범 방지

    - '가상메모리가 없는 경우'
        RAM의 메모리가 4GB라고 하고, 프로세스 A,B에 필요한 메모리가 4GB이라면
        메모리에 프로세스 A가 먼저 할당이 된다면 , 프로세스 B는 할당받을 메모리가 부족하여 사용할 수 없다.
    - '가상메모리가 있는 경우'
        RAM의 메모리가 4GB라고 하고, 프로세스 A,B,C가 있다고 한다면,
        프로세스가 현재 사용되는 메모리 만큼만 물리 메모리(RAM)에 할당과 해제를 반복하여
        메모리를 사용한다면 여러 프로세스가 사용 할 수 있다.


- 캐시의 지역성 원리
    - 캐시 메모리는 속도가 빠른 장치와 느린 장치간의 속도차에 따른 병목 현상을 줄이기 위한 범용 메모리이다.
      이러한 역할을 수행하기 위해서는 CPU 가 어떤 데이터를 원할 것인가를 어느 정도 예측할 수 있어야 한다.
        - 캐시 메모리에서 원하는 데이터를 찾는다면 메인 메모리까지 가서 찾지 않아도 되기 때문에 성능 향상
        - 캐시 메모리에 원하는 데이터에 적중률 성능의 관건
        - 이때 적중율을 높이기 위해 데이터 지역성 원리를 사용한다고 한다.

    - 시간 지역성
        - 최근에 참조된 주소의 내용은 곧 다음에 다시 참조되는 특성
        ex) for나 while 같은 반복문에 사용하는 조건 변수처럼 한번 참조된 데이터는 잠시 후에 또 참조될 가능성이 높다

    - 공간 지역성
        - 대부분의 실제 프로그램이 참조된 주소와 인접한 주소의 내용이 다시 참조되는 특성
        - 캐시 메모리에 데이터를 저장할 때 공간 지역성을 최대한 활용하기 위해 해당 데이터뿐만 아니라 옆 주소의 데이터도 같이 가져와 미래에 쓰일 것을 대비한다.
        ex) A[0], A[1]과 같은 데이터 배열에 연속으로 접근할 때 참조된 데이터 근처에 있는 데이터가 잠시 후에 사용될 가능성이 높다


- 교착상태(데드락, Deadlock)의 개념과 조건
    - DeadLock의 개념
        - 프로세스가 자원을 얻지 못해 다음 처리를 하지 못하는 상태로, ‘교착 상태’라고도 하며 시스템적으로 한정된 자원을 여러 곳에서 사용하려고 할 때 발생
        - 멀티 프로그래밍 환경에서 한정된 자원을 사용하려고 서로 경쟁하는 상황이 발생 할 수 있습니다. 어떤 프로세스가 자원을 요청 했을 때
          그 시각에 그 자원을 사용할 수 없는 상황이 발생할 수 있고 그 때는 프로세스가 대기 상태로 들어 가게됩니다. 대기 상태로 들어간 프로세스들이
          실행 상태로 변경 될 수 없을 때 이러한 상황을 교착 상태라 합니다.

    - 데드락 (Dead lock)의 발생 조건
        - 교착 상태는 한 시스템 내에서 다음의 네 가지 조건이 동시에 성립 할 때 발생합니다
        - 따라서, 아래의 네 가지 조건 중 하나라도 성립하지 않도록 만든다면 교착 상태를 해결할 수 있습니다.
            - 상호 배제 (Mutual exclusion)
                - 자원은 한 번에 한 프로세스만이 사용할 수 있어야 한다.
            - 점유 대기 (Hold and wait)
                - 최소한 하나의 자원을 점유하고 있으면서 다른 프로세스에 할당되어 사용하고 있는 자원을 추가로 점유하기 위해 대기하는 프로세스가 있어야 한다.
            - 비선점 (No preemption)
                - 다른 프로세스에 할당된 자원은 사용이 끝날 때까지 강제로 빼앗을 수 없어야 한다.
            - 순환 대기 (Circular wait)
                - 프로세스의 집합 {P0, P1, ,…Pn}에서 P0는 P1이 점유한 자원을 대기하고 P1은 P2가 점유한 자원을 대기하고 P2…Pn-1은 Pn이 점유한 자원을 대기하며 Pn은 P0가 점유한 자원을 요구해야 한다.

    - 데드락 (Dead lock) 처리
        - 교착 상태 예방(Prevention) 및 회피(Avoidance)
            <예방(Prevention)법> : 교착 상태 발생 조건 중 하나를 제거함으로써 해결하는 방법 - 자원의 낭비가 심하다.
            <회피(Avoidance)법> : 교착 상태가 발생하면 피해나가는 방법
            - 은행원 알고리즘 (Banker’s Algorithm)
                E,J,Dijkstra가 제안한 방법으로, 은행에서 모든 고객의 요구가 충족되도록 현금을 할당하는 데서 유래한 기법이다.프로세스가 자원을 요구할 때
                시스템은 자원을 할당한 후에도 안정 상태로 남아있게 되는지를 사전에 검사하여 교착 상태를 회피하는 기법 안정 상태에 있으면 자원을 할당하고,
                그렇지 않으면 다른 프로세스들이 자원을 해지할 때까지 대기함 교착 상태가 되지 않도록 보장하기 위하여 교착 상태를 예방하거나 회피하는 프로토콜을 이용하는 방법

        - 교착 상태 탐지 및 회복
            - 교착 상태가 되도록 허용한 다음에 회복시키는 방법 -> - 교착 상태 무시
            - 대부분의 시스템은 교착 상태가 잘 발생하지 않으며, 교착 상태 예방, 회피, 탐지, 복구하는 것은 비용이 많이 든다.

            <교착 상태 탐지 (Detection)>
            - 자원 할당 그래프를 통해 교착 상태를 탐지할 수 있다. - 자원을 요청할 때마다 탐지 알고리즘을 실행하면 그에 대한 오버헤드가 발생한다
            <교착 상태로부터 회복 (Recovery)> - 교착 상태를 일으킨 프로세스를 종료하거나, 할당된 자원을 해제함으로써 회복하는 것을 의미한다. - 프로세스를 종료하는 방법
            1. 교착 상태의 프로세스를 모두 중지
            2. 교착 상태가 제거될 때까지 한 프로세스씩 중지 - 자원을 선점하는 방법
            1. 교착 상태의 프로세스가 점유하고 있는 자원을 선점하여 다른 프로세스에게 할당하며, 해당 프로세스를 일시 정지 시키는 방법
            2. 우선 순위가 낮은 프로세스, 수행된 횟수가 적은 프로세스 등을 위주로 프로세스의 자원을 선점한다.


- 사용자 수준 스레드와 커널 수준 스레드
    - 사용자 레벨 스레드는 말그대로 우리가 #include 혹은 import를 통해 스레드를 이용하는 것을 의미한다.
    - 커널 레벨 스레드는 커널 내에 있는 스레드를 의미하게 되고 위와같이 3가지 방법으로 나뉜다.

    - Pure user-level
        커널 스레드 1개당 사용자 스레드 n개를 의미한다. 즉, 1 : n 방식이다.
        이 방식같은 경우에는 커널은 사용자 스레드가 100개가 있어도 전혀 모르기 때문에 사용자 스레드에서 I/O가 하나라도 발생하면 해당 프로세스는 I/O가 풀릴 때 까지 영원히 block된다.
    - Pure Kernel-level
        n개의 커널 스레드가 n개의 사용자 스레드를 담당하게 된다. 즉 1:1 방식이다. 1:1 방식이기에 병렬성은 좋으나 효율성 면에서 다소 떨어진다.
    - Combined
        커널 스레드와 사용자 스레드를 혼합하여 사용하는 방식이다. 위의 두 방식의 장점을 혼합한 방식이라 생각 할 수 있다.

    - 커널 레벨 스레드
        - 정의
            - 커널 스레드는 가장 가벼운 커널 스케쥴링 단위다.
            - 하나의 프로세스는 적어도 하나의 커널 스레드를 가지게 된다.
            - 커널 영역에서 스레드 연산을 수행하게 된다.
            - 커널이 스레드를 관리하기 때문에 커널에 종속적이다.
            - 프로그래머 요청에 따라 스레드를 생성하고 스케줄링하는 주체가 커널이면 커널 레벨(Kernel Level) 스레드라고 한다.

        - 장점
            - 프로세스의 스레드들을 몇몇 프로세서에 한꺼번에 디스패치 할 수 있기 때문에 멀티프로세서 환경에서 매우 빠르게 동작한다.
            - 다른 스레드가 입출력 작업이 다 끝날 때까지 다른 스레드를 사용해 다른 작업을 진행할 수 있다.
            - 커널이 각 스레드를 개별적으로 관리할 수 있다.
            - 커널이 직접 스레드를 제공해 주기 때문에 안정성과 다양한 기능이 제공된다.

        - 단점
            - 스케줄링과 동기화를 위해 커널을 호출하는데 무겁고 오래걸린다.(저장한 내용을 다시 불러오는 과정이 필요)
            - 즉, 사용자 모드에서 커널 모드로의 전환이 빈번하게 이뤄져 성능 저하가 발생한다.
            - 사용자가 프로그래밍할 때 구현하기 어렵고 자원을 더 많이 소비하는 경향이 있다.

    - 사용자 레벨 스레드
        - 정의
            - 사용자 영역에서 스레드 연산을 수행한다.
            - 사용자 영역에서 스레드 연산을 수행하기 때문에 운영체제에 투명하다.
            - 커널에 의존적이지 않은 형태로 스레드의 기능을 제공하는 라이브러리를 활용하는 방식이 사용자 레벨(User Level) 스레드다.

        - 장점
            - 운영체제에서 스레드를 지원할 필요가 없다.
            - 스케줄링 결정이나 동기화를 위해 커널을 호출하지 않기 때문에 인터럽트가 발생할 때 커널 레벨 스레드보다 오버헤드가 적다.
            - 즉, 위의 말은 사용자 영역 스레드에서 행동을 하기에 OS Scheduler의 context switch가 없다(유저레벨 스레드 스케줄러를 이용).
            - 커널은 사용자 레벨 스레드의 존재조차 모르기 때문에 모드 간의 전환이 없고 성능 이득이 발생한다.

        - 단점
            - 시스템 전반에 걸친 스케줄링 우선순위를 지원하지 않는다. (무슨 스레드가 먼저 동작할 지 모른다.)
            - 프로세스에 속한 스레드 중 I/O 작업등에 의해 하나라도 블록이 걸린다면 전체 스레드가 블록된다.

    * 커널?
        운영체제의 다른 모든 부분에 여러 기본적인 서비스를 제공하고 컴퓨터 하드웨어와 프로세스의 보안을 책임진다. 한정된 시스템 자원을 효율적으로 관리하여 프로그램의 실행을 원활하게 한다.

    * Context Switching
        현재 진행하고 있는 Task(Process, Thread)의 상태를 저장하고 다음 진행할 Task의 상태 값을 읽어 적용하는 과정


- 스와핑(Swapping)
    프로그래밍 언어에서 스왑(Swap)이라는 단어는 두 개의 값을 바꾼다는 뜻을 가지고 있다. OS 에서의 의미도 이와 크게 다르지 않다.
    예를 들어 유저 공간(주기억장치)이 있는데 이 곳은 최대 10개의 프로세스(Process)를 올릴 수 있다고 가정하고 현재 메모리를 꽉 차지하고 있다.
    이 상황에서 11번째 프로세스가 실행된다면 당연히 메모리가 가득 차서 올릴 수 없다. 그래서 10개의 프로세스 중에서 하나의 프로세스를 잠깐 내리고
    그 사이에 11번째 프로세스를 실행시키는 방법이다. 어떤 하나의 프로세스가 이벤트가 발생하기까지 기다리고 있는데 1시간이 넘었다.
    그렇다면 이 프로세는 앞으로도 쭉 기다릴 확률이 높다. 그렇다면 이 프로세스를 잠깐 이 공간(주기억장치)에서 쫓아내고 이 공간에 새로운 프로세스를 넣는 것이다.
    당연히 쫓아낼 때 그냥 쫓아내면 안되고 잠깐 다른 곳에 저장을 해놔야한다. 그리고 그 다른 곳은 하드디스크, SSD 와 같은 Secondary storage(보조기억장치)가 된다.
    이렇게 프로세스 단위로 쫓아내는 것을 Swap out 이라고 한다.
    시간이 지나고 아까 그 쫓아낸 프로세스 이벤트 요청이 왔다. 그럼 이 프로세스를 다시 메모리에 올려서 실행시키면 되고 이렇게 다시 메모리로 로딩하는 것을 Swap in 이라고 한다.
    이렇게 프로세스 단위로 Swap in, Swap out 하는 것을 Swapping 이라고 한다.
        - 하드디스크(보조기억장치)에 있던 것을 메모리에 다시 로딩하고 실행한다. 이렇게 교체 시스템에서 나타나는 Context Switching 시간은 생각보다 꽤 높다.
        - 시간은 오래 걸리지만 부족한 메모리에 더 많은 프로세스를 실행할 수 있다는 큰 장점이 있다.
        - 이러한 Swapping 기법은 이후 가상 메모리 관리 방법인 페이징 기법으로 발전했다.


- 메모리 단편화(Memory Fragmentation)
    - 주기억장치에서 메모리의 공간이 작은 조각으로 나누어져, 사용하기에 충분한 양의 메모리가 존재는 하지만 사실상 사용이 불가능한 경우 메모리 단편화가 발생했다고 한다.
      이러한 메모리 단편화는 내부 단편화(Internal Fragmentation)와 외부 단편화(External Fragmentation)로 구분할 수 있다.
        - 내부 단편화(Internal Fragmentation)
            필요한 양보다 더 큰 메모리가 할당이 되어 할당 된 메모리 내부에 사용하는 메모리 공간 이외에 사용하지 않는 메모리 공간이 발생했을 때를 말한다.

        - 외부 단편화(External Fragmentation)
            메모리가 할당이 되고 해제가 되는 작업이 반복될 때 작은 단위의 메모리가 띄엄띄엄 존재하게 된다.
            이 때, 비어있는 메모리의 공간은 충분한 양이지만 실제로 사용할 수 없는 경우를 말한다.

        - 이러한 메모리 단편화를 해결하는 방법에는 여러가지가 있다.
            - 압축(Compaction) 기법
                메모리 공간들을 하여, 단편화로 인해 분산되어 있는 메모리 공간들을 하나로 합치는 기법
            - 통합(Coalescing) 기법
                단편화로 인해 분산된 메모리 공간들을 인접해 있는 것끼리 통합시켜 큰 메모리 공간으로 합치는 기법

            압축기법과 통합기법의 차이는 압축은 재배치가 일어나지만 통합은 인접한 공간끼리 통합된다는 차이가 있다. 하지만 이렇게 비어 있는 공간을
            연속적인 공간으로 만들고 움직이는 작업은 메모리를 Copy 해야하고 이 시점에서 반드시 I/O Problem 이 발생하게 된다.
            하드디스크가 병목되기 때문에 좋은 방법은 아니다. 이는 결국 페이징(Paging)의 배경이 된다.


- 페이징(Paging) 기법과 세그멘테이션(Segmentation) 기법
    - 가상메모리
        우선 가상 메모리를 관리하는 기법인 페이징기법과 세그멘테이션기법을 살펴보기전에 가상메모리에 대해 알아보자.
        가상 메모리는 메모리에 로드된 즉, 실행중인 프로세스가 가상의 공간을 참조하여 마치 커다란 물리메모리를 갖고 있는 것처럼 착각하게 만드는 것이다.
        이러한 가상 메모리는 각 프로세스 당 메인 메모리와 동일한 크기로 하나씩 할당되며 그 공간은 보조기억장치(HDD, SSD) 공간을 이용한다.
        프로세스의 일부만 메모리에 로드하고 나머지는 보조기억장치에 두는 형태이다. 이렇게 할당이 되면 MMU(메모리 관리 장치로 CPU 코어 안에 탑재되어서
        가상 주소를 실제 메모리 주소로 변환해주는 장치) 에 의해 물리 주소로 변환이 된다. MMU 를 더 설명하기에는 내용이 길어지기에 생략하기로 한다.
        다만 프로세스를 동작시키기 위해서는 가상 주소가 아닌 실제 물리 주소가 필요하고 가상 메모리에 있는 가상 주소를 물리 주소로 변환시켜주는 장치라고 이해하고 넘어가자.

    - 페이징(Paging) 기법
        비어있는 메모리(Hole)보다 크기 때문에 발생한다. 그리고 이 Hole 의 크기도 제각각, 프로세스의 크기도 제각각이다.
        그래서 이러한 외부 단편화를 완벽히 발생시키지 않기 위한 방법이 페이징(Paging) 기법이다. 페이징은 Logical Address(프로세스) 를 동일한 크기로 자르고,
        Physical Address(메모리) 도 이것과 동일한 크기로 자른다(Frame 단위). 그리고 개별 페이지는 순서에 상관없이 Physical Memory 에 있는
        프레임에 Mapping 되어 저장된다고 볼 수 있다. 그림을 보면 조금 더 쉽게 이해가 된다.

        프로세스를 굳이 연속적으로 배치할 필요가 없다. 그래서 Paging 을 통해 외부단편화 문제를 해결할 수 있다.

        하지만 페이징(Paging)은 내부 단편화 문제를 해결할 수는 없다.
        프로세스의 크기가 나누어져있는 Page 의 크기에 딱 맞게끔 나누어진다는 보장이 없기 때문이다.
        한 페이지의 크기가 7K 일때 45K 의 프로세스를 실행한다고 하면 3K의 내부 단편화가 발생할 수 밖에 없다. 그럼 페이지의 크기를 더 잘게 쪼개면 되지 않느냐고 할 수 있다.
        하지만 페이지의 크기가 너무 작아질수록(혹은 너무 커질수록) 발생되는 문제점들이 있다.
            - 페이지 단편화 감소(증가)
            - 한 개의 페이지를 주기억장치로 이동하는 시간이 줄어든다.(늘어남)
            - 프로세스 수행에 필요한 내용만 주기억장치에 적재 가능(불가능)
            - 기억장치 효율성이 높아짐(낮아짐)
            - 페이지 맵 테이블의 크기가 커짐(작아짐)
            - Mapping 속도가 늦어진다.(빨라짐)
            - 입/출력 시간의 증가(감소)

    - 세그멘테이션(Segmentation) 기법
        - Paging 기법은 외부 단편화 문제를 해결할 수 있었지만, 내부 단편화 발생의 문제가 있었다. 그래서 내부 단편화 문제를 해결하기 위한 방법이 세그멘테이션(Segmentation) 기법이다.
          프로세스를 일정한 단위인 페이지 단위로 잘랐다면 물리적인 단위인 세그먼트 단위로 자르는 것이 세그멘테이션 기법이다.
        - 세그멘테이션은 프로세스를 세그먼트(서로 다른 크기를 가지는 논리적인 블록이 연속적인 공간에 배치되어 있는것)의 집합으로 생각한다.

        세그먼트라고 부른다. 그리고 이렇게 영역별로 쪼개는 기술을 세그멘테이션이라고 한다.
        사실 하나의 프로세스가 동작하기 위해서는 기본적으로 코드, 데이터, 스택 세 가지의 세그먼트는 항상 가지고 있다. 그리고 더 들어가 본다면
        코드에서도 main 함수가 있고 다른 함수들 또한 존재할 수 있다. 데이터를 본다고 해도 어떠한 구조체가 존재할 수도, 배열이 있을 수도 있다.
        그래서 세그멘테이션은 물리적인 크기의 단위가 아닌 논리적 내용의 단위로 자르기 때문에 세그먼트들의 크기는 일반적으로는 같지 않다.
        세그멘테이션을 사용하면 세그먼트 테이블(Segment Table)이라고 세그먼트의 시작주소(base)와 세그먼트의 길이(limit) 정보가 담긴 테이블을 운용한다.
        이로 인해 각각의 세그먼트를 offset 단위로 관리할 수 있다.

        MMU(CPU 코어 안에 탑재되어서 가상 주소를 실제 메모리 주소로 변환해주는 장치) 내의 재배치 레지스터를 이용하여 논리 주소를 물리 주소로 바꾸어 주는 방식을 취한다.
        MMU 는 세그먼트 테이블로 CPU 에서 할당한 논리 주소에 해당하는 물리 주소의 위치를 가지고 있다. 세그먼트는 세그먼트 테이블에 연속적으로 저장되기 때문에
        CPU 는 프로세스가 연속된 메모리 공간에 위치한다고 착각을 하게 된다. 하지만 세그멘테이션 기법은 동적 할당(세그먼트들의 크기가 다르기 때문에
        미리 분할해 둘 수 없고 그때 그때 빈 공간을 찾아 적재)이기 때문에 외부 단편화가 발생할 수 있다.

    - 페이징(Paging) + 세그멘테이션(Segmentation)
        페이징은 내부 단편화 문제를 가지고 있고 세그멘테이션은 외부 단편화 문제를 가지고 있다. 그래서 두 방식을 모두 사용하여 장점만을 가져오고자 한다.
        따라서 세그멘트를 페이징하는 방식을 취한다. A 우선 프로세스를 처음에 세그먼트 단위로 자른다. 의미 있는 단위로 나누게 되면 정보 보호와 공유를 하는 측면에서
        세그먼트의 이점을 가져오면서 내부단편화를 막는다. 하지만 외부단편화가 발생할 수 있기 때문에 잘라진 세그먼트를 다시 일정 간격인 페이지 단위로 자르는 페이징 기법을 취한다.
        그래서 메모리에 적재하게 되면 페이징의 일정 단위로 다시 잘렸기 때문에 외부 단편화가 발생하지 않는다. 하지만 이와 같은 경우에는 세그먼트 테이블과
        페이징 테이블 두 가지를 모두 겨쳐야하므로 속도 면에서는 조금 떨어질 수 있다. 실제로 x86 메모리 관리를 이런 식으로 두 가지 기법을 섞어 작업을 처리한다.


- Block/Non-Block
    - Block(막다)상황 제어권이 호출자에서 실행함수로 넘어간 경우
      함수가 완료되기 전까지는 호출자는 제어권이 없다.

    - Non-Block(막다)상황 제어권이 호출자에서 함수를 실행해도(함수가 실행중이여도) 호출자에게 바로 제어권이 넘어간다.





- thread save란?

- 갑자기 cpu가 높아지면?