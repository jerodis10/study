- 메모리 계층 (상-하층 순)
    - 레지스터
    - 캐시
    - 메모리
    - 하드디스크

- 외부 단편화와 내부 단편화란?
    - 외부 단편화 : 작업보다 많은 공간이 있더라도 실제로 그 작업을 받아들일 수 없는 경우 (메모리 배치에 따라 발생하는 문제)
    - 내부 단편화 : 작업에 필요한 공간보다 많은 공간을 할당받음으로써 발생하는 내부의 사용 불가능한 공간

- 사용자 수준 스레드 vs 커널 수준 스레드 차이는?
  #사용자 수준 스레드
  장점 : context switching이 없어서 커널 스레드보다 오버헤드가 적음 (스레드 전환 시 커널 스케줄러 호출할 필요가 없기 때문)
  단점 : 프로세스 내의 한 스레드가 커널로 진입하는 순간, 나머지 스레드들도 전부 정지됨 (커널이 스레드의 존재를 알지 못하기 때문에)

  #커널 수준 스레드
  장점 : 사용자 수준 스레드보다 효율적임. 커널 스레드를 쓰면 멀티프로세서를 활용할 수 있기 때문이다. 사용자 스레드는 CPU가 아무리 많아도 커널 모드의 스케줄이 되지 않으므로, 각 CPU에 효율적으로 스레드 배당할 수가 없음
  단점 : context switching이 발생함. 이 과정에서 프로세서 모드가 사용자 모드와 커널 모드 사이를 움직이기 때문에 많이 돌아다닐 수록 성능이 떨어지게 된다.

- Race Condition이란?
    - 두 개 이상의 프로세스가 공통 자원을 병행적으로 읽거나 쓸 때, 공용 데이터에 대한 접근이 순서에 따라 실행 결과가 달라지는 상황
    - ace Condition이 발생하게 되면, 모든 프로세스에 원하는 결과가 발생하는 것을 보장할 수 없음. 따라서 이러한 상황은 피해야 하며 상호배제나 임계구역으로 해결이 가능하다.

- 프로세스의 상태도 저장된다고 했는데 프로세스의 상태는 어떤것들이 있나요?
    - 실행 : CPU를 점유하고 있는 상태이며 실행되고 있는 상태입니다.
    - 준비 : CPU만 할당 받는다면 즉시 수행가능합니다.
    - 봉쇄 : CPU할당 받아도 실행못하는 상태입니다. 예를 들어 입출력 작업은 CPU가 할 수 있는일이 없기 때문에 다른 장치로부터 신호가 와야합니다.


- 스레드는 왜 독립적인 스택 메모리를 가지나요?
    - 스택영역의 정의부터 정확하게 알아야합니다. 스택영역은 스레드가 프로세스 내에서 함수호출시 전달되는 인자, 함수의 리턴주소등을 저장하기 위한 메모리 영역입니다.
      스레드가 하나의 프로세스 내에서 독립적인 기능을 하기 위해서는 독립적인 함수를 호출해야함을 의미하며 이는 독립적인 스택 메모리를 가져야만 가능합니다.


- 멀티 프로세스와 멀티 스레드를 비교해주세요.
    - 문맥교환시 멀티스테드는 스택영역의 내용만 변경되면 되지만 멀티 프로세스의 경우 힙,데이터,코드 영역의 정보까지 바뀌므로 문맥교환의 비용이 멀티프로세스가 높습니다.
    - 멀티 프로세스는 많은 CPU시간과 메모리공간을 차지하며 하나의 프로세스가 죽더라도 다른 프로세스에 영향을 주지 않습니다.
    - 멀티 스레드는 적은 메모리 공간을 차지하고 문맥교환 비용이 적으며 자원을 공유하기 때문에 동기화 문제가 발생할 수 있고 하나의 스레드 장애가 여러 스레드의 장애로 이어질 위험이 있습니다.


- 멀티 프로세스들이 실행되는 과정을 간략하게 표현해주세요.

- 프로세스간 통신은 어떻게 하나요?
    - 스레드는 서로의 자원을 공유하고 있기 때문에 스레드간 통신은 별도의 자원을 사용하지 않아도 됩니다.
      하지만 프로세스의 경우 데이터를 공유하고 있지 않기 때문에 공유메모리를 사용하는 방법과 메세지전달 방법으로 2가지 방법이 있습니다.
    - 공유 메모리 : 프로세스들이 주소 공간 일부를 공유하는 것을 의미합니다. 커널이 메모리에 별도의 shared memory를 할당합니다.
      프로세스들은 shared memory의 정보를 이용해 서로 데이터를 전달 및 공유 합니다.
    - 메세지 전달 : 시스템 콜을 이용하여 구현합니다. 시스템 콜이란 커널영역에서 사용하는 명령어들이며 커널을 통해 메세지를 보내고 받습니다.
      즉, 프로세스들 사이에 커널이 데이터를 중재하는 것입니다. 예를 들어, pipe , socket , message queue 가 있습니다.
    - 2가지 방식의 장단점 역시 존재합니다. 공유메모리를 사용할 경우 간단하지만 메모리를 공유하기 때문에 자원 동기화의 문제가 있을 것이며 메세지를 전달한다면 동기화 문제는 없지만 속도가 느립니다.


- 멀티 프로세스 , 스레드 환경에서 동기화 문제는 어떻게 해결하나요?
    - 해결하는 방법은 뮤텍스 또는 세마포어를 사용합니다.
    - 뮤텍스 : 1개의 스레드만 공유자원에 접근가능합니다.
    - 세마포어 : S개의 스레드만이 공유자원에 접근가능합니다. 스레드가 접근할 때 마다 S의 개수를 감소시키며 S가 0이라면 자원에 접근 할 수 없습니다.

    - critical section : 공유할 자원들 즉, 코드 영역에서 여러 스레드들이 접근 할 수 있는 부분을 통제하기 위해서 둘 이상의 프로세스 또는
      스레드가 접근하는 것을 막는 코드 부분을 임계영역 (critical section)이라고 합니다. 또한 임계영역내에서 코드들은 원자적으로 실행되어야 합니다.
    - entry section : 진입 허가 요청이 완료되어야 임계영역으로 갈수 잇습니다.
    - exit section : 임계영역 실행후 끝나는 부분
    - entry section -> critical secition -> exit section 으로 이동합니다.


- 운영체제는 프로세스를 메모리로 올릴 때 메모리 관리를 어떻게 하나요?
    - 프로세스를 통째로 연속된 메모리에 올려놓는 일은 거의 없습니다. 이때까지 편의를 위해 그림으로 그렇게 표현했지만 프로세스들을 무작정 물리 메모리에 순서대로 올리면 많은 문제들이 발생합니다.
    - 효율적으로 프로세스들에게 메모리를 할당해주기 위한 기법이 생기기 시작했습니다. 바로 페이징과 세그멘테이션입니다.


- 페이징(Paging)은 무엇인가요?
    - 물리 메모리를 동일한 크기로 조각조각 냅니다. 이때 하나의 조각을 페이지라고 합니다. 메모리에 올릴 프로세스도 페이지의 크기만큼 조각내서
      물리 메모리에 저장하는 방식입니다. 그렇게 되면 물리 메모리에 연속되서 저장될 필요없이 페이지 단위로 분산되어 저장 가능합니다.
      프로세스 입장에서는 자신의 페이지들이 어디있는지 알아야하는데 이를 위해 페이지 테이블이라는 것이 존재합니다.
    - 프로세스당 하나의 페이지 테이블을 가집니다. 하나의 프로세스 내에서 페이지단위로 물리적메모리에 저장되기 때문에 주소를 바인딩하기위해 별도의 테이블이 필요합니다.
      물리메모리 주소를 프레임번호라고도 합니다.


- 페이징 기법에서 문제점이 있나요?
    - 메모리 내부 단편화 문제가 발생합니다. 프로세스를 페이지 단위로 쪼개다보면 페이지 안에 안쓰는 공간이 남습니다.
      예를 들어 100 크기의 프로세스를 30크기의 페이지로 쪼개면 4개의 페이지로 나오고 마지막 페이지는 20이 남습니다.
      이러한 페이지를 물리메모리에 할당하게 되면 20이라는 공간은 아무것도 안하고 남아있는 공간이 됩니다.
    - 외부 단편화 문제는 발생하지 않습니다. 외부단편화는 남아있는 공간은 작은데 그 공간보다 큰 프로세스를 집어넣으려고 할때 발생하는데
      프로세스를 페이지 단위로 나누기 때문에 페이지 단위로 무조건 할당됨을 보장하므로 메모리를 할당하지 못해서 나오는 외부단편화 문제는 발생하지 않습니다.


- 세그먼테이션(Segmentation)이 무엇인가요?
    - 페이지는 프로세스를 크기단위로 똑같이 나뉘었다면 세그먼테이션은 의미단위로 메모리를 나누는 것을 의미합니다.
      프로세스를 코드,데이터,힙,스택으로 기능 단위로 정의한다면 4가지로 조각내어 메모리에 저장합니다.
      세그먼테이션은 페이징과 마찬가지로 물리적 주소로 변환하기 위한 세그먼테이션테이블을 가지고 있습니다.
    - 만약 논리적 주소가 2 / 200 이라면 어떨까요? 2는 세그먼트 번호를 의미하고 200은 떨어진 거리를 의미합니다.
      따라서 세그먼트2번의 주소가 3500 부터 시작하고 3500에서 200떨어진 거리인 3700이 2/200 의 논리적주소가 변경된 물리적 주소입니다.
    - 이처럼 세그먼트는 모든 크기가 다르기 때문에 항상 시작점과 limit를 같이 표기해야합니다.


- 세그먼테이션의 단점이 있나요?
    - 세그먼테이션은 메모리의 내부단편화 문제가 없습니다. 왜냐하면 필요한 만큼 메모리를 정확하게 할당 받기 때문입니다.
    - 외부 단편화문제가 존재합니다. 세그먼트가 너무 커서 특정 공간에 할당 못할 수도 있습니다. 세그먼트를 쪼개면 가능하겠지만 이는 불가능합니다.


- 가상메모리가 무엇인가요?
    - 운영체제는 메모리를 더욱더 효율적으로 사용하기 위해서 도입한 개념입니다.
    - 정의 : 프로세스 전체가 메모리에 올라오지 않아도 실행이 가능하도록 하는 것입니다.

    - 이때까지 메모리에 프로세스 전체가 올라가는 그림이 그려졌습니다. 하지만 가상메모리는 프로세스 전체가 메모리로 올라오지 않습니다.
      어떻게 메모리에 전체 프로세스를 올리지 않더라도 프로세스를 실행할 수 있는 것일까요?
    - 운영체제는 가상메모리 기법을 통해서 프로그램의 논리적 주소 영역에서 필요한 부분만 물리 메모리에 적재하고 직접적으로 필요하지 않은 부분은 디스크(SWAP)에 저장하는 것을 말합니다.

    - 페이징 기법을 쓰는 운영체제에서 가상메모리를 사용한다고 가정해봅시다. 당장 사용될 주소 공간을 page단위로 메모리에 적재하는 방법을 요구페이징이라고 합니다.
      요구 페이징에서는 특정 페이지에 대해 cpu요청이 들어온 후에 해당 페이지를 메모리에 적재합니다. 당장 실행에 필요한 페이지만을 메모리에 적재하기 때문에
      메모리 사용량이 감소하고 프로세스 전체를 메모리에 적재하는 입출력 오버헤드도 감소합니다. 요구페이징 기법에서는 유효/무효 비트를 두어서 각 페이지가 메모리에 존재하는지 표시하게 됩니다.


- 요구페이징에서 CPU가 요청한 메모리에 없고 디스크에 있다면 어떻게 되나요?
    1) CPU가 페이지 참조
    2) 페이지 테이블을 보니 페이지 상태가 "무효" 상태
    3) MMU에서 페이지 폴트 트랩을 발생
    4) 디스크에서 메모리로 페이지를 가져온 후 "유효" 상태로 변경

    이와 같이 요청한 페이지가 없는 경우를 페이지 폴트(Page fault)라고 합니다.

- 페이지 폴트 발생 후 메모리로 페이지를 가져오려는 데 메모리도 부족한 경우라면 어떤 페이지를 교체하나요?
    - 이를 위한 알고리즘이 존재합니다. 바로 페이지교체 알고리즘입니다.
    - LRU : 가장 오랫동안 사용하지 않은 페이지를 메모리에서 디스크로 방출합니다.
    - LFU : 참조 횟수가 가장 적었던 페이지를 메모리에서 디스크로 방출합니다.


- 동기, 비동기, Blocking, Non-Blocking
    - 동기
        - I/O가 진행되는 동안 다음 명령을 수행하지 않고 기다린다.
          I/O 상태의 프로세스는 blocked state로 전환된다.
          I/O가 완료되면 인터럽트를 통해 완료를 알린다. 이후 CPU의 제어권이 기존 프로그램에게 넘어간다.
          blocked state의 프로세스는 wait 상태로 돌아간다.

        - 여러 프로세스가 동시에 I/O 요청을 할 경우 각 요청을 큐에 넣어 순서대로 처리한다.

    - 비동기
        - CPU의 제어권을 입출력 연산을 호출한 프로그램에게 곧바로 다시 부여
        - CPU는 I/O 결과와 상관 없이 처리 가능한 작업부터 처리한다.
        - I/O 연산이 완료되면 인터럽트를 통해 알린다.

    - Blocking I/O
        - 직접 제어할 수 없는 대상의 작업(I/O)이 완료될 때까지 기다린다.
            - I/O가 완료되어야 제어권이 프로세스로 넘어간다.
        - 동기와 마찬가지로 자원이 낭비된다.

    - Non-Blocking I/O
        - 외부에 I/O 작업을 하도록 요청한 후, 즉시 다음 작업을 처리
            - 시스템 자원을 더 효율적으로 사용할 수 있게된다.
        - I/O 작업이 완료된 이후에 처리해야 하는 후속 작업이 있다면, I/O 작업이 완료될 때까지 기다려야 한다.
          따라서 이 후속 작업이 프로세스를 멈추지 않도록 만들기 위해 I/O 작업이 완료된 이후 후속 작업이 이어서 진행할 수 있도록 별도의 약속(Polling, Callback function 등)을 한다.

    - 동기/비동기는 인터럽트 발생으로 인한 제어권 반한 시점에 중점을 두고 Blocking/Non-Blocking은 제어권 자체에 중점을 둔다는 점에서 차이가 있다.

- 프로세스와 스레드는 각각 무엇이고 어떤 차이점이 있을까요?
    프로세스는 실행 중인 하나의 애플리케이션입니다.
    운영체제로부터 실행에 필요한 메모리를 할당받아 애플리케이션의 코드를 실행합니다.
    필요한 메모리 영역은 프로그램의 코드를 저장하는 Text 영역, 전역 정적 변수들을 저장하는 Data, 지역 변수들을 저장할 Stack, 동적 메모리 할당을 받을 Heap 영역입니다.

    운영체제에서 각 프로그램들은 동시에 실행할 수 있도록 멀티 프로세스로 동작합니다.
    이는 CPU 자원을 잘게 쪼갠 시간 내에 점유한다는 뜻입니다.
    하나의 프로세스가 CPU 자원을 점유하려는 순간 프로세스의 상태를 불러와야 합니다.
    이 순간에 프로세스 상태를 교체하고 새로운 CPU 레지스터 변수들을 불러오는 작업을 컨텍스트 스위칭(Context Switching)이라고 합니다.
    점유에 벗어나는 순간에 프로세스의 상태를 저장해야 합니다.

    프로세스마다 각자 고유 상태를 갖고 있습니다. 이를 PCB(Process Control Block)이라고 합니다.
    현재 프로세스 상태가 실행 중인지, 대기 중인지를 알려주는 Process State,
    고유 프로세스의 식별값을 나타내는 Process ID,
    다음 실행할 프로그램 코드 위치를 저장하는 Program Counter,
    메모리 정보를 관리할 페이지 테이블과 세그먼트 테이블, CPU의 스케쥴링 큐 포인터와 우선순위 정보, I/O 정보들 등등으로 이루어져 있습니다.

    반면, 스레드는 하나의 프로세스 안에서 실행되는 여러 코드 흐름을 뜻합니다.
    프로세스가 갖고 있는 Text 영역, Data 영역, Heap 영역을 공유하지만, 스레드 내부에서 Stack을 별도로 할당받습니다. 어떤 프로그램이든 하나의 주요 흐름을 실행하는 Main 스레드는 가지고 있습니다.
    Data 영역과 Heap 영역을 공유하기에 다른 스레드와 데이터를 통신할 수 있지만, 어느 스레드가 먼저 실행할 지 모르기에 동기화와 교착 상태에 각별한 주의가 필요합니다.


- 프로그램 코드 실행을 하드웨어단에서 어떻게 하나요?
    프로그램을 실행할 때, 적당한 메모리 위치에 프로그램이 쓸 영역을 올립니다.
    프로그램이 사용할 영역은 방금 말씀드렸다시피 코드를 저장하는 영역, 전역 정적 변수들을 저장하는 영역, 지역 변수들을 저장하는 영역, 동적메모리 할당을 받을 영역으로 나누어져있습니다.

    그 다음 코드를 해석하고 실행하는 장소는 CPU 입니다.
    CPU 에서는 Program Counter 라는 레지스터 변수로 다음 실행할 위치의 코드를 저장하는데요.
    PC 위치에 있는 코드를 메모리로부터 읽어오고 명령어를 해석하여 적절한 행동을 실행합니다.
    다시 PC에 다음 코드의 위치를 저장하고 프로그램이 종료할 때 까지 반복합니다.


- 프로그램을 메모리에 올릴 때, 적당한 메모리 공간의 위치를 어떻게 효율적으로 정할 수 있을까요?
    메모리에 필요한 공간을 할당할 때, 메모리의 빈 공간을 탐색해서 할당합니다.
    모든 프로그램마다 할당받을 메모리의 크기도 모두 다르기에, 무턱대고 할당하고 해제하기를 반복한다면
    메모리의 빈 공간을 탐색하는데 시간이 많이 들 뿐 더러, 불필요하게 빈 메모리가 많이 생길 것 입니다.

    효율적인 메모리 관리 기법으로 할당할 메모리 주소의 탐색 시간을 줄이고 빈 메모리 공간을 줄입니다.
    이를 관리하는 하나의 하드웨어 단위가 있습니다. 바로 MMU (Memory Management Unit) 입니다.

    최초의 메모리 할당 방법은 연속적으로 할당하는 방법입니다. MMU에서 Offset 레지스터로 CPU가 읽은 가상 주소로부터 물리 주소에 매핑을 시켜주는 역할을 합니다. 현재 프로그램 영역을 벗어나지 않도록 Limit 레지스터로 주소 접근을 제한하기도 합니다. 만약에 제한을 넘어갔을 때 인터럽트로 시스템에게 예외를 처리하도록 부탁합니다. 연속적 할당 방법의 장점은 구현이 간단합니다. 그러나 초반에 말했던 단점들은 고스란히 남아있습니다.
    할당과 해제의 과정을 거치면서 메모리 중간에 빈 공간들이 생겨나는데요.

    빈 공간들을 모조리 합치면 새로 들어올 프로그램의 메모리를 옮길 수 있지만,
    연속적으로 할당한다는 특징으로 할당이 불가능합니다.
    총 여유 메모리로 충분히 할당하고도 남지만, 실제로 할당할 수 없는 경우를 외부 단편화(External Fragmentation) 라고 합니다.
    메모리 공간의 빈 공간들을 모두 없애 앞쪽으로 땡기는 Compact 기법이 있습니다. 하나 하나 메모리 영역을 복사하여 빈 공간이 없도록 반복하게 되는데, I/O 문제가 발생할 수 밖에 없습니다.

    이를 해결할 방법은 메모리 영역을 쪼개는 방식입니다. 이를 페이징(Paging) 방식이라고 합니다.
    할당 받은 프로그램의 물리 메모리를 특정 크기의 프레임으로 쪼개 순서에 상관없이 저장합니다.
    그렇다면, 각 프로세스는 어떻게 순서대로 프로그램을 실행할 수 있을까요?
    프로세스 별로 페이지 테이블을 가지고 있습니다. 논리적인 페이징 테이블로 해당하는 실제 메모리 주소에 있는 프레임을 매핑하여 위치를 찾아냅니다.

    페이징 방식의 장점으로 프로그램 메모리를 잘개 쪼개 효율적으로 메모리에 저장하여 외부 단편화를 해소한다는 점이고요. 페이징 방식의 단점으로는 특정 크기의 프레임으로 메모리 영역을 분리하는데요.
    만약에 메모리 크기가 딱 맞지 않다면은 여전히 빈 공간이 생깁니다.
    이를 내부 단편화(Internal Fragmentation)라고 합니다.
    프레임 크기를 줄일수록 페이지 테이블이 커지고 그에 따른 오버헤드는 훨씬 빈번히 발생할 것 입니다.

    물리 메모리에 접근할 때 늘 페이지 테이블에 접근해서 매핑된 물리 메모리에, 총 두 번 접근합니다.
    이를 개선하기 위해 연관 메모리인 TLB(Translation Look-aside Buffer, 변환 색인 버퍼)를 사용합니다.
    페이지 테이블을 대상으로 일종의 캐시 역할을 해줍니다.

    더 나아가 메모리 크기가 커지면서 페이지 테이블을 여러 단계로 계층화 시킨다든지, 해쉬 테이블을 이용한다든지, 하나의 페이지 테이블로 통합시킨다든지 하는 방법들이 나왔습니다.
    자세히는 모르지만 현대에 와서는 멀티 코어 시대인만큼 병렬화를 적극 이용하지 않았을까 싶습니다.


- 프로그램 코드를 읽어올 때 CPU에서 어떻게 효과적으로 읽을 수 있는지 생각해보셨나요?
    CPU와 메모리의 연산 속도는 다르기에 CPU 에서도 효과적으로 메모리로부터 데이터를 읽는 방법이 필요합니다. CPU 내부에 여러 캐시 메모리를 장착하고 있어 최대한 CPU 사이클보다 느린 메모리로부터 데이터나 코드를 불러오는 것을 지양하면서 캐시 메모리를 적극 활용해 속도를 개선합니다.

    코드를 읽을 때, 메모리에서 다음에 실행할 코드 뭉치들을 미리 불러와 캐시 메모리에 저장합니다.
    캐시 메모리에 저장한 명렁어들을 소진될 때 까지 순차적으로 실행해 속도를 개선합니다.


- Context Switching이 일어날 때 운영체제 내에서 어떻게 동작하나요?
    현재 프로그램 코드에서 다른 프로그램 코드 영역으로 옮길 때, 현재 프로세스의 레지스터 정보와 PCB를 저장합니다. 운영체제 내에서 레지스터 정보와 PCB를 저장하는 테이블에 등록합니다.
    그리고 다른 프로세스의 정보를 탐색해 불러옵니다. 다시 원래 프로세스로 돌아올 때 저장했던 테이블에서 탐색해 불러옵니다.


- 스레드 동기화는 어떻게 이루어지나요?
    스레드 간에 객체를 공유하여 수정할 시, 다른 스레드에도 그 정보가 반영되지 않아 예상치 못한 오류가 일어날 수 있습니다.
    멀티 스레드 환경에서 단 하나의 스레드만 실행할 수 있는 코드 영역을 임계 영역이라고 합니다.
    자바에서는 이를 위해 동기화(synchronized) 메소드와 동기화 블록을 제공합니다.
    스레드 내부의 동기화 메소드 또는 블록에 들어가면 즉시 객체에 잠금을 걸어 다른 스레드가 해당 임계 영역 코드를 실행하지 못하도록 합니다.


- 스레드 개수가 많아지면 더 효율적일까요?
    과도한 스레드 개수로 애플리케이션의 성능이 저하될 수 있습니다.
    스레드 개수가 증가할수록, 스레드 생성과 스케줄링으로 CPU 점유율이 올라가고 메모리 사용량이 늘어납니다.

    스레드의 폭증을 막으려면 스레드풀(ThreadPool)을 사용해야 합니다.
    스레드를 제한된 개수만큼 정해 놓고 작업 큐에 들어오는 작업들을 하나씩 스레드가 맡아 처리합니다.
    작업 처리가 끝난 스레드는 다시 작업 큐에서 새로운 작업을 가져와 처리합니다.

    자바에서는 java.util.concurrent 패키지에서 ExecutorService 인터페이스와 Executors 클래스를 제공합니다.
    Executors 클래스의 정적 메소드로 다양한 ExecutorService 구현 객체인 스레드풀을 만들 수 있습니다.

    스레드풀의 종류는 세 가지가 있습니다.
    newCachedThreadPool()은 동적 크기로 처리할 작업이 있을 때 새 스레드를 생성합니다. 60초 동안 추가된 스레드가 아무 작업을 하지 않으면 추가된 스레드를 종료하고 풀에서 제거합니다.
    newFixedThreadPool()은 최대 스레드 개수를 설정하며, 유후 스레드가 있더라도 스레드 개수가 줄지 않습니다.
    newSingleThreadPool()은 하나의 스레드를 생성하며, 작업이 차례대로 실행되며 스레드 안전하다.


- 콘보이 현상(convoy effect)이란 무엇이고, 콘보이 현상이 발생될 수 있는 CPU 스케줄러 알고리즘은 무엇인지 설명해주세요.
    - 콘보이 현상이란 작업 시간이 긴 프로세스가 먼저 큐에 도착해서 다른 프로세스의 실행 시간이 전부 늦춰져 효율성을 떨어뜨리는 현상을 말합니다.
    - FCFS(First-Come First Served) 스케줄링은 비선점형으로, 순차적으로 먼저 큐에 들어온 작업부터 실행하므로 콘보이 현상이 발생할 수 있습니다.


- 선점형 스케줄링과 비선점형 스케줄링의 차이를 설명해주세요.
    - 선점형은 하나의 프로세스가 다른 프로세스 대신에 CPU를 차지할 수 있음을 말하고,
    - 비선점형은 하나의 프로세스가 끝나지 않으면 다른 프로세스는 CPU를 사용할 수 없음을 말합니다.


- Critical Section(임계영역)에 대해 설명해주세요.
    - 임계 영역이란 프로세스간에 공유자원을 접근하는데 있어 문제가 발생하지 않도록 한번에 하나의 프로세스만 이용하게끔 보장해줘야 하는 영역을 말합니다.
    - 임계 영역 문제를 해결하기 위해서는 아래의 3가지 조건을 충족해야 합니다.
        - 상호 배제(Mutual exclution) - 하나의 프로세스가 임계 영역에 들어가 있다면 다른 프로세스는 들어갈 수 없어야 한다.
        - 진행(Progress) - 임계 영역에 들어간 프로세스가 없는 상태에서 들어가려 하는 프로세스가 여러 개라면 어느 것이 들어갈지 결정 해주어야 한다.
        - 한정 대기(Bounded waiting) - 다른 프로세스의 기아를 방지하기 위해, 한 번 임계 구역에 들어간 프로세스는 다음 번 임계 영역에 들어갈 때 제한을 두어야 한다.


- 뮤텍스(Mutex)와 세마포어(Semaphore)의 차이에 대해 설명해주세요.
    - 뮤텍스는 Lock을 사용해 하나의 프로세스나 쓰레드를 단독으로 실행하게 합니다.
      반면에 세마포어는 공유자원에 세마포어 변수만큼의 프로세스(또는 쓰레드)가 접근할 수 있습니다.
    - 현재 수행중인 프로세스가 아닌 다른 프로세스가 세마포어를 해제할 수 있습니다.
      하지만 뮤텍스는 락(lock)을 획득한 프로세스가 반드시 그 락을 해제해야 합니다.


- 페이지 교체 알고리즘에 대해 설명해주세요.
    - 페이징 기법으로 메모리를 관리하는 운영체제에서 필요한 페이지가 주기억장치에 적재되지 않았을 시(페이징 부재시) 어떤 페이지 프레임을 선택해 교체할 것인지 결정하는 방법을 페이지 교체 알고리즘이라고 합니다.

    - FIFO(first in first out)
        가장 간단한 알고리즘으로, 메모리에 올라온 지 가장 오래된 페이지를 교체합니다. 간단하고, 초기화 코드에 대해 적절한 방법이며, 페이지가 올라온 순서를 큐에 저장합니다.
    - 최적(Optimal) 페이지 교체
        앞으로 가장 오랫동안 사용되지 않을 페이지를 교체하는 알고리즘입니다. 최적 페이지 교체는 선행 조건이 있는데, 프로세스가 앞으로 사용할 페이지를 미리 알아야 한다는 것입니다. 이 조건은 실제 활용에선 알 방법이 없기 때문에 최적 알고리즘은 구현이 불가능한 알고리즘입니다. 때문에 연구를 목적으로 주로 사용됩니다.
    - LRU(least-recently-used)
        가장 오래 사용되지 않은 페이지를 교체하는 알고리즘입니다. OPT 알고리즘의 방식과 비슷한 효과를 낼 수 있는 방법이며, OPT 알고리즘보다 페이지 교체 횟수가 높지만 FIFO 알고리즘 보다 효율적입니다.
    - LFU(least-frequently-used)
        참조 횟수가 가장 작은 페이지를 교체하는 알고리즘입니다. 만약 대상인 페이지가 여러 개 일 경우, LRU 알고리즘을 따라 가장 오래 사용되지 않은 페이지로 교체합니다.
    - MFU(most-frequently-used)
        LFU 알고리즘과 반대로, 참조 횟수가 가장 많은 페이지를 교체하는 알고리즘입니다.
    - LFU와 MFU는 실제 사용에 잘 쓰이지 않는다.
        구현에 상당한 비용이 들고,
        최적 페이지 교체 정책을 (LRU 만큼) 제대로 유사하게 구현해내지 못하기 때문이다.


- 페이징과 세그멘테이션
    https://steady-coding.tistory.com/524


******* 동기 vs 비동기 Blocking vs Non-Blocking
    https://youngkyonyou.github.io/interview/2021/12/23/Interview-interview-07.html
    https://dev-coco.tistory.com/46