- 성능 테스트 툴 비교
    - nGrinder
        - The Grinder라는 오픈소스 기반으로 네이버에서 개발한 오픈소스 프로젝트
        - IDE에서 Groovy 스크립트를 개발 및 테스트하고 분산 에이전트에서 실행한다.
        - 사용자 정의 라이브러리(jar, py, maven 종속성)로 테스트를 무제한으로 확장할 수 있다.
        - Jython 또는 Groovy 스크립트를 사용하여 테스트 시나리오와 다수의 에이전트를 사용하여 JVM내에서 스트레스를 생성할 수 있다.
        - 여러 테스트를 동시에 실행한다. 사전 설치된 여러 에이전트를 할당하여 각 에이전트의 활용도를 최대화한다
        - 스트레스를 생성하는 에이전트와 스트레스를 받는 대상 머신의 상태를 모니터링할 수 있다.
        - 테스트 타켓 서버 모니터링 가능합니다.
        - 툴이 다소 무거운 감이 있으나, 스크립트 수정으로 세밀한 성능 테스트 가능합니다.
        - 설치만 하면 사용하기 쉽습니다.
        - docker에서 사용 가능합니다.
        - 그래프를 합치지 못해서 데이터 시각화하는데 불편함이 존재한다.
        - 스크립트를 작성해야 하기 때문에 스크립트 작성 방법에 대한 학습이 필요합니다.
        - 시나리오 테스트가 불가능합니다. 개별 트랜잭션 TPS 측정에 중점을 두고 있습니다.
        - agent와 controller를 각자 실행해야합니다.

        - 2014년 초 이후 릴리즈 중단
        - 툴이 다소 무거운 감이 있으나, 스크립트 수정으로 세밀한 성능테스트 가능

        - Groovy의 경우 JUnit 기반으로 되어 있어, 테스트 스크립트를 nGrinder 내에서 실행하기 전에, IDE에서 먼저 확인해보고 디버깅할수 있다는 점이 큰 장점이다.
          또한 스크립트를 수정할 수 있으므로 세밀한 성능 테스트가 가능하다.
          계정 관리 기능이 존재하여 계정별 테스트 스케줄 및 이력 기능을 제공한다.

        - nGrinder는 Controller, Agent, Target으로 구성됩니다. Controller는 WAS 기반으로 웹 브라우저로 접속하여 GUI로 사용할 수 있습니다.
          Agent는 직접 부하를 발생시키는 머신입니다. Controller의 지휘하에 동작합니다. Target은 부하가 발생할 대상 서버를 의미합니다.

        - WAS 기반으로 동작하기 때문에 젠킨스나 소나큐브 대시보드와 같이 개발자 각각의 계정을 가질 수 있고, 계정 별 부하 테스트 히스토리를 관리할수도 있는 점이 장점입니다.

        - 기존 시스템 인터페이스 이용 및 변경 적음
        - TPS 기반의 단순 테스트에는 유리함

        - 시나리오 기반 테스트 및 가중치 테스트 불가
        - 빈약한 리포트
        - Groovy의 경우 JUnit 기반으로 되어 있어, 테스트 스크립트를 nGrinder 내에서 실행하기 전에, IDE에서 먼저 확인해보고 디버깅할 수 있다.
          또한 스크립트를 수정할 수 있으므로 세밀한 성능 테스트가 가능하다.
          계정 관리 기능이 존재하여 계정별 테스트 스케줄 및 이력 기능을 제공한다.

    - Jmeter
        - Apache에서 개발한 순수 자바로 개발된 성능 테스팅 도구이다.
        - Jenkins와 연동을 지원한다.
        - 플러그인을 이용하여 테스트 케이스를 XML 형식으로 작성 할 수 있다.
          플러그인으로 다른 프로그램과 연동 및 다양한 결과 분석 리포트를 얻을 수 있습니다.
        - 시나리오 기반 테스트가 가능합니다.
        - Non-GUI, 즉 CLI를 지원합니다. Script를 통해 다른 설정들을 가진 부하 테스트들을 한 번에, 쉽게 실행할 수 있습니다.
            - CI 또는 CD 툴과 연동할 때 편리하다.
            - UI를 사용하는 것보다 메모리 등 시스템 리소스를 적게 사용한다.
        - 멀티 스레드를 사용하여 다양한 사용자를 시뮬레이션하고 target에 대한 load를 생성할 수 있다.
        - 플러그인으로 많은 기능을 지원해줘서 JMeter 하나만으로 여러 테스트를 진행할 수 있고, 다양한 리포트를 얻을 수 있어서 좋았습니다.
        - JMeter가 직접 제공하지 않는 기능은 플러그인을 설치하여 사용하면 됩니다. 자체적으로 Plugin Manager 내장하고 있어, 별도의 설정 없이 플러그인을 설치할 수 있습니다.
        - 리스너의 설정(ex. result output path)은 CLI로 되지 않기 때문에, 많은 리스너가 존재할 경우 설정을 일일이 해줘야 해서 귀찮습니다.
          결과는 리스너로 만들어 보는데 모니터링이 불편함
        - 스레드 기반이라 성능제약이 있다고 합니다.
        - Swing 기반의 Component를 지원하기 때문에 GUI가 이쁘지 않다. (중요)
        - 아파치에서 만든 오래된 툴 유명하고 자료가 많다

        - 빠른 시나리오 작성 가능
        - 다양한 결과 리포트를 쉽게 얻을 수 있음
            - RPS(Requests Per Second)와 처리량(throughput), 지연 속도(latency), 응답 시간(response time) 등을 확인할 수 있는 리포트도 제공
        - TPS / 시나리오 기반 / 가중치 기반 테스트 가능
        - 풍부한 플러그인 제공
            - 플러그인을 통해 테스팅 능력, 데이터 분석, 가시화, 지속적인 통합 라이브러리 확장 제공

        - assert 수행 시 특정 로직을 실행하는 게 필요했고, 조건이 복잡해지면 테스트를 수행하는 게 어려워짐.
        - JMeter CLI를 이용해 테스트를 수행할 때도 테스트 플랜 파일을 작성하고 수정하는 건 GUI(Graphic User Interface)를 이용해야 함.

        - 스레드 개수를 늘려서 부하량을 높이는 방식

        - 빈약한 리포트

        - Java 8 문법을 내부적으로 사용하기 때문에 자바 8 버전 이상이 설치되어 있어야한다.

        - 최근까지도 꾸준한 릴리즈
        - 자세한 메뉴얼과 많은 검색 결과
        - 다양한 프로토콜 지원

        - Bean Shell & Selenium을 이용하여 자동화 테스트를 수행 할 수 있다.

        - 안정성이 입증되어 있고, 높은 점유율로 트러블 슈팅과 확장성이 용이하고, 초기 학습 비용이 낮아 바로 배워 사용할 수 있는 JMeter를 사용
        - Multi-Threading으로 부하테스트도 가능하고, 시나리오대로 트래픽을 발생시켜서 시나리오 테스트도 가능하다.
          그리고 JMeter는 CLI도 지원하기 때문에 CI/CD를 연동하기도 편리하고 UI를 사용하는 것보다 메모리나 시스템 리소스를 적게 사용한다.

        - Script Recording
          스크립트 레코딩은 기본적인 테스트 템플릿을 만드는 효율적인 방법입니다. JMeter에는 스크립트 레코딩 기능이 내장되어 있고,
          브라우저내에서 스크립트를 레코딩할 수 있는 확장 플러그인도 제공합니다.

        - 주요 개념
            - Thread Group: 한 쓰레드에 유저 한명 즉, 유저의 그룹으로 동시에 여러 유저가 요청을 보내는 경우를 세팅할 수 있다.
            - Sampler: 임의의 유저가 어떤 액션을 해야하는가에 대해서 나타낸다.
            - Listener: 요청에 대한 응답을 보고 응답 하나하나의 시간을 계산하거나 응답들을 조합해 그래프를 그린다.
            - Configuration: Sampler나 Listener가 사용할 설정 값(쿠키, JDBC 커넥션 등)
            - Assertion: 응답을 받았을때 해당 응답이 성공적인지 확인하는 방법을 제공한다.(응답 코드, 본문 내용등)

        - RPS(Requests Per Second)와 처리량(throughput), 지연 속도(latency), 응답 시간(response time) 등을 확인할 수 있는
          리포트도 제공하고 있어서 테스트 자동화 시스템에 적용하기에 편리
          널리 사용되는 도구인 만큼 향후 유지 보수가 용이하고 새로 참여하는 개발 인력이 업무를 빠르게 파악할 수 있다는 점도 장점
        - 하지만 assert 수행 시 특정 로직을 실행하는 게 필요했고, 조건이 복잡해지면 테스트를 수행하는 게 어려워질 수도 있음
          또한 JMeter CLI를 이용해 테스트를 수행할 때도 테스트 플랜 파일을 작성하고 수정하는 건 GUI(Graphic User Interface)를 이용해야 했고,
          스레드 개수를 늘려서 부하량을 높이는 방식이어서 저희가 원하는 양의 트래픽을 생성하는 게 어려울 것 같다고 판단
          물론 JMeter가 여러 개의 장비에서 대량의 트래픽을 생성하는 분산(distributed) 테스트 기능을 제공하긴 하지만, 시스템을 복잡하게 만들고 싶지 않았고 장비도 너무 많이 필요

        - Blazemeter : Jmeter 스크립트 생성 도구(크롬 플로그인)
            - 스크립트 녹화
            - 녹화된 스크립트 편집
            - 다양한 포맷으로 저장


- 성능 테스트 순서
    1. 테스트 환경을 파악
      - 실제 테스트할 환경을 파악하는 단계이다. 실제 라이브될 운영환경인지 특정 테스트를 위한 환경인지? 어떤 테스트 툴을 사용할 수 있는지? (운영에서는 불가한 테스트 툴이 있을 수 있다.) 하드웨어, 소프트웨어, 네트워크 환경 등 파악

    2. 성능에 대한 수용 가능한 기준 식별
      - 성능 테스트에 대한 목표를 정하는 단계이다. 어떤 테스트를 하는 것이며 어떤 조건하에 어느정도의 트래픽 하에서 버틸 수 있는지? 어느정도의 속도를 내야 하는지? 이런 수치들은 다른 유사한 서비스들을 밴치마킹 하여 설정해도 좋다.

    3. 성능 테스트 계획 수립
      - end user를 어떤 방식으로 가정할건지 설정하고, 테스트 시 어떤 데이터와 지표들을 수집할 것인지 확인하는 단계이다. 몇명의 유저를 어떤식으로 발생시킬지? 등.. 잘 알려진 ngrinder, jmeter 등을 선택하여 설정할 수 있다.

    4. 테스트 환경 설정
      - 실제 실행하기 전 테스트 환경을 준비하는 단계이다. 어플리케이션을 준비하고, 제약 조건을 적용하고, 필요한 테스트 리소스를 준비한다.

    5. 테스트 실행

    6. 분석, 튜닝, 다시 테스트
      - 테스트 결과를 분석하여 성능을 확인하고, 시스템을 개선한다. 테스트를 재시작할 때마다 개선된 지표를 확인할 수 있어야 한다.

- 성능 테스트의 목적
    1. 목표 성능 도달 여부 확인
    2. 한계 성능 측정
    3. 부하 스트레스 하에서 기능 안정성 확인


* 내가 만든 이 시스템은 실제 장비에서

  어느 정도의 부하 를 견딜 수 있는가?
  시스템(데이터베이스, 서버), 자원(cpu, disk, memory 등)에서 병목 이 생기진 않는가?
  자원을 효율적 으로 사용하는가?
  메모리 누수, 오류, 오버플로우 는 발생하지 않는가?
  최악의 상황 에선 어떤 동작을 하는가? (예측하고 대비하기 위하여)
  장애 조치와 복구 가 잘 동작을 하는가?
  전혀 감이 안 잡혔습니다. 이것은 흔히 말하는 localhost:8080 레벨에서는 확인할 수 없는 문제였습니다.

  위의 수많은 공포에 대하여, ‘내가 예상하는 데로’, ‘내가 정의해놓은 데로, 작성해놓은 데로 잘 동작하겠지’ 라고 믿을 수는 없었습니다. 호되게 당한 적도 있기도 하고, 실제 사용자의 돈이 움직이는 결제 시스템에서 전혀 보이지 않는 불안요소를 가지고 가기에는 너무나 두렵고 무서웠습니다.

  그래서 테스트 코드가 두려움을 해소해줬듯, 이번에도 두려움을 해소해줄 성능 테스트 환경을 만들게 되었습니다.


- 성능 테스트시 하지 말아야 할 Mocking 방식
    1. 객체 Mocking
        객체 Mocking은 테스트 코드를 작성할 때 가장 많이 사용하는 방식이라 친숙할 것 입니다. 그러나 로직에 대해 검증을 하는 테스트와 달리, 성능 테스트는 어플리케이션 동작과 자원의 사용을 모두 보아야만 하는 테스트입니다.
        객체 Mocking 은 해당 객체의 행위 뒤로 들어가야 할 동작들을 무시해버리게 됩니다. 예를 들어 Spring Profile 을 사용하여 RestOpertation 을 객체를 Mock 처리하였을 때
            http connection Pool 미사용
            connection thread 미사용
            io가 발생하지 않음
        등등 성능 테스트에서 중요한 관점인 Thread 사용, 리소스 사용을 전부 무시하게 됩니다.
        외부 인터페이스를 Mocking 하는 것처럼 보이지만, 내부 인터페이스도 Mocking 해버리는 객체 Mocking 은 성능 테스트에서 피해야 합니다.

    2. 같은 어플리케이션에 Dummy Controller 생성
        이 방식도 아주 간혹 테스트 코드를 작성할 때 사용하는 방식입니다. 이 방식이 1번 방식과 다른 것은 실제로 요청을 보내고 받으며 자원을 사용한다는 것 입니다.
        그러나 Dummy Controller 의 로직은 테스트 시스템의 자원과 리소스를 같이 사용해버리게 됩니다. 테스트 대상 시스템이 더 늘어나 버리는
        신뢰성이 굉장히 떨어지는 의미없는 성능 테스트를 하게 됩니다. 테스트를 위한 요소는 대상 시스템에 절대로 영향을 미쳐서는 안 됩니다.

- 테스트 계획
    - 성능 테스트 툴 선정 기준
        - 제대로 된 테스트를 위해서는 아래의 3가지 기능을 지원하는게 좋습니다.
            - 시나리오 기반의 테스트가 가능해야 합니다.
            - 동시 접속자 수, 요청 간격, 최대 Throughput 등 부하를 조정할 수 있어야 합니다.
            - 부하 테스트 서버 스케일 아웃을 지원하는 등 충분한 부하를 줄 수 있어야 합니다.

        - 조직에 맞는 Load Testing (부하 발생) 도구 선별이 중요
            - CI/CD와의 연동이 편한가?
                - Jenkins 같은 CI/CD 툴하고 쉽게 연동이 되어, 배포 이전에 쉽게 TPS나 성능 측정을 할수 있는가?
            - 학습과 트러블슈팅이 용이한가? 관리나 유지보수가 되는가?
                - 자료가 많이 공유되어 있고, 도움을 받을 수 있는 커뮤니티가 구성되어 있는지?
                - 끊임없이 제품을 유지보수하거나 관리가 되는 오픈소스 인지?
            - 부하 스크립트를 작성하는 주체는 누구인가? (개발자 vs 비 개발자)
                - 개발자 : 스크립트 기반으로 높은 자율성을 가지고 개발이 가능한지? (Gatling)
                - 비개발자 : GUI 툴로 뚝딱 만들 수 있는 도구를 선호 (Jmeter)
    - 테스트 계획
        - 바로 테스트에 들어가기 전에 테스트 계획부터 세워야합니다. 부하테스트는 시스템의 응답 성능과 한계치를 파악하기 위한 테스트입니다.
            - 서비스가 어느 트래픽까지 처리할 수 있는지(어느 정도의 부하를 견디는지) → 어떻게 대응할지 알 수 있음
            - 병목이 발생하는 부분이 어디인지 → 어떻게 개선할지 알 수 있음

        - 이를 파악하기 위한 지표는 아래와 같습니다.
            - Users: 동시에 사용할 수 있는 사람
            - TPS: 일정 시간 동안 얼마나 처리할 수 있는지
                - 한 명만 사용해도 처리량이 좋지 않으면 → scale up
                - 부하 증가시 문제 → scale out
            - Time: 얼마나 빠른지

    - 전제조건
        - 테스트하려는 Target 시스템의 범위를 정해야 합니다.
        - 부하 테스트시에 저장될 데이터 건수와 크기를 결정하세요. 서비스 이용자 수, 사용자의 행동 패턴, 사용 기간 등을 고려하여 계산합니다.
        - 목푯값에 대한 성능 유지기간을 정해야 합니다.
        - 서버에 같이 동작하고 있는 다른 시스템, 제약 사항 등을 파악합니다.

        1) Target 시스템의 범위
            - Was부터 시작하는 범위
        2) 데이터 수
            - 각각 20만개의 초기 더미 데이터를 넣고 진행
        3) 목푯값에 대한 성능 유지기간
            - 주요 기능의 성능이 30분 동안 유지되는 것을 목표로 테스트 & 개선

    - 테스트 종류
        1) Smoke 테스트
            - VUser: 1 ~ 2
            - 최소의 부하로 시나리오를 검증해봅니다.
        2) Load 테스트
            - 평소 트래픽과 최대 트래픽일 때 VUser를 계산 후 시나리오를 검증해봅니다.
            - 결과에 따라 개선해보면서 테스트를 반복합니다.
        3) Stress 테스트
            - 최대 사용자 혹은 최대 처리량인 경우의 한계점을 확인하는 테스트입니다.
            - 점진적으로 부하를 증가시켜봅니다.
            - 테스트 이후 시스템이 수동 개입 없이 자동 복구되는지 확인해봅니다.

    - 성능 목표 설정
        - 성능 목표를 정하기 위해서는 VUser를 구해야합니다.(Load 테스트인 경우)
        - VUser를 구하기 위해서는 아래와 같은 지표들이 필요합니다.
            - DAU(일일 활동 사용자 수)
            - 피크시간대 집중률(최대 트래픽 / 평소 트래픽)
            - 1명당 1일 평균 요청 수
        - 코드봐줘의 경우 실제 사용자가 거의 없기 때문에 정확하지는 않지만 similarweb을 이용해 비슷한 서비스의 트래픽을 보고 지표들을 가정하여 진행해보려고 합니다.
          (사이트 주소를 입력하시면 간단한 트래픽 정보를 볼 수 있습니다.)

        - 성능 목표를 정하기 위해서는 VUser를 구해야합니다.(Load 테스트인 경우)
        - VUser를 구하기 위해서는 아래와 같은 지표들이 필요합니다.
            DAU(일일 활동 사용자 수)
            피크시간대 집중률(최대 트래픽 / 평소 트래픽)
            1명 당 1일 평균 요청 수

        ex)
            위 사이트는 한달에 약 210만명이 방문하고 있습니다. 따라서 정확하지는 않겠지만 DAU를 210만/30(한달)로 계산하여 7만명으로 잡고 계산하겠습니다.
            피크시간대 집중률은 저녁 시간대에 늘어날 것으로 예상은 되지만, 최대 트래픽과 평소 트래픽의 차이가 크게는 없을 것이라고 생각하여
            3배로, 1명당 1일 평균 요청 수는 14정도로 가정하고 진행하겠습니다.
                - DAU(일일 활동 사용자 수): 70000
                - 피크시간대 집중률(최대 트래픽 / 평소 트래픽): 3
                - 1명당 1일 평균 요청 수: 14

            위의 정보를 가지고 throughput(1일 평균 rps ~ 1일 최대 rps)를 계산할 수 있습니다.
                - DAU * 1명당 1일 평균 접속수 = 1일 총 접속수
                    → 70000 * 14 = 980000
                - 1일 총 접속수 / 86400(초 / 일) = 1일 평균 rps
                    → 980000 / 86400 = 약 11.3
                - 1일 평균 rps * (최대 트래픽 / 평소 트래픽) = 1일 최대 rps
                    → 11.3 * 3 = 약 34

            즉 throughput은 11.3(1일 평균 rps) ~ 34(1일 최대 rps)가 나오게 됩니다. 이 정보를 통해서 VUser를 계산할 수 있습니다.
                VUser: (목표 rps * T) / R

                - R: 시나리오에 포함된 요청의 수
                - T: 시나리오 완료 시간보다 큰 값(VUser 반복을 완료하는데 필요한 시간보다 큰 값)
                    - T = (R * 왕복시간(http_req_duration)) + 지연시간(내부망일 경우 추가하기! - 외부에서 처리되는 시간을 보정해주기 위함!)
                        - 왕복 시간 = 얼마 안에 끝나야 하는지

            이번에 테스트해 볼 시나리오에 포함된 요청의 수는 7, http_req_duration을 0.5s, 지연시간을 1s로 설정한다면 아래와 같은 결과가 도출됩니다.
                - R = 7
                - T = (7 * 0.5) + 1 = 4.5
                - 평소 트래픽 VUser: (11.3 * 4.5) / 8 = 약 7
                - 최대 트래픽 VUser: (34 * 4.5) / 8 = 약 22

            보통 VUser(active user)가 80정도면 헤비한 트래픽이 발생하는 것이라고 생각한다고 합니다. 보통은 10 ~ 15명 정도로도 충분하다고 합니다.

    - 테스트 목표
        1) Smoke 테스트
            - VUser: 1 ~ 2
            - Throughput: 11.3 ~ 34 이상
            - Latency: 50 ~ 100ms 이하
        2) Load 테스트
            - 평소 트래픽 VUser: 7
            - 최대 트래픽 VUser: 22
            - Throughput: 11.3 ~ 34 이상
            - Latency: 50 ~ 100ms 이하
            - 성능 유지 기간: 30분
        3) Stress 테스트
            - VUser: 점진적으로 증가시켜보기

    - 시나리오
        - 시나리오 대상은 접속 빈도 높거나 서버 리소스 소비량이 많거나 DB를 사용하는 기능을 선택해보는게 좋습니다.
            ex)
                1) 시나리오1
                    로그인 - 언어 기술 목록 조회 - 리뷰어 목록 조회 - 리뷰어 단일 조회 - 내가 받은 리뷰 목록 조회 - 내가 리뷰한 리뷰 목록 조회 - 리뷰 상세 조회
                    추후에 CUD관련 부분도 테스트 해보려고 합니다.
                2) 시나리오2
                    로그인 - 언어 기술 목록 조회 - 리뷰어 목록 조회 - 리뷰어 선택 - 리뷰 요청

        - 시나리오 선택
            - 주요 시나리오 테스트
                고객사와 협의하여 주요 시나리오 선정 후 (가입, 강좌 수강 등) 개별 시나리오 별로 얼마나 견디는지 테스트하는 방법

            - 주요 시나리오 가중치 테스트
                위 선정된 주요 시나리오에 가중치를 부여하는 테스트 방법
                예) A 시나리오 20%, B 시나리오 30%, C 시나리오 50%
                시나리오 별로 가중치를 다르게 해서, 여러 개의 시나리오를 동시에 던지는 방법

            - 사용 빈도가 높은 Top N의 트랜잭션을 추출하여 테스트
                APM과 같은 솔루션으로 트랜잭션의 빈도를 추출하여, 많이 호출되는 상위 트랜잭션을 20~30개를 뽑아내어 테스트 시나리오를 만드는 방법

            -> 이 중 가장 현실세계와 가까운 것은 2번째인 주요 시나리오 가중치 테스트입니다.

        - 시나리오 선택2
            - 주요 시나리오 테스트
                - 주요 시나리오 선정 후 (가입, 강좌 수강 등) 시나리오별로 얼마나 견디는지 테스트
            - 트랜잭션별 단위 테스트
                - Top 20 에 대한 트랜잭션에 대해서 얼마나 견디는지 테스트
            - 주요 시나리오 가중치 테스트
                - 위 선정된 시나리오에 가중치를 부여하여 테스트

    - 테스트 계획 템플렛
        https://docs.google.com/spreadsheets/d/1EY4IEjNThJCM_dnYrP-RCbQDLTFJuHSDM5FuSVidtgY/edit#gid=1846954690

    - 실제 테스트
        - nGrinder의 TPS는 Test Per Seconds의 약자로 초당 몇 번의 테스트가 일어났는지를 뜻합니다.

        - 이상적으로 TPS는 VUser가 증가하는 것에 비례해서 늘어납니다. 그러다가 어느 시점에 TPS가 더 이상 증가하지 않게 되는데 이 지점에 도달하는 순간
          사용자의 응답시간에 영향을 미칩니다.(이 지점에서 오히려 TPS가 떨어질 수도 있는데 이 경우 서버 튜닝이 제대로 되지 않은 것입니다.)
          처리해야할 요청은 늘어나는데 처리량은 멈춰있기 때문입니다.
          이런경우 scale up이나 scale out으로 TPS를 증가시킬 수 있습니다.

        - 평균 테스트 시간 그래프는 한 번의 요청부터 응답까지 몇 초가 걸렸는지를 나타냅니다.
          하지만 이 그래프에는 문제가 하나 있습니다. 위의 그래프들은 시나리오에 포함된 모든 요청에 대한 평균값을 나타냅니다.
          따라서 제 시나리오를 예시로 들면 시나리오 안에 총 7개의 요청이 있기 때문에 7개 요청의 평균값을 그래프로 나타낸 것입니다.
          따라서 특정 요청에 대한 결과를 보기는 힘듭니다.
          이는 로그 파일이나 CSV 파일을 통해 파악할 수 있습니다.

        - Pinpoint 활용하기
            - 각 요청에 대한 처리 시간은 Pinpoint를 활용하여 자세히 확인할 수 있습니다.
            - 부하 테스트를 진행한 시간대를 설정해주면 아래와 같이 시간대에 따른 요청을 초록색 점으로 나타냅니다. 하나의 점이 하나의 요청입니다.
            - 초록색 점들을 드래그 해보면 각 요청에 대한 세부 정보(각 메서드 당 실행 시간 등)를 확인할 수 있습니다.

    - Smoke Test
        - 최소한의 부하를 견딜 수 있는지 smoke 테스트를 먼저 진행해보겠습니다. 최소한의 부하이므로 테스트 시간은 10분 정도만 진행하겠습니다.
            - VUser: 2
            - 테스트 시간: 10분
        - 목표: MTT: 50 ~ 100ms 이하

    - Load Test
        - 평균 트래픽일 경우와 최대 트래픽일 경우를 나눠서 테스트해보겠습니다. 테스트 시간은 30분으로 진행합니다.
            - 평균 트래픽
                - VUser: 7
                - 테스트 시간: 30분
            - 최대 트래픽
                - VUser: 22
                - 테스트 시간: 30분
            - 목표: MTT: 50 ~ 100ms 이하

        -> 당연히 이미 문제가 되었던 쿼리는 더 성능이 안좋아졌고, 새로운 문제도 찾을 수 있었습니다. connection 과정에서 많은 비용이 소모되는 걸 볼 수 있었습니다.
          다른 요청들도 모두 시간이 늘어났으므로 모두 확인해보니 findAll과 login을 제외한 요청은 전부 connection에서 비용이 확 증가한 것이었습니다. connection pool도 조정해야겠습니다.

- 테스트 전 준비 사항
    - 목적 파악
        - 성능 테스트를 할 것인지, 부하 테스트를 할 것인지 목적을 설정한다.
        - 사내에는 스케일 아웃 조건을 파악하기 위해 테스트를 한다.
        - 성능 테스트의 경우 Response time, TPS 등을 확인한다.
        - 부하 테스트의 경우 서버가 다운되는 동시 접속량을 확인한다.

    - 테스트 플랜 설정하기
        - Thread group 생성
            - Number of Threads : 테스트할 서버로 접속을 시도하는 사용자의 수
            - Lamp-Up Period : 사용자간 접속을 요청하는 간격
            - Loop count : 사용자마다 요청 하는 횟수
            - Number of Threads, Lamp-Up Period, Loop count를 설정한다.
            - 예를 들어 Lamp-Up Period를 2로 설정했다면 첫 번째 사용자가 접속 요청후 2초 후 두 번째 사용자가 접속 요청을 한다.
            - Number of Threads * Loop count = 총 요청 횟수 이다.
            - 접속을 계속 시도하고 싶은 경우 Loop count를 Forever로 체크하면 된다.

        - Sampler 생성
            - 사용자의 행동을 대행한다.
            - API test의 경우 HTTP Request를 사용한다. ( API는 http로 호출하기 때문.)
            - HTTP Request에서 protocol, ip, port, method, path, body data( POST일 경우 )을 설정한다.
            - HTTP Reqeust를 보낼때 json형식으로 보내야 된다면, HTTP Header Manager 를 추가하고 설정한다.( Thread Group 오른쪽 마우스 클릭 -> config element -> HTTP Header Manager)

        - Listener 생성
            - 처리 상황 및 결과를 데이터나 그래프로 보여준다.
            - 응답시간이 필요한 경우 Plugin으로 Response Times Over Time을 설치하여 사용한다.


***** 성능 test 를 위한 보고서
    https://cyberx.tistory.com/213

***** 실전 Web Application 부하 테스트 1편
    https://blog.imqa.io/load_test1/

***** 실전 Web Application 부하 테스트 - 2편
    https://blog.imqa.io/loadtesting2/

* 서버 사이드 테스트 자동화 여정
    1. 테스트 자동화를 시작한 계기와 그 첫 발걸음
        https://engineering.linecorp.com/ko/blog/server-side-test-automation-journey-1

    2. 통합 테스트 수준의 회귀 테스트 환경 구축 및 Docker 활용
        https://engineering.linecorp.com/ko/blog/server-side-test-automation-journey-2

    3. Docker를 활용한 통합 테스트 환경 개선
        https://engineering.linecorp.com/ko/blog/server-side-test-automation-journey-3

    4. 성능 테스트 자동화 목표 설정 및 테스트 환경 구성
        https://engineering.linecorp.com/ko/blog/server-side-test-automation-4

    5. 성능 테스트 리포트 생성 및 자동화 시스템 업무 적용 결과
        https://engineering.linecorp.com/ko/blog/server-side-test-automation-5



[테스트 시나리오]
https://velog.io/@max9106/nGrinderPinpoint-test1

[비교]
https://loosie.tistory.com/822
https://hsik0225.github.io/jmeter/ngrinder/k6/2021/09/15/%EC%84%9C%EB%B2%84-%ED%8D%BC%ED%8F%AC%EB%A8%BC%EC%8A%A4-%ED%85%8C%EC%8A%A4%ED%8A%B8-%EB%8F%84%EA%B5%AC-%EB%B6%84%EC%84%9D/

[JMeter]
https://m.blog.naver.com/dbstnrrud93/221848689002
https://catsbi.oopy.io/4e6b21f7-d4e6-47c6-9c2d-180edfa68c02
https://cyberx.tistory.com/214
https://jangseongwoo.github.io/test/jmeter_basic/
https://12bme.tistory.com/503
https://creampuffy.tistory.com/209

(예제)
https://www.slideshare.net/IMQAGroup/ss-183469952

[nGrinder]
https://choibulldog.tistory.com/60
https://cyberx.tistory.com/220

(예제)
https://nesoy.github.io/articles/2018-10/nGrinder-Start



- Elastic APM
    - 다양한 언어 지원
    - 오픈 소스로 누구나 사용 가능
    - 실시간 장애 인지보다는 장애 후 원인 분석 가능

- pinpoint
    - 무료로 사용할 수 있으며, 인프라로 구성한 분산 서버를 테스트하기 위한 전용 기능들을 제공하고,
      이미 네이버, NHN, 우아한형제들과 같은 대용량 트래픽을 다루는 업체들에서도 사용할 정도로 검증이 된 도구이다.

***** Ngrinder로 성능을 테스트 후 캐시로 성능 올리기
    https://shrewd.tistory.com/m/50